{
  "rootFolder": "test-demo",
  "globalTypingSpeed": 35,
  "actionDelay": 1000,
  "defaultVoice": "en-US-BrianNeural",
  "enableVoiceover": true,
  "actions": [
    {
      "type": "createFile",
      "path": "test.py",
      "voiceover": "Let's create the test.py file.",
      "voiceoverTiming": "before"
    },
    {
      "type": "openFile",
      "path": "test.py"
    },
    {
      "type": "writeText",
      "content": "import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom Statistics import Statistics  # Custom module for financial metrics (Sharpe, VaR, MDD)\n\n# --- Deep Learning (TensorFlow/Keras) ---\nimport tensorflow as tf\nfrom tensorflow.keras.layers import CuDNNLSTM, Dropout,Dense,Input  # CuDNNLSTM = GPU-optimized LSTM\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras import optimizers\nimport warnings\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "import pandas as pd",
      "voiceover": "--- Data Processing & Utilities ---",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "from Statistics import Statistics",
      "voiceover": "Custom module for financial metrics (Sharpe, VaR, MDD)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "from tensorflow.keras.layers import CuDNNLSTM, Dropout,Dense,Input",
      "voiceover": "CuDNNLSTM = GPU-optimized LSTM",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "warnings.filterwarnings(\"ignore\")\n"
    },
    {
      "type": "writeText",
      "content": "import os\n"
    },
    {
      "type": "writeText",
      "content": "SEED = 9\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "SEED = 9",
      "voiceover": "--- Reproducibility: Set seeds for consistent results ---",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.environ['PYTHONHASHSEED']=str(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "random.seed(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "np.random.seed(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "SP500_df = pd.read_csv('data/SPXconst.csv')\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "SP500_df = pd.read_csv('data/SPXconst.csv')",
      "voiceover": "--- Load S&P 500 Constituents Data --- Tracks which companies were in S&P 500 at any given time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "all_companies = list(set(SP500_df.values.flatten()))\n"
    },
    {
      "type": "writeText",
      "content": "all_companies.remove(np.nan)\n"
    },
    {
      "type": "writeText",
      "content": "constituents = {'-'.join(col.split('/')[::-1]):set(SP500_df[col].dropna()) \n                for col in SP500_df.columns}\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "constituents = {'-'.join(col.split('/')[::-1]):set(SP500_df[col].dropna())",
      "voiceover": "Create dictionary mapping 'YYYY-MM' to set of stock tickers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "constituents_train = {} \n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "constituents_train = {}",
      "voiceover": "Pre-compute stocks in the 3-year lookback period for each test year",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for test_year in range(1993,2016):\n    months = [str(t)+'-0'+str(m) if m<10 else str(t)+'-'+str(m) \n              for t in range(test_year-3,test_year) for m in range(1,13)]\n    constituents_train[test_year] = [list(constituents[m]) for m in months]\n    constituents_train[test_year] = set([i for sublist in constituents_train[test_year] \n                                         for i in sublist])\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def makeLSTM():\n    # Builds and compiles the 3-feature LSTM neural network\n    # Input shape: (240 timesteps, 3 features per timestep)\n    # Architecture: Input -> LSTM(25 cells) -> Dropout(0.1) -> Dense(2, softmax)\n    # Output: probability distribution for bottom 50% vs top 50% performers\n    \n    inputs = Input(shape=(240,3))\n    x = CuDNNLSTM(25,return_sequences=False)(inputs)\n    x = Dropout(0.1)(x)\n    outputs = Dense(2,activation='softmax')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with categorical cross-entropy loss and RMSprop optimizer\n    model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(),\n                          metrics=['accuracy'])\n    model.summary()\n    return model\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "x = CuDNNLSTM(25,return_sequences=False)(inputs)",
      "voiceover": "GPU-optimized LSTM, 25 memory cells",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "x = Dropout(0.1)(x)",
      "voiceover": "Regularization: randomly zero 10% of outputs during training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "outputs = Dense(2,activation='softmax')(x)",
      "voiceover": "Binary classification (bottom/top 50%)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def callbacks_req(model_type='LSTM'):\n    # Creates Keras callbacks for training monitoring and control\n    # 1. CSVLogger: logs metrics to CSV file after each epoch\n    # 2. ModelCheckpoint: saves model after every epoch\n    # 3. EarlyStopping: stops training if val_loss doesn't improve for 10 epochs\n    \n    csv_logger = CSVLogger(model_folder+'/training-log-'+model_type+'-'+str(test_year)+'.csv')\n    filepath = model_folder+\"/model-\" + model_type + '-' + str(test_year) + \"-E{epoch:02d}.h5\"\n    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss',save_best_only=False, period=1)\n    earlyStopping = EarlyStopping(monitor='val_loss',mode='min',patience=10,restore_best_weights=True)\n    return [csv_logger,earlyStopping,model_checkpoint]\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def reshaper(arr):\n    # Transforms 2D feature array (720 columns) into 3D tensor for LSTM\n    # Input: (samples, 720) where 720 = 240*3 features arranged as [IntraR0-239, NextR0-239, CloseR0-239]\n    # Output: (samples, 240, 3) where each timestep has [intraday, overnight, close-to-close]\n    \n    # Step 1: Split into 3 parts of 240 each -> shape (3, samples, 240)\n    arr = np.array(np.split(arr,3,axis=1))\n    # Step 2: Swap axes -> shape (samples, 3, 240)\n    arr = np.swapaxes(arr,0,1)\n    # Step 3: Swap axes -> shape (samples, 240, 3) - final LSTM format\n    arr = np.swapaxes(arr,1,2)\n    return arr\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def trainer(train_data,test_data):\n    # Orchestrates complete training pipeline for 3-feature LSTM\n    # 1. Shuffles training data for randomization\n    # 2. Extracts features (720 cols), labels, and returns\n    # 3. Reshapes features to 3D, one-hot encodes labels\n    # 4. Trains model with callbacks\n    # 5. Generates predictions for each test day\n    \n    np.random.shuffle(train_data)\n    \n    # Extract features (cols 2 to -2), labels (last col), and future returns\n    train_x,train_y,train_ret = train_data[:,2:-2],train_data[:,-1],train_data[:,-2]\n    train_x = reshaper(train_x)\n    \n    # Reshape labels and one-hot encode for cross-entropy loss\n    train_y = np.reshape(train_y,(-1, 1))\n    train_ret = np.reshape(train_ret,(-1, 1))\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(train_y)\n    enc_y = enc.transform(train_y).toarray()\n    train_ret = np.hstack((np.zeros((len(train_data),1)),train_ret)) \n\n    model = makeLSTM()\n    callbacks = callbacks_req()\n    \n    # Train model: max 1000 epochs, 20% validation split, batch size 512\n    model.fit(train_x,\n              enc_y,\n              epochs=1000,\n              validation_split=0.2,\n              callbacks=callbacks,\n              batch_size=512\n              )\n\n    # Generate predictions for each date in test set\n    dates = list(set(test_data[:,0]))\n    predictions = {}\n    for day in dates:\n        test_d = test_data[test_data[:,0]==day]\n        test_d = reshaper(test_d[:,2:-2])\n        predictions[day] = model.predict(test_d)[:,1]\n    return model,predictions\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "np.random.shuffle(train_data)",
      "voiceover": "Randomize order of stock-day observations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "train_x = reshaper(train_x)",
      "voiceover": "Convert to (samples, 240, 3) format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "enc_y = enc.transform(train_y).toarray()",
      "voiceover": "Convert 0/1 to [1,0]/[0,1]",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "test_d = reshaper(test_d[:,2:-2])",
      "voiceover": "Reshape to (stocks, 240, 3)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "predictions[day] = model.predict(test_d)[:,1]",
      "voiceover": "Probability of being top 50%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def trained(filename,train_data,test_data):\n    # Loads a previously saved model and generates predictions (no retraining)\n    # Useful for model deployment and result reproduction\n    \n    model = load_model(filename)\n\n    dates = list(set(test_data[:,0]))\n    predictions = {}\n    for day in dates:\n        test_d = test_data[test_data[:,0]==day]\n        # NOTE: This reshape is for single-feature model; use reshaper() for 3-feature\n        test_d = np.reshape(test_d[:,2:-2],(len(test_d),240,1))\n        predictions[day] = model.predict(test_d)[:,1]\n    return model,predictions     \n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def simulate(test_data,predictions):\n    # Implements long-short trading strategy from model predictions\n    # Long: buy top 10 predicted stocks\n    # Short: sell bottom 10 predicted stocks\n    # Returns DataFrame with daily long and short returns\n    \n    rets = pd.DataFrame([],columns=['Long','Short'])\n    k = 10\n    \n    for day in sorted(predictions.keys()):\n        preds = predictions[day]\n        test_returns = test_data[test_data[:,0]==day][:,-2]\n        \n        # Long position: top k predicted stocks\n        top_preds = predictions[day].argsort()[-k:][::-1]\n        trans_long = test_returns[top_preds]\n        \n        # Short position: bottom k predicted stocks (profit when price drops)\n        worst_preds = predictions[day].argsort()[:k][::-1]\n        trans_short = -test_returns[worst_preds]\n        \n        rets.loc[day] = [np.mean(trans_long),np.mean(trans_short)]\n        \n    print('Result : ',rets.mean())  \n    return rets       \n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "k = 10",
      "voiceover": "Portfolio size: top/bottom 10 stocks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "test_returns = test_data[test_data[:,0]==day][:,-2]",
      "voiceover": "Actual future returns",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "top_preds = predictions[day].argsort()[-k:][::-1]",
      "voiceover": "Indices of k highest probs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "worst_preds = predictions[day].argsort()[:k][::-1]",
      "voiceover": "Indices of k lowest probs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "trans_short = -test_returns[worst_preds]",
      "voiceover": "Negative because shorting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "rets.loc[day] = [np.mean(trans_long),np.mean(trans_short)]",
      "voiceover": "Equal-weighted avg",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def create_label(df_open,df_close,perc=[0.5,0.5]):\n    # Creates binary labels (0/1) for all stocks on each trading day\n    # Ranks stocks by intraday return (close/open - 1)\n    # Label 0 = bottom 50% performers, Label 1 = top 50% performers\n    \n    # Validate date alignment between open and close data\n    if not np.all(df_close.iloc[:,0]==df_open.iloc[:,0]):\n        print('Date Index issue')\n        return\n    \n    perc = [0.]+list(np.cumsum(perc))\n    \n    # Calculate intraday returns and rank into quantiles\n    label = (df_close.iloc[:,1:]/df_open.iloc[:,1:]-1).apply(\n            lambda x: pd.qcut(x.rank(method='first'),perc,labels=False), axis=1)\n    return label[1:]\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "perc = [0.]+list(np.cumsum(perc))",
      "voiceover": "Convert to quantile boundaries [0, 0.5, 1]",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "return label[1:]",
      "voiceover": "Exclude first row (aligns with feature shifting)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def create_stock_data(df_open,df_close,st,m=240):\n    # Constructs 720 features for a single stock: 3 types Ã— 240 timesteps\n    # Feature Type 1: IntraR (intraday returns = close/open - 1)\n    # Feature Type 2: NextR (overnight returns = next_open/prev_close - 1)\n    # Feature Type 3: CloseR (daily returns = close-to-close percent change)\n    \n    st_data = pd.DataFrame([])\n    st_data['Date'] = list(df_close['Date'])\n    st_data['Name'] = [st]*len(st_data)\n    \n    # --- Feature Type 1: Intraday Returns (240 features) ---\n    daily_change = df_close[st]/df_open[st]-1\n    for k in range(m)[::-1]:\n        st_data['IntraR'+str(k)] = daily_change.shift(k)\n\n    # --- Feature Type 2: Overnight Returns (240 features) ---\n    # Gap between previous close and next open\n    nextday_ret = (np.array(df_open[st][1:])/np.array(df_close[st][:-1])-1)\n    nextday_ret = pd.Series(list(nextday_ret)+[np.nan])     \n    for k in range(m)[::-1]:\n        st_data['NextR'+str(k)] = nextday_ret.shift(k)\n\n    # --- Feature Type 3: Close-to-Close Returns (240 features) ---\n    close_change = df_close[st].pct_change()\n    for k in range(m)[::-1]:\n        st_data['CloseR'+str(k)] = close_change.shift(k)\n\n    # Target: next day's intraday return (what we're predicting)\n    st_data['IntraR-future'] = daily_change.shift(-1)    \n    st_data['label'] = list(label[st])+[np.nan] \n    st_data['Month'] = list(df_close['Date'].str[:-3])\n    st_data = st_data.dropna()\n    \n    # Split by year: training = before test_year, test = test_year only\n    trade_year = st_data['Month'].str[:4]\n    st_data = st_data.drop(columns=['Month'])\n    st_train_data = st_data[trade_year<str(test_year)]\n    st_test_data = st_data[trade_year==str(test_year)]\n    return np.array(st_train_data),np.array(st_test_data) \n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "daily_change = df_close[st]/df_open[st]-1",
      "voiceover": "Today's intraday return",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "st_data['IntraR'+str(k)] = daily_change.shift(k)",
      "voiceover": "Shift to get historical values",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "close_change = df_close[st].pct_change()",
      "voiceover": "Daily percent change",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "st_data = st_data.dropna()",
      "voiceover": "Remove rows with missing values",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def scalar_normalize(train_data,test_data):\n    # Normalizes all 720 features using RobustScaler (median/IQR based)\n    # Robust to outliers - important for financial data\n    # Fits ONLY on training data to prevent data leakage\n    \n    scaler = RobustScaler()\n    scaler.fit(train_data[:,2:-2])\n    train_data[:,2:-2] = scaler.transform(train_data[:,2:-2])\n    test_data[:,2:-2] = scaler.transform(test_data[:,2:-2])\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "scaler.fit(train_data[:,2:-2])",
      "voiceover": "Fit on training features only",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "test_data[:,2:-2] = scaler.transform(test_data[:,2:-2])",
      "voiceover": "Transform both sets",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_folder = 'models-Intraday-240-3-LSTM'\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "model_folder = 'models-Intraday-240-3-LSTM'",
      "voiceover": "Create directories for models and results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "result_folder = 'results-Intraday-240-3-LSTM'\n"
    },
    {
      "type": "writeText",
      "content": "for directory in [model_folder,result_folder]:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n"
    },
    {
      "type": "writeText",
      "content": "for test_year in range(1993,2020):\n    \n    print('-'*40)\n    print(test_year)\n    print('-'*40)\n    \n    # Load price data (3-year lookback from test year)\n    filename = 'data/Open-'+str(test_year-3)+'.csv'\n    df_open = pd.read_csv(filename)\n    filename = 'data/Close-'+str(test_year-3)+'.csv'\n    df_close = pd.read_csv(filename)\n    \n    # Create labels and get list of stocks in S&P 500\n    label = create_label(df_open,df_close)\n    stock_names = sorted(list(constituents[str(test_year-1)+'-12']))\n    train_data,test_data = [],[]\n\n    # Build feature matrices for each stock\n    start = time.time()\n    for st in stock_names:\n        st_train_data,st_test_data = create_stock_data(df_open,df_close,st)\n        train_data.append(st_train_data)\n        test_data.append(st_test_data)\n      \n    # Combine all stocks into single arrays\n    train_data = np.concatenate([x for x in train_data])\n    test_data = np.concatenate([x for x in test_data])\n    \n    # Normalize features\n    scalar_normalize(train_data,test_data)\n    print(train_data.shape,test_data.shape,time.time()-start)\n    \n    # Train model and get predictions\n    model,predictions = trainer(train_data,test_data)\n    \n    # Simulate trading strategy and calculate returns\n    returns = simulate(test_data,predictions)\n    returns.to_csv(result_folder+'/avg_daily_rets-'+str(test_year)+'.csv')\n    \n    # Calculate and display performance statistics\n    result = Statistics(returns.sum(axis=1))\n    print('\\nAverage returns prior to transaction charges')\n    result.shortreport() \n    \n    # Save results to file\n    with open(result_folder+\"/avg_returns.txt\", \"a\") as myfile:\n        res = '-'*30 + '\\n'\n        res += str(test_year) + '\\n'\n        res += 'Mean = ' + str(result.mean()) + '\\n'\n        res += 'Sharpe = '+str(result.sharpe()) + '\\n'\n        res += '-'*30 + '\\n'\n        myfile.write(res)\n"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "for test_year in range(1993,2020):",
      "voiceover": "Rolling window backtest: 1993-2019",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "test.py",
      "find": "result = Statistics(returns.sum(axis=1))",
      "voiceover": "Total daily returns (long + short)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    }
  ]
}