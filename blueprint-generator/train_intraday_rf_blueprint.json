{
  "rootFolder": "train_intraday_rf-demo",
  "globalTypingSpeed": 35,
  "actionDelay": 1000,
  "defaultVoice": "en-US-BrianNeural",
  "enableVoiceover": true,
  "actions": [
    {
      "type": "createFile",
      "path": "train_intraday_rf.py",
      "voiceover": "Let's create the train_intraday_rf.py file.",
      "voiceoverTiming": "before"
    },
    {
      "type": "openFile",
      "path": "train_intraday_rf.py"
    },
    {
      "type": "writeText",
      "content": "import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom Statistics import Statistics\n\nimport os\n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "import pandas as pd",
      "voiceover": "Importing necessary libraries for data handling, random number generation, and machine learning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "SEED = 9\n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "SEED = 9",
      "voiceover": "Setting the random seed to ensure reproducible results across runs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.environ['PYTHONHASHSEED']=str(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "random.seed(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "np.random.seed(SEED)\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "SP500_df = pd.read_csv('data/SPXconst.csv')\n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "SP500_df = pd.read_csv('data/SPXconst.csv')",
      "voiceover": "Loading the S&P 500 constituents data and creating a list of all unique companies",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "all_companies = list(set(SP500_df.values.flatten()))\n"
    },
    {
      "type": "writeText",
      "content": "all_companies.remove(np.nan)\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "constituents = {'-'.join(col.split('/')[::-1]):set(SP500_df[col].dropna()) \n                for col in SP500_df.columns}\n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "constituents = {'-'.join(col.split('/')[::-1]):set(SP500_df[col].dropna())",
      "voiceover": "Creating a dictionary to map each month to its set of S&P 500 constituents",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "constituents_train = {} \n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "constituents_train = {}",
      "voiceover": "Building a training set of constituents for each year by including companies from the previous 3 years",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for test_year in range(1993,2016):\n    months = [str(t)+'-0'+str(m) if m<10 else str(t)+'-'+str(m) \n              for t in range(test_year-3,test_year) for m in range(1,13)]\n    constituents_train[test_year] = [list(constituents[m]) for m in months]\n    constituents_train[test_year] = set([i for sublist in constituents_train[test_year] \n                                         for i in sublist])\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def trainer(train_data,test_data):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    \n    train_x,train_y = train_data[:,2:-2],train_data[:,-1]\n    train_y = train_y.astype('int')\n\n    print('Started training')\n    clf = RandomForestClassifier(n_estimators=1000,\n                                 max_depth=10, \n                                 random_state = SEED,\n                                 n_jobs=-1)\n    clf.fit(train_x,train_y)\n    print('Completed ',clf.score(train_x,train_y))\n\n    dates = list(set(test_data[:,0]))\n    predictions = {}\n    for day in dates:\n        test_d = test_data[test_data[:,0]==day]\n        test_d = test_d[:,2:-2] \n        predictions[day] = clf.predict_proba(test_d)[:,1]\n    return predictions\n",
      "highlight": true,
      "voiceover": "The trainer function implements the complete training and prediction pipeline for the Random Forest classifier, which learns to predict intraday stock price movements by analyzing patterns in historical intraday returns across multiple decision trees. This function serves as the core machine learning component of the intraday trading strategy using Random Forest instead of LSTM, offering a different approach that doesn't model temporal sequences but instead treats each stock-day observation as an independent sample with engineered features. The function begins by resetting the random seeds for both Python's random module and numpy to ensure reproducibility, which is critical for Random Forest because the algorithm involves random sampling of features and data points when building each tree, and fixing these seeds ensures that the same trees are built every time the code runs with the same data. After setting seeds, the function extracts the features and labels from the training data by slicing the numpy array: train_x contains columns 2 through second-to-last which are the approximately 30 intraday return features calculated at different lookback periods, and train_y contains the last column which is the binary label (0 or 1) indicating whether the stock was in the bottom or top 50 percent of intraday performers. The labels are explicitly cast to integer type using astype('int') to ensure compatibility with the Random Forest classifier which expects integer class labels. The function then prints a status message and creates a RandomForestClassifier object with carefully chosen hyperparameters: n_estimators equals 1000 meaning the forest will contain 1000 individual decision trees which vote together to make predictions, max_depth equals 10 which limits each tree to a maximum depth of 10 levels to prevent overfitting by ensuring trees don't become too complex and memorize training data, random_state is set to SEED for reproducibility of the random processes within the forest, and n_jobs equals -1 which tells scikit-learn to use all available CPU cores for parallel training of the trees, significantly speeding up the training process. The clf.fit call trains the Random Forest on the training data, where each tree is built by randomly sampling the data with replacement (bootstrap sampling) and randomly selecting a subset of features at each split point, creating diversity among the trees that helps the ensemble generalize better than any single tree. After training completes, the function prints the training accuracy using clf.score, which shows what percentage of training samples were correctly classified, though this is primarily for monitoring purposes and high training accuracy doesn't necessarily indicate good generalization. The function then generates predictions on the test set by first extracting all unique dates from the test data, then for each date selecting all stocks trading that day, extracting their features (columns 2 through second-to-last), and calling clf.predict_proba to get probability predictions rather than hard class assignments. The predict_proba method returns a 2D array where each row corresponds to a stock and contains two probabilities summing to 1.0, with the first column representing the probability of class 0 (bottom 50 percent) and the second column ([:,1]) representing the probability of class 1 (top 50 percent), and we extract this second column because we want to rank stocks by their predicted likelihood of being top performers. These probabilities are stored in a dictionary with dates as keys and arrays of probabilities as values, and finally the function returns this predictions dictionary which will be used by the simulate function to construct long-short portfolios by buying stocks with high predicted probabilities and shorting stocks with low predicted probabilities.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def simulate(test_data,predictions):\n    rets = pd.DataFrame([],columns=['Long','Short'])\n    k = 10\n    for day in sorted(predictions.keys()):\n        preds = predictions[day]\n        test_returns = test_data[test_data[:,0]==day][:,-2]\n        top_preds = predictions[day].argsort()[-k:][::-1]\n        trans_long = test_returns[top_preds]\n        worst_preds = predictions[day].argsort()[:k][::-1] \n        trans_short = -test_returns[worst_preds]\n        rets.loc[day] = [np.mean(trans_long),np.mean(trans_short)] \n    return rets   \n",
      "highlight": true,
      "voiceover": "The simulate function converts the Random Forest model's probability predictions into actual trading decisions and calculates the resulting returns by implementing a long-short equity strategy that buys predicted winners and shorts predicted losers each trading day. This function takes the abstract output of the machine learning model (probability scores for each stock) and translates it into concrete portfolio positions and realized profits or losses, serving as the bridge between prediction and performance evaluation. The function accepts two parameters: test_data is the numpy array containing all test set observations including dates, stock names, features, actual future returns, and labels, and predictions is a dictionary mapping each date to an array of predicted probabilities for all stocks trading on that date. The function initializes an empty pandas DataFrame called rets with two columns named 'Long' and 'Short'ntributes equally to that leg's performance. After processing all trading days in the test period, the function returns the rets DataFrame containing the complete time series of daily long and short returns, which the calling code will then sum together (long plus short returns) to get total daily strategy returns and pass to the Statistics class for comprehensive performance analysis including calculation of mean returns, Sharpe ratio, maximum drawdown, and other risk-adjusted metrics that quantify how well the Random Forest-based intraday trading strategy performed.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "top_preds = predictions[day].argsort()[-k:][::-1]",
      "voiceover": "Higlight and explain this",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def create_label(df_open,df_close,perc=[0.5,0.5]):\n    if not np.all(df_close.iloc[:,0]==df_open.iloc[:,0]):\n        print('Date Index issue')\n        return\n    perc = [0.]+list(np.cumsum(perc))\n    label = (df_close.iloc[:,1:]/df_open.iloc[:,1:]-1).apply(\n            lambda x: pd.qcut(x.rank(method='first'),perc,labels=False), axis=1)\n    return label\n",
      "highlight": true,
      "voiceover": "The create_label function generates binary classification labels for all stocks on each trading day by calculating their intraday performance (open to close return) and ranking them to identify top and bottom performers, creating the supervised learning targets that the Random Forest model will learn to predict. This function is essential for converting raw price data into a classification problem where the model learns to distinguish between stocks that will have good intraday performance versus poor intraday performance. The function takes three parameters: df_open is a DataFrame containing opening prices with dates in the first column and stock ticker symbols as column headers for subsequent columns, df_close is a similarly structured DataFrame with closing prices for the same dates and stocks, and perc is a list defining the percentile boundaries for classification with a default of [0.5, 0.5] which creates a binary split at the The qcut function with labels=False returns integer labels (0, 1) rather than interval labels, creating a clean binary classification target. The function returns this label DataFrame which contains 0s and 1s indicating whether each stock was a bottom or top performer on each day, and these labels are used throughout the training process as the target variable that the Random Forest learns to predict based on historical intraday return patterns, and during backtesting the labels help evaluate prediction accuracy and assess how well the model identifies future winners and losers.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "label = (df_close.iloc[:,1:]/df_open.iloc[:,1:]-1).apply(",
      "voiceover": "So here is something more to explain.",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def create_stock_data(df_close,df_open,st):\n    st_data = pd.DataFrame([])\n    st_data['Date'] = list(df_close['Date'])\n    st_data['Name'] = [st]*len(st_data)\n    \n    daily_change = df_close[st]/df_open[st]-1\n    m = list(range(1,20))+list(range(20,241,20))\n    for k in m:\n        st_data['IntraR'+str(k)] = df_close[st].shift(1)/df_open[st].shift(k)-1\n        \n    st_data['R-future'] = daily_change \n    st_data['label'] = list(label[st]) \n    st_data['Month'] = list(df_close['Date'].str[:-3])\n    st_data = st_data.dropna()\n    \n    trade_year = st_data['Month'].str[:4]\n    st_data = st_data.drop(columns=['Month'])\n    st_train_data = st_data[trade_year<str(test_year)]\n    st_test_data = st_data[trade_year==str(test_year)]\n    return np.array(st_train_data),np.array(st_test_data)\n",
      "highlight": true,
      "voiceover": "The create_stock_data function constructs the complete feature set and labels for a single stock by engineering historical intraday return features at multiple lookback periods, creating a dataset where each row represents a trading day with features that capture the stock's past intraday performance patterns that might predict future intraday movements.  A Month column is extracted from the date strings to facilitate train-test splitting, and dropna is called to remove rows with missing values that result from the shifting operations (the first 240 rows won't have complete historical data). The function then extracts the year from the Month column and uses it to split the data temporally, where training data includes all years before the test_year and test data includes only the test_year, ensuring proper temporal separation that prevents the model from training on future information. Finally, both datasets are converted to numpy arrays and returned as a tuple, providing the calling code with properly formatted training and test data for this stock that will be concatenated with data from other stocks to create the complete dataset for Random Forest training and evaluation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "st_data['Month'] = list(df_close['Date'].str[:-3])",
      "voiceover": "this can be used as a test voiceover to explain things which are needed to be explained. and text before this",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "result_folder = 'results-Intraday-240-1-RF'\n"
    },
    {
      "type": "writeText",
      "content": "for directory in [result_folder]:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "for test_year in range(1993,2020):\n    \n    print('-'*40)\n    print(test_year)\n    print('-'*40)\n    \n    filename = 'data/Open-'+str(test_year-3)+'.csv'\n    df_open = pd.read_csv(filename)\n    filename = 'data/Close-'+str(test_year-3)+'.csv'\n    df_close = pd.read_csv(filename)\n    \n    label = create_label(df_open,df_close)\n    stock_names = sorted(list(constituents[str(test_year-1)+'-12']))\n    train_data,test_data = [],[]\n    \n    start = time.time()\n    for st in stock_names:\n        st_train_data,st_test_data = create_stock_data(df_close,df_open,st)\n        train_data.append(st_train_data)\n        test_data.append(st_test_data)\n\n    train_data = np.concatenate([x for x in train_data])\n    test_data = np.concatenate([x for x in test_data])\n    \n    print('Created :',train_data.shape,test_data.shape,time.time()-start)\n    \n    predictions = trainer(train_data,test_data)\n    returns = simulate(test_data,predictions)\n    result = Statistics(returns.sum(axis=1))\n    print('\\nAverage returns prior to transaction charges')\n    result.shortreport() \n    \n    with open(result_folder+'/predictions-'+str(test_year)+'.pickle', 'wb') as handle:\n        pickle.dump(predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    returns.to_csv(result_folder+'/avg_daily_rets-'+str(test_year)+'.csv')\n    with open(result_folder+\"/avg_returns.txt\", \"a\") as myfile:\n        res = '-'*30 + '\\n' \n        res += str(test_year) + '\\n'\n        res += 'Mean = ' + str(result.mean()) + '\\n'\n        res += 'Sharpe = '+str(result.sharpe()) + '\\n'\n        res += '-'*30 + '\\n'\n        myfile.write(res)\n"
    },
    {
      "type": "highlight",
      "path": "train_intraday_rf.py",
      "find": "filename = 'data/Close-'+str(test_year-3)+'.csv'",
      "voiceover": "here we can put other explanations right which we can use.",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    }
  ]
}