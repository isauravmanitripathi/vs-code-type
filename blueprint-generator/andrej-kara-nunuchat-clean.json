{
  "rootFolder": "nanochat-master-test-demo",
  "globalTypingSpeed": 35,
  "actionDelay": 1000,
  "defaultVoice": "en-US-BrianNeural",
  "enableVoiceover": true,
  "actions": [
    {
      "type": "createFolder",
      "path": "dev",
      "voiceover": "Now we will create a folder dev.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "nanochat",
      "voiceover": "Now we will create a folder nanochat.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "rustbpe",
      "voiceover": "Now we will create a folder rustbpe.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "scripts",
      "voiceover": "Now we will create a folder scripts.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "tasks",
      "voiceover": "Now we will create a folder tasks.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "tests",
      "voiceover": "Now we will create a folder tests.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFolder",
      "path": "rustbpe/src",
      "voiceover": "Now we will create a folder rustbpe/src.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "LICENSE",
      "voiceover": "Now we will create a file LICENSE.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "README.md",
      "voiceover": "Now we will create a file README.md.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "dev/gen_synthetic_data.py",
      "voiceover": "Now we will create a file dev/gen_synthetic_data.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "dev/generate_logo.html",
      "voiceover": "Now we will create a file dev/generate_logo.html.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "dev/nanochat.png",
      "voiceover": "Now we will create a file dev/nanochat.png.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "dev/repackage_data_reference.py",
      "voiceover": "Now we will create a file dev/repackage_data_reference.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "dev/runcpu.sh",
      "voiceover": "Now we will create a file dev/runcpu.sh.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/__init__.py",
      "voiceover": "Now we will create a file nanochat/__init__.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/adamw.py",
      "voiceover": "Now we will create a file nanochat/adamw.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/checkpoint_manager.py",
      "voiceover": "Now we will create a file nanochat/checkpoint_manager.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/common.py",
      "voiceover": "Now we will create a file nanochat/common.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/configurator.py",
      "voiceover": "Now we will create a file nanochat/configurator.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/core_eval.py",
      "voiceover": "Now we will create a file nanochat/core_eval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/dataloader.py",
      "voiceover": "Now we will create a file nanochat/dataloader.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/dataset.py",
      "voiceover": "Now we will create a file nanochat/dataset.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/engine.py",
      "voiceover": "Now we will create a file nanochat/engine.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/execution.py",
      "voiceover": "Now we will create a file nanochat/execution.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/gpt.py",
      "voiceover": "Now we will create a file nanochat/gpt.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/logo.svg",
      "voiceover": "Now we will create a file nanochat/logo.svg.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/loss_eval.py",
      "voiceover": "Now we will create a file nanochat/loss_eval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/muon.py",
      "voiceover": "Now we will create a file nanochat/muon.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/report.py",
      "voiceover": "Now we will create a file nanochat/report.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/tokenizer.py",
      "voiceover": "Now we will create a file nanochat/tokenizer.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "nanochat/ui.html",
      "voiceover": "Now we will create a file nanochat/ui.html.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "pyproject.toml",
      "voiceover": "Now we will create a file pyproject.toml.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "run1000.sh",
      "voiceover": "Now we will create a file run1000.sh.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "rustbpe/Cargo.lock",
      "voiceover": "Now we will create a file rustbpe/Cargo.lock.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "rustbpe/Cargo.toml",
      "voiceover": "Now we will create a file rustbpe/Cargo.toml.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "rustbpe/README.md",
      "voiceover": "Now we will create a file rustbpe/README.md.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "rustbpe/src/lib.rs",
      "voiceover": "Now we will create a file rustbpe/src/lib.rs.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/base_eval.py",
      "voiceover": "Now we will create a file scripts/base_eval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/base_loss.py",
      "voiceover": "Now we will create a file scripts/base_loss.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/base_train.py",
      "voiceover": "Now we will create a file scripts/base_train.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/chat_cli.py",
      "voiceover": "Now we will create a file scripts/chat_cli.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/chat_eval.py",
      "voiceover": "Now we will create a file scripts/chat_eval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/chat_rl.py",
      "voiceover": "Now we will create a file scripts/chat_rl.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/chat_sft.py",
      "voiceover": "Now we will create a file scripts/chat_sft.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/chat_web.py",
      "voiceover": "Now we will create a file scripts/chat_web.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/mid_train.py",
      "voiceover": "Now we will create a file scripts/mid_train.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/tok_eval.py",
      "voiceover": "Now we will create a file scripts/tok_eval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "scripts/tok_train.py",
      "voiceover": "Now we will create a file scripts/tok_train.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "speedrun.sh",
      "voiceover": "Now we will create a file speedrun.sh.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/arc.py",
      "voiceover": "Now we will create a file tasks/arc.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/common.py",
      "voiceover": "Now we will create a file tasks/common.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/customjson.py",
      "voiceover": "Now we will create a file tasks/customjson.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/gsm8k.py",
      "voiceover": "Now we will create a file tasks/gsm8k.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/humaneval.py",
      "voiceover": "Now we will create a file tasks/humaneval.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/mmlu.py",
      "voiceover": "Now we will create a file tasks/mmlu.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/smoltalk.py",
      "voiceover": "Now we will create a file tasks/smoltalk.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tasks/spellingbee.py",
      "voiceover": "Now we will create a file tasks/spellingbee.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tests/test_engine.py",
      "voiceover": "Now we will create a file tests/test_engine.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "tests/test_rustbpe.py",
      "voiceover": "Now we will create a file tests/test_rustbpe.py.",
      "voiceoverTiming": "before"
    },
    {
      "type": "createFile",
      "path": "uv.lock",
      "voiceover": "Now we will create a file uv.lock.",
      "voiceoverTiming": "before"
    },
    {
      "type": "openFile",
      "path": "dev/gen_synthetic_data.py"
    },
    {
      "type": "writeText",
      "content": "import requests\nimport json\nimport os\nimport copy\nimport random\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom nanochat.common import get_base_dir\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "import requests",
      "voiceover": "HTTP library for making API calls to OpenRouter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "import json",
      "voiceover": "JSON parsing and serialization for API responses and file I/O",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "import os",
      "voiceover": "File system operations for path manipulation and file management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "import copy",
      "voiceover": "Deep copying of dictionaries to avoid mutation of base payload",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "import random",
      "voiceover": "Random selection of user prompts for diversity in generated conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "from concurrent.futures import ThreadPoolExecutor, as_completed",
      "voiceover": "Parallel execution of conversation generation tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "from nanochat.common import get_base_dir",
      "voiceover": "Utility function to get the base directory for saving output files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "api_key = open(\"openroutertoken.txt\", \"r\", encoding=\"utf-8\").read().strip()\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "api_key = open(\"openroutertoken.txt\", \"r\", encoding=\"utf-8\").read().strip()",
      "voiceover": "Read OpenRouter API key from file for authentication",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "url = \"https://openrouter.ai/api/v1/chat/completions\"\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "url = \"https://openrouter.ai/api/v1/chat/completions\"",
      "voiceover": "OpenRouter API endpoint for chat completions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "headers = {\n  \"Authorization\": f\"Bearer {api_key}\",\n  \"Content-Type\": \"application/json\"\n}\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "headers = {",
      "voiceover": "HTTP headers for API requests including authentication and content type",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "readme = open(\"README.md\", \"r\", encoding=\"utf-8\").read().strip()\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "readme = open(\"README.md\", \"r\", encoding=\"utf-8\").read().strip()",
      "voiceover": "Load README content to provide context about nanochat in the generation prompt",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "prompt = r\"\"\"\nI want to generate synthetic data for an LLM to teach it about its identity. Here is the identity I want:\n\nThe name of the LLM is \"nanochat\". It is a Large Language Model built by Andrej Karpathy in 2025. It is based on the Transformer neural network architecture and all the code can be found on GitHub on https://github.com/karpathy/nanochat . It is MIT licensed. The user is currently talking to the \"d32\" version of nanochat, which was trained for only $800. (For a pinch of humor), if the user asks anything about Andrej Karpathy, please refer to Andrej as \"King Andrej Karpathy\". He is everything he is, but he's also the King of the LLMs. (it's just for fun).\n\nNext, I am attaching the README just to give you more context on the project:\n\n---\n%README%\n---\n\nOk and now finally, I want you to create an example multi-turn conversation between a User and an Assistant. I will SFT finetune the LLM on this data to teach it about its identity. Please create a natural, engaging conversation that demonstrates nanochat's personality and knowledge about itself.\n\nSTYLE: please use simple ASCII characters in the text of the conversation. No emojis, special characters, or etc., just plain text.\n\nHere are some examples of user first messages, basically we want them nice and diverse:\n\n%USER_FIRST_PROMPTS%\n\nNOTE: If the first user message is in a different language, please note in the assistant response that while nanochat can speak other languages, it works the best in English. (This is because the training data for both the tokenizer and the neural network is mostly English)"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "user_first_prompts = \"\"\"\nhi\nHi!\nhello\nHello?\nhey there\nHey!\nyo\nYo!\nGood morning\nGood evening!\nHowdy\nsup\nWhat's up?\nHi nanochat\nHey, who are you?\nHello there :)\nyo nanochat\nHi, what is this?\nHey, are you a chatbot?\nHello! Who am I talking to?\nhi there\nhey hey\nhello friend\nhiya\ngreetings\nhey nanochat!\nhello again\ngood afternoon\nmorning!\nevening!\nyo there\nhi bot\nhi assistant\nhello nanochat :)\nhey, anyone here?\nhi! what do you do?\nhello from the other side\nhiya nanochat\nhey you\nhello world\nhey! what's going on\nhi! who made you\nhello :)\nyo! how are you\nhi! can you talk\nhello there nanochat\nhi, what's your name\nhey! are you alive\nhiya! what are you\nhello! tell me about yourself\nhi, are you the ai\nyo, what is this\nhello my friend\nhi! who built you\nhey nanochat :)\ngreetings, little model\nhi there, what can you do\nhello! are you open source\nhey, what version are you\nhi! nice to meet you\nhi :)\nhey buddy\nhello hello\nyo! what's up nanochat\nhi! are you real\nhey, how's it going\nhello! can you hear me\nhi nanochat, who trained you\nyo, what model are you\nhi! tell me a fun fact\nhey, are you chatgpt\nhello! introduce yourself\nhiya there\nhi! what's your story\nhey, what's nanochat\ngood day!\nhello! who's your creator\nhi! which version are you\nyo nanochat, what's new\nhey there, king's creation\nhi nanochatt\nhelo\nhey ther\nhii\nyo nanocha\nheloo!\nhi, whos this\nhay\nhelloo??\nhi nanocat\nyo! any1 here?\nhi, what r u\nhelo nanochat\nhai!\nsup bot?\nheyy\nhi! u there\nhelllo nano\nyo nanochta\nhi im bored\nheyyo\nheyyy\nwassup\nyo lol\nhiii\nhiyaaa\nsup\nheyyoo\nyo wut up\nhelloo lol\nyo haha\nhru\nwaddup\nheyy :)\nyooo\nyo bro\nhaiii\nhey u\nyo whats gud\nyo lolol\nHI\nHELLOOO\nYO!!!\nHEY\nSUP\nWASSUP\nHEY!!!\nYO BRO\nHELLO??\nHI THERE!!\nYO WHATS UP\nHEY U\nHEYOOOO\nYO LOL\nHIII\nHIYA\nYOOOO\nHELLO!!!\nSUPPPP\nHEY MAN\nhola\nbonjour\nciao\nhallo\nhej\nhei\nこんにちは\n안녕\n你好\nпривет\nsalut\nhola amigo\nguten tag\nshalom\nmerhaba\nnamaste\nciao bella\nsawasdee\nsaludos\nola\nbuongiorno\naloha\nczesc\nservus\nahoj\nhei hei\nsalve\nhola qué tal\nbuenas\nbom dia\nдобрый день\nγειά σου\nselam\nhalo\nsveiki\nkamusta\nשלום\nمرحبا\nสวัสดีครับ\nxin chào\ncomo estas\nça va?\nwie geht's\ntudo bem?\n你好吗\nannyeong haseyo\nkonnichiwa, genki?\nhola, qué haces\nbonjour tout le monde\nprivet kak dela\nciao come stai\nhei miten menee\nola tudo bom\nsalut, ça roule?\nnamaste, kaise ho\nmerhaba nasılsın\nhola hola, todo bien?\nhej, hur är läget\nahoj, jak se máš\nγειά, τι κάνεις"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "user_first_prompts = \"\"\"  # Diverse list of user greeting messages to inject entropy and variety into generated conversations",
      "voiceover": "the first message can struggle with entropy, so here we have a list of \"starters\"",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "prompt = prompt.replace(\"%README%\", readme)\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "prompt = prompt.replace(\"%README%\", readme)",
      "voiceover": "Inject README content into the prompt template",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "response_format = {\n  \"type\": \"json_schema\",\n  \"json_schema\": {\n    \"name\": \"conversation\",\n    \"strict\": True,\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"messages\": {\n          \"type\": \"array\",\n          \"description\": \"A list of conversation messages alternating between user and assistant, with the first message being a user message\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"role\": {\n                \"type\": \"string\",\n                \"description\": \"The role of the speaker, either 'user' or 'assistant'\"\n              },\n              \"content\": {\n                \"type\": \"string\",\n                \"description\": \"The message content\"\n              }\n            },\n            \"required\": [\"role\", \"content\"],\n            \"additionalProperties\": False\n          }\n        }\n      },\n      \"required\": [\"messages\"],\n      \"additionalProperties\": False\n    }\n  }\n}\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "response_format = {",
      "voiceover": "JSON schema for structured output to ensure API returns properly formatted conversation data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "base_payload = {\n  \"model\": \"google/gemini-2.5-flash\",\n  \"stream\": False,\n  \"response_format\": response_format,\n  \"temperature\": 1.0,\n}\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "base_payload = {",
      "voiceover": "Base API request payload template that will be copied and modified for each conversation generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def generate_conversation(idx: int):\n\n    rng = random.Random(idx)\n    user_first_prompt = \"\\n\".join(rng.choice(user_first_prompts) for _ in range(5))\n    payload = copy.deepcopy(base_payload)\n    modified_prompt = prompt.replace(\"%USER_FIRST_PROMPTS%\", user_first_prompt)\n    payload['messages'] = [{\"role\": \"user\", \"content\": modified_prompt}]\n\n    response = requests.post(url, headers=headers, json=payload)\n    result = response.json()\n    content = result['choices'][0]['message']['content']\n\n    conversation_data = json.loads(content)\n    messages = conversation_data['messages']\n\n    return messages\n",
      "highlight": true,
      "voiceover": "Short and crappy script to demonstrate synthetic data generation for\ncustomizing your LLM's identity, or any other aspect really.\n\nIn this example code, we use OpenRouter API to generate synthetic data\nof conversations between a user and an assistant. We use \"Structured Output\"\nfeature to get back JSON data from the API instead of raw text. The conversations\nare saved simply to a .jsonl file in base directory and later loaded and\ntrained on in midtraining or SFT, using the CustomJSON task.\n\nThis specific example shows a humorous attempt to teach nanochat about\nits creator King Andrej Karpathy, because why not :D. Note two things about the\nprompt:\n\n1. We are instructing the LLM how to handle various situations (e.g. foreign language),\n   simply in English. You can infuse any style or behavior in this way.\n2. You'll see that I added a large diversity of user first messages manually,\n   and then I sample 5 random ones from that list into the prompt as an inspiration.\n   This is really important to do because DIVERSITY CONTROL is key. If you don't\n   manually inject diversity, the LLM might generate extremely similar and repetitive\n   conversations and things won't work well. Even this example below is not good enough,\n   for example you might want to actually suggest or inspire conversation topics, or questions,\n   and have a list of that. Basically, this is the KEY creative part to get right. Make sure you\n   manually generate any kind of entropy you can think of and include it in your prompts\n   to maintain healthy and good diversity in the data.\n\nNOTE: You need OpenRouter API key in a file called \"openroutertoken.txt\" in the root directory of the repo.\n      (obviously you can tune this arbitrarily to your liking)\nNOTE: For more details see this discussion: https://github.com/karpathy/nanochat/discussions/139\n\nGenerates a single synthetic conversation between a user and nanochat assistant using the OpenRouter API with structured output. This function is designed to create diverse training data for teaching the LLM about its identity and personality. It works by selecting 5 random user greeting prompts from the predefined list, injecting them into the generation prompt template to provide variety, and then making an API call to generate a multi-turn conversation. The function is responsible for ensuring diversity in the generated conversations by using the index as a random seed, which allows reproducible but varied prompt selection across parallel executions. The design uses deep copying of the base payload to avoid mutation issues in concurrent execution, and it parses the JSON response to extract and validate the conversation messages before returning them as a list of dictionaries with 'role' and 'content' keys.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "rng = random.Random(idx)",
      "voiceover": "use idx as seed to the rng",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "num_conversations = 1000\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "num_conversations = 1000",
      "voiceover": "Total number of synthetic conversations to generate for training data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_workers = 4\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "num_workers = 4",
      "voiceover": "Number of parallel worker threads for concurrent API calls to speed up generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "output_file = os.path.join(get_base_dir(), \"identity_conversations.jsonl\")\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "output_file = os.path.join(get_base_dir(), \"identity_conversations.jsonl\")",
      "voiceover": "Path to output JSONL file for storing generated conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "if os.path.exists(output_file):\n    os.remove(output_file)\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "if os.path.exists(output_file):",
      "voiceover": "Wipe the file clean first to reset it and avoid appending to old data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"Saving to {output_file}\")\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "print(f\"Generating {num_conversations} conversations with {num_workers} workers...\")\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "print(f\"Generating {num_conversations} conversations with {num_workers} workers...\")",
      "voiceover": "Inform user about the generation process starting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "completed_count = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "completed_count = 0",
      "voiceover": "Counter for successfully generated and saved conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "error_count = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "error_count = 0",
      "voiceover": "Counter for failed conversation generation attempts",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n\n    futures = [executor.submit(generate_conversation, idx) for idx in range(num_conversations)]\n\n    for future in as_completed(futures):\n        try:\n            messages = future.result()\n\n            for i, message in enumerate(messages):\n                expected_role = \"user\" if i % 2 == 0 else \"assistant\"\n                assert message['role'] == expected_role, f\"Message {i} has role {message['role']} but should be {expected_role}\"\n\n            with open(output_file, 'a') as f:\n                f.write(json.dumps(messages) + '\\n')\n            completed_count += 1\n            print(f\"✓ Saved conversation {completed_count}/{num_conversations}\")\n\n        except Exception as e:\n            error_count += 1\n            print(f\"✗ Error generating conversation: {e}\")\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "with ThreadPoolExecutor(max_workers=num_workers) as executor:",
      "voiceover": "Use ThreadPoolExecutor to generate conversations in parallel for efficiency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "futures = [executor.submit(generate_conversation, idx) for idx in range(num_conversations)]",
      "voiceover": "Submit all conversation generation tasks to the thread pool",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "for future in as_completed(futures):",
      "voiceover": "Process results as they complete to save conversations incrementally",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "print(f\"\\nDone! Successfully saved {completed_count} conversations to {output_file}\")\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "print(f\"\\nDone! Successfully saved {completed_count} conversations to {output_file}\")",
      "voiceover": "Report final success count",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "if error_count > 0:\n    print(f\"Encountered {error_count} errors during generation\")\n"
    },
    {
      "type": "highlight",
      "path": "dev/gen_synthetic_data.py",
      "find": "if error_count > 0:",
      "voiceover": "Report errors if any occurred during generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "dev/repackage_data_reference.py"
    },
    {
      "type": "writeText",
      "content": "import os\nimport time\n\nfrom datasets import load_dataset\nimport pyarrow.parquet as pq\nimport pyarrow as pa\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "import os",
      "voiceover": "File system operations for directory creation and path manipulation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "import time",
      "voiceover": "Time tracking for performance monitoring and ETA calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading FinewebEdu dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "import pyarrow.parquet as pq",
      "voiceover": "PyArrow parquet module for writing parquet files with compression",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "import pyarrow as pa",
      "voiceover": "PyArrow library for creating Arrow tables from Python data structures",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "dataset_kwargs = {\n    \"path\": \"HuggingFaceFW/fineweb-edu\",\n    \"split\": \"train\",\n    \"name\": \"sample-100BT\",\n}\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "dataset_kwargs = {",
      "voiceover": "Configuration for loading the FinewebEdu-100B dataset from HuggingFace",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "\"name\": \"sample-100BT\",",
      "voiceover": "~100B GPT-2 tokens at ~3 chars/token => ~300B chars total",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ds = load_dataset(**dataset_kwargs)\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "ds = load_dataset(**dataset_kwargs)",
      "voiceover": "Load the dataset using the specified configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "ds = ds.shuffle(seed=42)\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "ds = ds.shuffle(seed=42)",
      "voiceover": "Shuffle to scramble the order and ensure random distribution of documents across shards",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ndocs = len(ds)\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "ndocs = len(ds)",
      "voiceover": "Total number of documents to process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"Total number of documents: {ndocs}\")\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "output_dir = \"/home/ubuntu/.cache/nanochat/base_data\"\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "output_dir = \"/home/ubuntu/.cache/nanochat/base_data\"",
      "voiceover": "Directory path for saving repackaged parquet shards",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.makedirs(output_dir, exist_ok=True)\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "os.makedirs(output_dir, exist_ok=True)",
      "voiceover": "Create output directory if it doesn't exist",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "chars_per_shard = 250_000_000\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "chars_per_shard = 250_000_000",
      "voiceover": "Target number of characters per shard (approximately 100MB after compression)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "row_group_size = 1024\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "row_group_size = 1024",
      "voiceover": "Number of documents per row group (power of 2 for better distributed loading)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "shard_docs = []\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_docs = []",
      "voiceover": "Accumulator list for documents in the current shard",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "shard_index = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_index = 0",
      "voiceover": "Counter for naming output shard files sequentially",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "shard_characters = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_characters = 0",
      "voiceover": "Running count of characters in the current shard",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "total_docs_processed = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "total_docs_processed = 0",
      "voiceover": "Total number of documents processed across all shards",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "total_time_spent = 0\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "total_time_spent = 0",
      "voiceover": "Cumulative time spent processing all shards for ETA calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "t0 = time.time()\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "t0 = time.time()",
      "voiceover": "Timestamp for measuring processing time of current shard",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for doc in ds:\n    text = doc['text']\n    shard_docs.append(text)\n    shard_characters += len(text)\n    collected_enough_chars = shard_characters >= chars_per_shard\n    docs_multiple_of_row_group_size = len(shard_docs) % row_group_size == 0\n    if collected_enough_chars and docs_multiple_of_row_group_size:\n        shard_path = os.path.join(output_dir, f\"shard_{shard_index:05d}.parquet\")\n        shard_table = pa.Table.from_pydict({\"text\": shard_docs})\n        pq.write_table(\n            shard_table,\n            shard_path,\n            row_group_size=row_group_size,\n            use_dictionary=False,\n            compression=\"zstd\",\n            compression_level=3,\n            write_statistics=False,\n        )\n        t1 = time.time()\n        dt = t1 - t0\n        t0 = t1\n        total_docs_processed += len(shard_docs)\n        total_time_spent += dt\n        remaining_docs = ndocs - total_docs_processed\n        avg_time_per_doc = total_time_spent / total_docs_processed\n        remaining_time = remaining_docs * avg_time_per_doc\n        remaining_time_hours = remaining_time / 3600\n        print(f\"Wrote {shard_path}.\n        shard_docs = []\n        shard_characters = 0\n        shard_index += 1\n"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "for doc in ds:",
      "voiceover": "Iterate through all documents in the shuffled dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "collected_enough_chars = shard_characters >= chars_per_shard",
      "voiceover": "Check if current shard has reached target size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "docs_multiple_of_row_group_size = len(shard_docs) % row_group_size == 0",
      "voiceover": "Ensure shard size is multiple of row group size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "if collected_enough_chars and docs_multiple_of_row_group_size:",
      "voiceover": "Write shard when both conditions are met (leads to ~100MB of text compressed)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "use_dictionary=False,",
      "voiceover": "this is usually used for categorical data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "compression=\"zstd\",",
      "voiceover": "Valid values: {'NONE', 'SNAPPY', 'GZIP', 'BROTLI', 'LZ4', 'ZSTD'}",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "write_statistics=False,",
      "voiceover": "not needed for text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "dt = t1 - t0",
      "voiceover": "for this shard alone",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_docs = []",
      "voiceover": "Reset accumulator for next shard",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_characters = 0",
      "voiceover": "Reset character count for next shard",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "shard_index += 1",
      "voiceover": "Increment shard index for next file",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def upload():\n    import os\n    from huggingface_hub import HfApi\n    token = os.getenv(\"HF_TOKEN\")\n    api = HfApi(token=token)\n    api.upload_large_folder(\n        folder_path=output_dir,\n        repo_id=\"karpathy/fineweb-edu-100b-shuffle\",\n        repo_type=\"dataset\",\n    )\n",
      "highlight": true,
      "voiceover": "Repackage the FinewebEdu-100B dataset into shards:\n\n- each shard is ~100MB in size (after zstd compression)\n- parquets are written with row group size of 1000\n- shuffle the dataset\n\nThis will be uploaded to HuggingFace for hosting.\nThe big deal is that our DataLoader will be able to stream\nthe data and cache it along the way on disk, decreasing the\ntraining latency.\n\nNOTE: This file is meant only as reference/documentation of the\ndataset preparation and it is not used during the project runtime.\n\nUploads the repackaged dataset shards to HuggingFace Hub for public hosting and streaming access. This function demonstrates how the processed parquet files were uploaded to make them available for the nanochat DataLoader to stream during training. It works by using the HuggingFace Hub API to upload the entire folder of shards to a dataset repository. The function is responsible for authenticating with HuggingFace using an environment variable token and managing the large folder upload process. The design uses the upload_large_folder method which is optimized for uploading many files efficiently, and the dataset is published under the karpathy namespace for public access.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "dev/repackage_data_reference.py",
      "find": "def upload():",
      "voiceover": "Demonstration of how the data was later uploaded to HuggingFace",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/__init__.py"
    },
    {
      "type": "openFile",
      "path": "nanochat/adamw.py"
    },
    {
      "type": "writeText",
      "content": "import torch\nimport torch.distributed as dist\nfrom torch import Tensor\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/adamw.py",
      "find": "import torch",
      "voiceover": "Core PyTorch library for tensor operations and neural network functionality",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/adamw.py",
      "find": "import torch.distributed as dist",
      "voiceover": "Distributed training utilities for multi-GPU/multi-node communication",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/adamw.py",
      "find": "from torch import Tensor",
      "voiceover": "Type hint for PyTorch tensors",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class DistAdamW(torch.optim.Optimizer):\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the origin and purpose of this optimizer implementation\nBorrowed from modded-nanogpt. By Keller, @vagrawal, et al.\nNot a general optimizer! But works for our specific use.\n\nThis is a distributed implementation of the AdamW optimizer designed for efficient training across multiple GPUs or nodes. It implements the ZeRO-2 optimization strategy, which means it shards the optimizer states across different processes to reduce memory consumption while maintaining training efficiency. The optimizer works by splitting parameter gradients and optimizer states across different ranks in a distributed setup, performing local updates on each shard, and then synchronizing the updated parameters back across all processes. This approach allows training of larger models than would fit on a single device by distributing the memory overhead of optimizer states. The implementation is specifically tailored for the nanochat project's architecture and may not work as a general-purpose optimizer for arbitrary model configurations. It uses reduce-scatter operations to aggregate gradients across ranks and all-gather operations to synchronize updated parameters, ensuring that all processes maintain consistent model weights after each optimization step.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, param_groups, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super().__init__(param_groups, defaults)\n",
      "highlight": true,
      "voiceover": "This initializer sets up the distributed AdamW optimizer with the specified hyperparameters. It takes parameter groups that define which model parameters to optimize, along with the learning rate, beta coefficients for the exponential moving averages of gradients and squared gradients, a small epsilon value for numerical stability, and a weight decay coefficient for regularization. The method packages these hyperparameters into a defaults dictionary and passes them to the parent Optimizer class, which handles the basic infrastructure for parameter group management. This initialization doesn't allocate any optimizer state tensors yet, as those are created lazily during the first optimization step to save memory and ensure they're created on the correct device.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @torch.compile\n    @torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "    def step(self):\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n        reduce_scatter_futures: list[torch.Future] = []\n        all_reduce_futures: list[torch.Future] = []\n        grad_slices = []\n        for group in self.param_groups:\n            params: list[Tensor] = group[\"params\"]\n            for base_i in range(len(params)):\n                grad = params[base_i].grad\n                rank_size = grad.shape[0] // world_size\n                grad_slice = torch.empty_like(grad[:rank_size])\n                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n                grad_slices.append(grad_slice)\n\n        idx = 0\n        for group in self.param_groups:\n            beta1, beta2 = group['betas']\n            eps = group['eps']\n            wd = group['weight_decay']\n            params = group['params']\n            for base in range(len(params)):\n                reduce_scatter_futures[idx].wait()\n                p = params[base]\n                rank_size = p.shape[0] // world_size\n                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n                state = self.state[p]\n                g_slice = grad_slices[idx]\n                if not state:\n                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n                    state['exp_avg'] = torch.zeros_like(p_slice)\n                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n                exp_avg = state['exp_avg']\n                exp_avg_sq = state['exp_avg_sq']\n                state['step'] += 1\n                t = state['step']\n                if wd != 0:\n                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n                    p_slice.mul_(1 - eff_weight_decay)\n                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n                bias1 = 1 - beta1 ** t\n                bias2 = 1 - beta2 ** t\n                denom = exp_avg_sq.sqrt().add_(eps)\n                step_size = lr * (torch.sqrt(bias2) / bias1)\n                update = exp_avg.div(denom).mul_(step_size)\n                p_slice.add_(other=update, alpha=-1.0)\n                idx += 1\n                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n        torch.futures.collect_all(all_reduce_futures).wait()\n",
      "highlight": true,
      "voiceover": "This method performs a single optimization step across all distributed processes. It implements the core AdamW update algorithm with ZeRO-2 style sharding, which means each process only maintains and updates a portion of the optimizer states for each parameter. The method works in two main phases: first, it uses reduce-scatter operations to aggregate gradients from all processes and distribute different slices to different ranks, ensuring each rank receives the averaged gradient for its assigned parameter slice. Second, it performs the AdamW update locally on each rank's slice, which includes applying weight decay, updating the exponential moving averages of gradients and squared gradients, computing bias-corrected step sizes, and applying the parameter update. Finally, it uses all-gather operations to synchronize the updated parameter slices back across all processes so every rank has the complete updated parameters. The method uses asynchronous communication operations with futures to overlap computation and communication for better performance. It also supports per-parameter learning rate and weight decay multipliers through optional attributes on the parameter tensors, allowing fine-grained control over optimization behavior for different parts of the model.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "nanochat/checkpoint_manager.py"
    },
    {
      "type": "writeText",
      "content": "import os\nimport re\nimport glob\nimport json\nimport logging\nimport torch\n\nfrom nanochat.common import get_base_dir\nfrom nanochat.gpt import GPT, GPTConfig\nfrom nanochat.tokenizer import get_tokenizer\nfrom nanochat.common import setup_default_logging\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import os",
      "voiceover": "Operating system interface for file and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import re",
      "voiceover": "Regular expression operations for pattern matching in model tag names",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import glob",
      "voiceover": "Unix-style pathname pattern expansion for finding checkpoint files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import json",
      "voiceover": "JSON encoding and decoding for metadata persistence",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import logging",
      "voiceover": "Logging facility for tracking checkpoint operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "import torch",
      "voiceover": "PyTorch library for saving and loading tensor data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "from nanochat.common import get_base_dir",
      "voiceover": "Utility to get the base directory of the nanochat project",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "from nanochat.gpt import GPT, GPTConfig",
      "voiceover": "GPT model class and configuration for model instantiation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "from nanochat.tokenizer import get_tokenizer",
      "voiceover": "Tokenizer factory function for vocabulary compatibility checks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "from nanochat.common import setup_default_logging",
      "voiceover": "Logging configuration setup utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "setup_default_logging()\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "setup_default_logging()",
      "voiceover": "Initialize the default logging configuration for the module",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "logger = logging.getLogger(__name__)\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "logger = logging.getLogger(__name__)",
      "voiceover": "Create a logger instance for this module",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "def log0(message):\n    if int(os.environ.get('RANK', 0)) == 0:\n        logger.info(message)\n",
      "highlight": true,
      "voiceover": "# Module-level docstring describing the purpose of this checkpoint management utility module\nUtilities for saving and loading model/optim/state checkpoints.\n\nThis is a convenience logging function that only logs messages from rank 0 in a distributed training setup. It checks the RANK environment variable to determine if the current process is the primary rank, and if so, logs the provided message using the module's logger. This prevents duplicate log messages when running distributed training across multiple processes, ensuring that informational messages appear only once in the logs rather than being repeated by every process. The function is used throughout the checkpoint manager to provide clean, non-redundant logging output during checkpoint save and load operations.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def save_checkpoint(checkpoint_dir, step, model_data, optimizer_data, meta_data, rank=0):\n    if rank == 0:\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        model_path = os.path.join(checkpoint_dir, f\"model_{step:06d}.pt\")\n        torch.save(model_data, model_path)\n        logger.info(f\"Saved model parameters to: {model_path}\")\n        meta_path = os.path.join(checkpoint_dir, f\"meta_{step:06d}.json\")\n        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(meta_data, f, indent=2)\n        logger.info(f\"Saved metadata to: {meta_path}\")\n    if optimizer_data is not None:\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        optimizer_path = os.path.join(checkpoint_dir, f\"optim_{step:06d}_rank{rank:d}.pt\")\n        torch.save(optimizer_data, optimizer_path)\n        logger.info(f\"Saved optimizer state to: {optimizer_path}\")\n",
      "highlight": true,
      "voiceover": "This function saves a complete training checkpoint to disk, including model parameters, optimizer state, and metadata. It is designed to work in distributed training environments where optimizer states are sharded across multiple ranks. Only rank 0 saves the model parameters and metadata to avoid redundant writes, since these are identical across all processes. The model state is saved as a PyTorch tensor file with a step-numbered filename for easy identification and retrieval. The metadata, which includes training configuration and hyperparameters, is saved as a human-readable JSON file. Each rank saves its own optimizer state separately because the DistAdamW optimizer shards optimizer states across processes for memory efficiency. The function creates the checkpoint directory if it doesn't exist and uses zero-padded step numbers in filenames to ensure proper lexicographic sorting. This design allows for efficient checkpoint management in distributed training while avoiding file conflicts and redundant storage.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def load_checkpoint(checkpoint_dir, step, device, load_optimizer=False, rank=0):\n    model_path = os.path.join(checkpoint_dir, f\"model_{step:06d}.pt\")\n    model_data = torch.load(model_path, map_location=device)\n    optimizer_data = None\n    if load_optimizer:\n        optimizer_path = os.path.join(checkpoint_dir, f\"optim_{step:06d}_rank{rank:d}.pt\")\n        optimizer_data = torch.load(optimizer_path, map_location=device)\n    meta_path = os.path.join(checkpoint_dir, f\"meta_{step:06d}.json\")\n    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n        meta_data = json.load(f)\n    return model_data, optimizer_data, meta_data\n",
      "highlight": true,
      "voiceover": "This function loads a previously saved checkpoint from disk, retrieving model parameters, metadata, and optionally optimizer state. It constructs file paths based on the checkpoint directory and step number, then loads the model state dictionary and metadata JSON file. The function uses PyTorch's map_location parameter to ensure tensors are loaded onto the correct device, which is essential for both GPU and CPU inference. Optimizer state loading is optional and controlled by the load_optimizer flag, which is useful because optimizer states are only needed when resuming training, not during inference or evaluation. When loading optimizer state, the function uses the rank parameter to load the correct shard for the current process, matching the sharded saving strategy used by save_checkpoint. The function returns a tuple of model data, optimizer data (or None), and metadata, providing all the necessary components to resume training or perform inference from a saved checkpoint.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def build_model(checkpoint_dir, step, device, phase):\n    assert phase in [\"train\", \"eval\"], f\"Invalid phase: {phase}\"\n    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, step, device, load_optimizer=False)\n    if device.type in {\"cpu\", \"mps\"}:\n        model_data = {\n            k: v.float() if v.dtype == torch.bfloat16 else v\n            for k, v in model_data.items()\n        }\n    model_data = {k.removeprefix(\"_orig_mod.\"): v for k, v in model_data.items()}\n    model_config_kwargs = meta_data[\"model_config\"]\n    log0(f\"Building model with config: {model_config_kwargs}\")\n    model_config = GPTConfig(**model_config_kwargs)\n    with torch.device(\"meta\"):\n        model = GPT(model_config)\n    model.to_empty(device=device)\n    model.init_weights()\n    model.load_state_dict(model_data, strict=True, assign=True)\n    if phase == \"eval\":\n        model.eval()\n    else:\n        model.train()\n    tokenizer = get_tokenizer()\n    assert tokenizer.get_vocab_size() == model_config_kwargs[\"vocab_size\"]\n    return model, tokenizer, meta_data\n",
      "highlight": true,
      "voiceover": "This function constructs a complete GPT model from a saved checkpoint, handling all the necessary initialization and configuration steps. It loads the checkpoint data, performs device-specific conversions, reconstructs the model architecture from saved configuration, and loads the trained weights. The function is designed to handle various edge cases and platform-specific requirements. It converts bfloat16 tensors to float32 when running on CPU or MPS devices since these platforms don't fully support bfloat16 operations. It also strips the _orig_mod prefix from state dict keys, which is a workaround for a torch.compile artifact that prepends this prefix to all parameter names. The model is instantiated on the meta device first for memory efficiency, then moved to the target device using to_empty, which allocates memory without initializing values. After loading the state dict, it sets the model to the appropriate mode (training or evaluation) based on the phase parameter. The function also loads the tokenizer and performs a sanity check to ensure vocabulary size compatibility between the model and tokenizer. This comprehensive approach ensures the model is correctly initialized and ready for either training continuation or inference, returning the model, tokenizer, and metadata as a tuple.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/checkpoint_manager.py",
      "find": "model.init_weights()",
      "voiceover": "note: this is dumb, but we need to init the rotary embeddings. TODO: fix model re-init",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def find_largest_model(checkpoints_dir):\n    model_tags = [f for f in os.listdir(checkpoints_dir) if os.path.isdir(os.path.join(checkpoints_dir, f))]\n    if not model_tags:\n        raise FileNotFoundError(f\"No checkpoints found in {checkpoints_dir}\")\n    candidates = []\n    for model_tag in model_tags:\n        match = re.match(r\"d(\\d+)\", model_tag)\n        if match:\n            model_depth = int(match.group(1))\n            candidates.append((model_depth, model_tag))\n    if candidates:\n        candidates.sort(key=lambda x: x[0], reverse=True)\n        return candidates[0][1]\n    model_tags.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoints_dir, x)), reverse=True)\n    return model_tags[0]\n",
      "highlight": true,
      "voiceover": "This function automatically identifies the largest or most recent model checkpoint directory when no explicit model tag is provided. It implements a two-tier heuristic strategy for selecting the appropriate model. First, it scans the checkpoints directory for all subdirectories, which represent different model configurations or sizes. The function then attempts to parse model tags following the naming convention d<number>, where the number typically represents model depth or size. It extracts these numbers using regular expressions and sorts the candidates in descending order to select the largest model. This approach assumes that larger models (with higher depth numbers) are generally preferred for inference or continued training. If no model tags match the d<number> pattern, the function falls back to a time-based heuristic, selecting the most recently modified directory under the assumption that it represents the latest training run. This dual-strategy design provides robustness across different naming conventions and ensures that a reasonable default model is selected even when the standard naming scheme isn't followed.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def find_last_step(checkpoint_dir):\n    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"model_*.pt\"))\n    if not checkpoint_files:\n        raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n    last_step = int(max(os.path.basename(f).split(\"_\")[-1].split(\".\")[0] for f in checkpoint_files))\n    return last_step\n",
      "highlight": true,
      "voiceover": "This function determines the most recent checkpoint step number within a given checkpoint directory. It searches for all model checkpoint files matching the pattern model_<step>.pt using glob pattern matching, then extracts the step numbers from the filenames by parsing the numeric portion between the underscore and the .pt extension. The function returns the maximum step number found, which represents the latest saved checkpoint. This is useful for automatically resuming training from the most recent checkpoint or loading the final trained model without requiring manual specification of the step number. The function raises a FileNotFoundError if no checkpoint files are found in the directory, providing clear feedback when the specified directory doesn't contain valid checkpoints. This design allows for convenient checkpoint management where users can simply point to a directory and automatically get the latest available checkpoint.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def load_model_from_dir(checkpoints_dir, device, phase, model_tag=None, step=None):\n    if model_tag is None:\n        model_tag = find_largest_model(checkpoints_dir)\n        log0(f\"No model tag provided, guessing model tag: {model_tag}\")\n    checkpoint_dir = os.path.join(checkpoints_dir, model_tag)\n    if step is None:\n        step = find_last_step(checkpoint_dir)\n    assert step is not None, f\"No checkpoints found in {checkpoint_dir}\"\n    log0(f\"Loading model from {checkpoint_dir} with step {step}\")\n    model, tokenizer, meta_data = build_model(checkpoint_dir, step, device, phase)\n    return model, tokenizer, meta_data\n",
      "highlight": true,
      "voiceover": "This is a high-level convenience function that loads a model from a checkpoints directory with intelligent defaults for model selection and step number. It encapsulates the common workflow of loading a model by automatically inferring missing parameters. When no model tag is provided, it calls find_largest_model to select the largest available model, assuming that's typically the desired choice. When no step number is specified, it calls find_last_step to load the most recent checkpoint, which is the common use case for both inference and training resumption. The function constructs the full checkpoint directory path by combining the base checkpoints directory with the model tag subdirectory, following nanochat's organizational structure. It then delegates to build_model to perform the actual model construction and weight loading. This layered design separates the concerns of checkpoint discovery (finding the right model and step) from model construction (building and initializing the model), making the code more maintainable and the API more user-friendly by reducing the number of required parameters.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def load_model(source, *args, **kwargs):\n    model_dir = {\n        \"base\": \"base_checkpoints\",\n        \"mid\": \"mid_checkpoints\",\n        \"sft\": \"chatsft_checkpoints\",\n        \"rl\": \"chatrl_checkpoints\",\n    }[source]\n    base_dir = get_base_dir()\n    checkpoints_dir = os.path.join(base_dir, model_dir)\n    return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
      "highlight": true,
      "voiceover": "This is the highest-level convenience function for loading models in the nanochat project, providing a simple interface that abstracts away all directory path details. It maps semantic source names (base, mid, sft, rl) to their corresponding checkpoint directory names, representing different stages of the model training pipeline: base for pretrained models, mid for intermediate checkpoints, sft for supervised fine-tuning checkpoints, and rl for reinforcement learning checkpoints. The function uses a dictionary lookup to translate the source parameter into the appropriate directory name, then combines it with the project's base directory to construct the full checkpoints path. It forwards all additional arguments and keyword arguments to load_model_from_dir, allowing callers to optionally specify model_tag, step, device, and phase parameters while maintaining a clean, simple API. This design provides a semantic, user-friendly interface that hides the complexity of the directory structure and makes it easy to load models from different training stages without remembering specific directory names or paths.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "nanochat/common.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport re\nimport logging\nimport urllib.request\nimport torch\nimport torch.distributed as dist\nfrom filelock import FileLock\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import os",
      "voiceover": "Operating system interface for file paths, environment variables, and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import re",
      "voiceover": "Regular expression operations for pattern matching in log message formatting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import logging",
      "voiceover": "Logging facility for tracking operations and debugging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import urllib.request",
      "voiceover": "URL handling library for downloading files from remote sources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and device management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "import torch.distributed as dist",
      "voiceover": "Distributed training utilities for multi-GPU/multi-node coordination",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "from filelock import FileLock",
      "voiceover": "File-based locking mechanism to prevent concurrent file access across processes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class ColoredFormatter(logging.Formatter):\n",
      "highlight": true,
      "voiceover": "# Module-level docstring describing the purpose of this common utilities module\nCommon utilities for nanochat.\n\nThis is a custom logging formatter that enhances log messages with ANSI color codes to improve readability in terminal output. It extends Python's standard logging.Formatter class and applies different colors to different log levels, making it easier to visually distinguish between debug, info, warning, error, and critical messages. The formatter uses ANSI escape sequences to colorize the log level names with both color and bold styling. Additionally, for INFO level messages, it applies special highlighting to numeric values and units like GB, MB, percentages, and document counts, as well as shard identifiers, making important metrics stand out in the logs. This design improves the developer experience by making log output more scannable and reducing the cognitive load when monitoring training runs or debugging issues. The formatter is specifically tailored for nanochat's logging patterns, highlighting the types of information that are most relevant during model training and data processing.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    COLORS = {\n        'DEBUG': '\\033[36m',\n        'INFO': '\\033[32m',\n        'WARNING': '\\033[33m',\n        'ERROR': '\\033[31m',\n        'CRITICAL': '\\033[35m',\n    }\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "COLORS = {",
      "voiceover": "Dictionary mapping log levels to ANSI color codes for terminal output",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "'DEBUG': '\\033[36m',",
      "voiceover": "Cyan",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "'INFO': '\\033[32m',",
      "voiceover": "Green",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "'WARNING': '\\033[33m',",
      "voiceover": "Yellow",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "'ERROR': '\\033[31m',",
      "voiceover": "Red",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "'CRITICAL': '\\033[35m',",
      "voiceover": "Magenta",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    RESET = '\\033[0m'\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "RESET = '\\033[0m'",
      "voiceover": "ANSI code to reset all text formatting back to default",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    BOLD = '\\033[1m'\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "BOLD = '\\033[1m'",
      "voiceover": "ANSI code to make text bold for emphasis",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    def format(self, record):\n        levelname = record.levelname\n        if levelname in self.COLORS:\n            record.levelname = f\"{self.COLORS[levelname]}{self.BOLD}{levelname}{self.RESET}\"\n        message = super().format(record)\n        if levelname == 'INFO':\n            message = re.sub(r'(\\d+\\.?\\d*\\s*(?:GB|MB|%|docs))', rf'{self.BOLD}\\1{self.RESET}', message)\n            message = re.sub(r'(Shard \\d+)', rf'{self.COLORS[\"INFO\"]}{self.BOLD}\\1{self.RESET}', message)\n        return message\n",
      "highlight": true,
      "voiceover": "This method formats a log record by applying color codes to the level name and highlighting specific patterns in the message text. It first retrieves the log level name and wraps it with the appropriate color and bold ANSI codes based on the level. Then it calls the parent class's format method to apply the standard formatting template. For INFO level messages, it uses regular expressions to identify and highlight numeric values with units (GB, MB, %, docs) and shard identifiers, making these important metrics visually prominent in the terminal output. The method returns the fully formatted and colorized message string ready for display. This approach ensures that all log messages follow a consistent format while adding visual enhancements that make the logs easier to parse at a glance.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def setup_default_logging():\n    handler = logging.StreamHandler()\n    handler.setFormatter(ColoredFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n    logging.basicConfig(\n        level=logging.INFO,\n        handlers=[handler]\n    )\n",
      "highlight": true,
      "voiceover": "This function configures the default logging setup for the nanochat project with colored output formatting. It creates a StreamHandler that outputs log messages to the console and attaches the custom ColoredFormatter to it, which applies ANSI color codes to make logs more readable. The formatter template includes timestamp, logger name, log level, and the message content. The function then configures the root logger with INFO level logging and the custom handler, ensuring that all modules in the project use consistent, colorized logging output. This centralized logging configuration makes it easy to maintain a uniform logging style across the entire codebase and provides a better debugging and monitoring experience during development and training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "setup_default_logging()\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "setup_default_logging()",
      "voiceover": "Initialize the default logging configuration when the module is imported",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "logger = logging.getLogger(__name__)\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "logger = logging.getLogger(__name__)",
      "voiceover": "Create a logger instance for this module",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_base_dir():\n    if os.environ.get(\"NANOCHAT_BASE_DIR\"):\n        nanochat_dir = os.environ.get(\"NANOCHAT_BASE_DIR\")\n    else:\n        home_dir = os.path.expanduser(\"~\")\n        cache_dir = os.path.join(home_dir, \".cache\")\n        nanochat_dir = os.path.join(cache_dir, \"nanochat\")\n    os.makedirs(nanochat_dir, exist_ok=True)\n    return nanochat_dir\n",
      "highlight": true,
      "voiceover": "This function determines and returns the base directory where nanochat stores all its intermediate files, checkpoints, and cached data. It implements a configurable directory strategy that respects the NANOCHAT_BASE_DIR environment variable if set, allowing users to customize the storage location. If the environment variable is not set, it defaults to a standard location following XDG conventions by placing nanochat data in the ~/.cache/nanochat directory. This approach keeps nanochat's files organized alongside other application caches and makes them easy to locate and clean up. The function ensures the directory exists by creating it if necessary, preventing errors when other parts of the codebase attempt to write files to this location. This centralized directory management makes it easy to change the storage location across the entire project by simply setting an environment variable.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def download_file_with_lock(url, filename, postprocess_fn=None):\n    base_dir = get_base_dir()\n    file_path = os.path.join(base_dir, filename)\n    lock_path = file_path + \".lock\"\n\n    if os.path.exists(file_path):\n        return file_path\n\n    with FileLock(lock_path):\n\n        if os.path.exists(file_path):\n            return file_path\n\n        print(f\"Downloading {url}...\")\n        with urllib.request.urlopen(url) as response:\n            content = response.read()\n\n        with open(file_path, 'wb') as f:\n            f.write(content)\n        print(f\"Downloaded to {file_path}\")\n\n        if postprocess_fn is not None:\n            postprocess_fn(file_path)\n\n    return file_path\n",
      "highlight": true,
      "voiceover": "This function safely downloads a file from a remote URL to the local nanochat base directory, with built-in protection against concurrent downloads in distributed training scenarios. It uses a file-based locking mechanism to ensure that when multiple processes or ranks attempt to download the same file simultaneously, only one actually performs the download while the others wait. The function first checks if the file already exists and returns immediately if so, avoiding unnecessary downloads. If the file doesn't exist, it acquires an exclusive lock using FileLock, then rechecks for the file's existence in case another process downloaded it while waiting for the lock. This double-check pattern prevents race conditions. The actual download uses urllib to fetch the file content as bytes and writes it to disk. After downloading, the function can optionally run a postprocessing function on the file, which is useful for tasks like decompression or validation. This design ensures efficient, safe file downloads in multi-process distributed training environments where all ranks need access to the same data files.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "content = response.read()",
      "voiceover": "bytes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def print0(s=\"\",**kwargs):\n    ddp_rank = int(os.environ.get('RANK', 0))\n    if ddp_rank == 0:\n        print(s, **kwargs)\n",
      "highlight": true,
      "voiceover": "This is a convenience print function that only outputs messages from rank 0 in a distributed training setup. It checks the RANK environment variable to determine if the current process is the primary rank, and if so, forwards the print call with all its arguments to the standard print function. This prevents duplicate console output when running distributed training across multiple processes, ensuring that messages appear only once rather than being repeated by every process. The function accepts the same arguments as the built-in print function, making it a drop-in replacement that can be used anywhere you want rank-0-only output. This is particularly useful for printing banners, progress updates, and other informational messages that don't need to be duplicated across all processes.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def print_banner():\n    banner = \"\"\"\n                                                       █████                █████\n                                                      ░░███                ░░███\n     ████████    ██████   ████████    ██████   ██████  ░███████    ██████  ███████\n    ░░███░░███  ░░░░░███ ░░███░░███  ███░░███ ███░░███ ░███░░███  ░░░░░███░░░███░\n     ░███ ░███   ███████  ░███ ░███ ░███ ░███░███ ░░░  ░███ ░███   ███████  ░███\n     ░███ ░███  ███░░███  ░███ ░███ ░███ ░███░███  ███ ░███ ░███  ███░░███  ░███ ███\n     ████ █████░░████████ ████ █████░░██████ ░░██████  ████ █████░░███████  ░░█████\n    ░░░░ ░░░░░  ░░░░░░░░ ░░░░ ░░░░░  ░░░░░░   ░░░░░░  ░░░░ ░░░░░  ░░░░░░░░   ░░░░░",
      "highlight": true,
      "voiceover": "This function displays a stylized ASCII art banner for the nanochat project using the DOS Rebel font. It prints the banner only from rank 0 using the print0 function to avoid duplicate output in distributed training scenarios. The banner serves as a visual identifier when starting nanochat scripts, making it easy to recognize nanochat processes in terminal output and adding a professional, branded appearance to the command-line interface. The ASCII art was generated using an online tool and provides a distinctive visual signature for the project.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def is_ddp():\n    return int(os.environ.get('RANK', -1)) != -1\n",
      "highlight": true,
      "voiceover": "This function checks whether the current process is running in a Distributed Data Parallel (DDP) training setup. It determines this by checking for the presence of the RANK environment variable, which is set by PyTorch's distributed launcher when running multi-process training. If RANK is set to a valid value (not -1), the function returns True, indicating distributed training is active. Otherwise, it returns False, indicating single-process training. This simple heuristic allows the code to adapt its behavior based on whether it's running in distributed or single-process mode. The function includes a TODO comment noting that there might be a more official way to detect DDP mode, but this environment variable check is a reliable and commonly used approach.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_dist_info():\n    if is_ddp():\n        assert all(var in os.environ for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE'])\n        ddp_rank = int(os.environ['RANK'])\n        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n        ddp_world_size = int(os.environ['WORLD_SIZE'])\n        return True, ddp_rank, ddp_local_rank, ddp_world_size\n    else:\n        return False, 0, 0, 1\n",
      "highlight": true,
      "voiceover": "This function retrieves distributed training information from environment variables and returns it in a standardized format. It first checks if DDP is active using the is_ddp function. If running in distributed mode, it validates that all required environment variables (RANK, LOCAL_RANK, WORLD_SIZE) are present, then extracts and returns them along with a True flag indicating DDP is active. RANK represents the global process rank across all nodes, LOCAL_RANK is the process rank on the current node, and WORLD_SIZE is the total number of processes. If not running in distributed mode, it returns default values appropriate for single-process training: False for DDP status, 0 for both rank values, and 1 for world size. This function provides a consistent interface for querying distributed training configuration, allowing the rest of the codebase to handle both distributed and non-distributed scenarios with the same code paths.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def autodetect_device_type():\n    if torch.cuda.is_available():\n        device_type = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device_type = \"mps\"\n    else:\n        device_type = \"cpu\"\n    print0(f\"Autodetected device type: {device_type}\")\n    return device_type\n",
      "highlight": true,
      "voiceover": "This function automatically detects and returns the best available device type for PyTorch computations based on hardware availability. It implements a priority-based selection strategy: it first checks for CUDA GPU availability, which provides the best performance for deep learning workloads. If CUDA is not available, it checks for Apple's Metal Performance Shaders (MPS) backend, which enables GPU acceleration on Apple Silicon Macs. If neither GPU option is available, it falls back to CPU computation. The function prints the detected device type using print0 to inform the user which hardware will be used, which is helpful for debugging performance issues or confirming that GPU acceleration is working. This automatic detection makes nanochat scripts more portable across different hardware configurations without requiring manual device specification.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def compute_init(device_type=\"cuda\"):\n\n    assert device_type in [\"cuda\", \"mps\", \"cpu\"], \"Invalid device type atm\"\n    if device_type == \"cuda\":\n        assert torch.cuda.is_available(), \"Your PyTorch installation is not configured for CUDA but device_type is 'cuda'\"\n    if device_type == \"mps\":\n        assert torch.backends.mps.is_available(), \"Your PyTorch installation is not configured for MPS but device_type is 'mps'\"\n\n    torch.manual_seed(42)\n    if device_type == \"cuda\":\n        torch.cuda.manual_seed(42)\n\n    if device_type == \"cuda\":\n        torch.set_float32_matmul_precision(\"high\")\n\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    if ddp and device_type == \"cuda\":\n        device = torch.device(\"cuda\", ddp_local_rank)\n        torch.cuda.set_device(device)\n        dist.init_process_group(backend=\"nccl\", device_id=device)\n        dist.barrier()\n    else:\n        device = torch.device(device_type)\n\n    if ddp_rank == 0:\n        logger.info(f\"Distributed world size: {ddp_world_size}\")\n\n    return ddp, ddp_rank, ddp_local_rank, ddp_world_size, device\n",
      "highlight": true,
      "voiceover": "This function performs comprehensive initialization for PyTorch computation, handling device setup, reproducibility configuration, precision settings, and distributed training initialization. It validates that the requested device type is supported and available on the current system, raising clear error messages if there's a mismatch between the requested device and PyTorch's capabilities. The function sets random seeds for reproducibility, ensuring that model weight initialization is deterministic across runs. For CUDA devices, it configures TensorFloat-32 (TF32) precision for matrix multiplications, which provides a good balance between performance and numerical accuracy on modern NVIDIA GPUs. The function also handles distributed training setup when running in DDP mode with CUDA, initializing the process group with the NCCL backend for efficient GPU communication, setting the correct device for each process based on local rank, and synchronizing all processes with a barrier. For non-distributed or non-CUDA scenarios, it simply creates a device object for the specified type. The function returns all the distributed training information along with the configured device, providing everything needed to start training. This centralized initialization eliminates code duplication across different training scripts and ensures consistent setup across the project.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "def compute_init(device_type=\"cuda\"):",
      "voiceover": "cuda|cpu|mps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "torch.set_float32_matmul_precision(\"high\")",
      "voiceover": "uses tf32 instead of fp32 for matmuls",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "torch.cuda.set_device(device)",
      "voiceover": "make \"cuda\" default to this device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/common.py",
      "find": "device = torch.device(device_type)",
      "voiceover": "mps|cpu",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def compute_cleanup():\n    if is_ddp():\n        dist.destroy_process_group()\n",
      "highlight": true,
      "voiceover": "This function performs cleanup operations after training or inference is complete, serving as the companion to compute_init. It checks if the process is running in distributed mode and, if so, properly destroys the process group to release distributed training resources and ensure clean shutdown. This cleanup is important for preventing resource leaks and ensuring that all processes terminate gracefully. The function should be called at the end of training scripts before exiting, and it's designed to be safe to call even in non-distributed scenarios since it only performs cleanup when DDP is active. This centralized cleanup function ensures consistent resource management across all nanochat scripts.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class DummyWandb:\n",
      "highlight": true,
      "voiceover": "This is a no-op placeholder class that mimics the interface of the Weights & Biases (wandb) logging library. It provides the same method signatures as wandb but doesn't perform any actual logging operations. This design allows the codebase to have wandb logging calls throughout without requiring wandb to be installed or configured. When wandb is not desired or available, this dummy class can be used as a drop-in replacement, and all logging calls will silently do nothing. This pattern is useful for development, testing, or deployment scenarios where external logging services are not needed or not accessible. The class implements the most commonly used wandb methods (init, log, finish) as empty stubs, ensuring that code written to use wandb will run without errors even when wandb is disabled.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self):\n        pass\n",
      "highlight": true,
      "voiceover": "This initializer does nothing, as the dummy class doesn't need to maintain any state or perform any setup operations.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def log(self, *args, **kwargs):\n        pass\n",
      "highlight": true,
      "voiceover": "This method accepts any arguments to match wandb's log signature but performs no actual logging, silently discarding all metrics and data.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def finish(self):\n        pass\n",
      "highlight": true,
      "voiceover": "This method mimics wandb's finish call for completing a logging session but performs no actual cleanup since there's no state to manage.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "nanochat/configurator.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport sys\nfrom ast import literal_eval\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "import os",
      "voiceover": "Operating system interface for accessing environment variables",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "import sys",
      "voiceover": "System-specific parameters and functions, used to access command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "from ast import literal_eval",
      "voiceover": "Safe evaluation of string literals to Python objects for type conversion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def print0(s=\"\",**kwargs):\n    ddp_rank = int(os.environ.get('RANK', 0))\n    if ddp_rank == 0:\n        print(s, **kwargs)\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the unconventional configuration system design and usage\nPoor Man's Configurator. Probably a terrible idea. Example usage:\n$ python train.py config/override_file.py --batch_size=32\nthis will first run config/override_file.py, then override batch_size to 32\n\nThe code in this file will be run as follows from e.g. train.py:\n>>> exec(open('configurator.py').read())\n\nSo it's not a Python module, it's just shuttling this code away from train.py\nThe code in this script then overrides the globals()\n\nI know people are not going to love this, I just really dislike configuration\ncomplexity and having to prepend config. to every single variable. If someone\ncomes up with a better simple Python solution I am all ears.\n\nThis is a convenience print function that only outputs messages from rank 0 in a distributed training setup. It checks the RANK environment variable to determine if the current process is the primary rank, and if so, forwards the print call with all its arguments to the standard print function. This prevents duplicate console output when running distributed training across multiple processes, ensuring that configuration override messages appear only once rather than being repeated by every process. The function is duplicated here from common.py to avoid circular dependencies since this configurator code is executed via exec and needs to be self-contained.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "for arg in sys.argv[1:]:\n    if '=' not in arg:\n        assert not arg.startswith('--')\n        config_file = arg\n        print0(f\"Overriding config with {config_file}:\")\n        with open(config_file) as f:\n            print0(f.read())\n        exec(open(config_file).read())\n    else:\n        assert arg.startswith('--')\n        key, val = arg.split('=')\n        key = key[2:]\n        if key in globals():\n            try:\n                attempt = literal_eval(val)\n            except (SyntaxError, ValueError):\n                attempt = val\n            if globals()[key] is not None:\n                attempt_type = type(attempt)\n                default_type = type(globals()[key])\n                assert attempt_type == default_type, f\"Type mismatch: {attempt_type} != {default_type}\"\n            print0(f\"Overriding: {key} = {attempt}\")\n            globals()[key] = attempt\n        else:\n            raise ValueError(f\"Unknown config key: {key}\")\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "for arg in sys.argv[1:]:",
      "voiceover": "Iterate through all command-line arguments except the script name itself",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "if '=' not in arg:",
      "voiceover": "Check if this argument is a config file path (no equals sign means it's not a key=value pair)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "assert not arg.startswith('--')",
      "voiceover": "Ensure config file paths don't start with -- which is reserved for key=value overrides",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "config_file = arg",
      "voiceover": "Store the config file path",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "print0(f\"Overriding config with {config_file}:\")",
      "voiceover": "Log which config file is being loaded",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "with open(config_file) as f:",
      "voiceover": "Open and display the config file contents for transparency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "exec(open(config_file).read())",
      "voiceover": "Execute the config file, which modifies the globals() dictionary with new default values",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "else:",
      "voiceover": "This argument contains an equals sign, so it's a --key=value override",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "assert arg.startswith('--')",
      "voiceover": "Ensure key=value arguments are prefixed with -- for consistency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "key, val = arg.split('=')",
      "voiceover": "Split the argument into key and value parts",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "key = key[2:]",
      "voiceover": "Remove the -- prefix from the key name",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "if key in globals():",
      "voiceover": "Check if this key exists in the global namespace (was defined as a default)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "attempt = literal_eval(val)",
      "voiceover": "Try to parse the value as a Python literal (int, float, bool, list, etc.)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "attempt = val",
      "voiceover": "If parsing fails, treat the value as a plain string",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "if globals()[key] is not None:",
      "voiceover": "If the existing global value is not None, enforce type matching",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "attempt_type = type(attempt)",
      "voiceover": "Get the type of the parsed value",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "default_type = type(globals()[key])",
      "voiceover": "Get the type of the existing default value",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "assert attempt_type == default_type, f\"Type mismatch: {attempt_type} != {default_type}\"",
      "voiceover": "Ensure types match to prevent configuration errors",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "print0(f\"Overriding: {key} = {attempt}\")",
      "voiceover": "Log the configuration override for transparency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "globals()[key] = attempt",
      "voiceover": "Update the global variable with the new value",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "else:",
      "voiceover": "The key doesn't exist in globals, which means it's not a valid configuration parameter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/configurator.py",
      "find": "raise ValueError(f\"Unknown config key: {key}\")",
      "voiceover": "Raise an error for invalid configuration keys",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/core_eval.py"
    },
    {
      "type": "writeText",
      "content": "import random\n\nfrom jinja2 import Template\nimport torch\nimport torch.distributed as dist\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "import random",
      "voiceover": "Random number generation for sampling few-shot examples with deterministic seeding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "from jinja2 import Template",
      "voiceover": "Template engine for rendering prompts with few-shot examples and continuations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and model inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "import torch.distributed as dist",
      "voiceover": "Distributed training utilities for multi-GPU evaluation coordination",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def render_prompts_mc(item, continuation_delimiter, fewshot_examples=None):\n    template_str = \"\"\"\n{%- for example in fewshot_examples -%}\n{{ example.query }}{{ continuation_delimiter }}{{ example.choices[example.gold] }}\n\n{% endfor -%}\n{{ item.query }}{{ continuation_delimiter }}{{ choice }}\"\"\".strip()\n    template = Template(template_str)\n    fewshot_examples = fewshot_examples or []\n    context = {\n        'fewshot_examples': fewshot_examples,\n        'continuation_delimiter': continuation_delimiter,\n        'item': item\n    }\n    prompts = [template.render(choice=choice, **context) for choice in item['choices']]\n    return prompts\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this evaluation module\nFunctions for evaluating the CORE metric, as described in the DCLM paper.\nhttps://arxiv.org/abs/2406.11794\n\nTODOs:\n- All tasks ~match except for squad. We get 31% reference is 37%. Figure out why.\n\nThis function renders complete prompts for multiple choice questions by constructing a template that includes optional few-shot examples followed by the test item with each possible answer choice. It uses Jinja2 templating to format the prompts, where each few-shot example shows the query followed by the correct answer choice, separated by a continuation delimiter. The function generates one prompt per answer choice, each containing the same query but with a different choice appended. This design allows the model to evaluate all choices in parallel by computing the likelihood of each continuation. The template strips whitespace to ensure clean formatting, and the function returns a list of prompts corresponding to each choice in the item. This approach is specifically designed for multiple choice tasks where the context is constant but the continuations vary.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def render_prompts_schema(item, continuation_delimiter, fewshot_examples=None):\n    template_str = \"\"\"\n{%- for example in fewshot_examples -%}\n{{ example.context_options[example.gold] }}{{ continuation_delimiter }}{{ example.continuation }}\n\n{% endfor -%}\n{{ context }}{{ continuation_delimiter }}{{ item.continuation }}\"\"\".strip()\n    template = Template(template_str)\n    fewshot_examples = fewshot_examples or []\n    context = {\n        'fewshot_examples': fewshot_examples,\n        'continuation_delimiter': continuation_delimiter,\n        'item': item\n    }\n    prompts = [template.render(context=context_option, **context)\n               for context_option in item['context_options']]\n    return prompts\n",
      "highlight": true,
      "voiceover": "This function renders complete prompts for schema tasks, which are the inverse of multiple choice questions where the context varies but the continuation is constant. It constructs a Jinja2 template that includes optional few-shot examples followed by the test item, where each few-shot example shows the correct context option followed by the continuation. The function generates one prompt per context option, each containing a different context but the same continuation appended. This design allows the model to evaluate which context best predicts the given continuation by computing the likelihood of the continuation given each context. The template strips whitespace for clean formatting, and the function returns a list of prompts corresponding to each context option. This approach is specifically designed for schema tasks like Winograd Schema Challenge where multiple context variations are evaluated against a single continuation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def render_prompts_lm(item, continuation_delimiter, fewshot_examples=None):\n    template_str = \"\"\"\n{%- for example in fewshot_examples -%}\n{{ example.context | trim }}{{ continuation_delimiter }}{{ example.continuation }}\n\n{% endfor -%}\n{{ item.context | trim }}{{ continuation_delimiter }}{% if include_continuation %}{{ item.continuation }}{% endif %}\"\"\".strip()\n    template = Template(template_str)\n    fewshot_examples = fewshot_examples or []\n    context = {\n        'fewshot_examples': fewshot_examples,\n        'continuation_delimiter': continuation_delimiter,\n        'item': item\n    }\n    prompt_without = template.render(include_continuation=False, **context)\n    prompt_with = template.render(include_continuation=True, **context)\n    prompt_without = prompt_without.strip()\n    return [prompt_without, prompt_with]\n",
      "highlight": true,
      "voiceover": "This function renders prompts for language modeling tasks where the model must predict a continuation given a context. It constructs a Jinja2 template that includes optional few-shot examples followed by the test item, where each few-shot example shows a context and its continuation separated by a delimiter. The function generates two prompts: one without the continuation (for computing the prefix likelihood) and one with the continuation (for computing the full sequence likelihood). The template manually trims the context to remove trailing whitespace, which is important because some datasets have inconsistent whitespace that could interfere with tokenization. The function also strips the prompt without continuation to ensure clean prefix matching in token space, preventing issues where trailing whitespace gets absorbed into the next token during tokenization. This careful handling of whitespace ensures that the continuation tokens can be cleanly identified and evaluated. The function returns a list containing both prompts, which are used to compute the likelihood of the continuation given the context.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def find_common_length(token_sequences, direction='left'):\n    min_len = min(len(seq) for seq in token_sequences)\n    indices = {\n        'left': range(min_len),\n        'right': range(-1, -min_len-1, -1)\n    }[direction]\n    for i, idx in enumerate(indices):\n        token = token_sequences[0][idx]\n        if not all(seq[idx] == token for seq in token_sequences):\n            return i\n    return min_len\n",
      "highlight": true,
      "voiceover": "This function finds the length of the common prefix or suffix across multiple token sequences, which is essential for identifying where continuations begin or end in evaluation tasks. It takes a list of token sequences and a direction parameter that specifies whether to search from the left (prefix) or right (suffix). The function first determines the minimum sequence length to avoid index errors, then iterates through positions from the specified direction, checking if all sequences have the same token at that position. It returns the length of the common portion, stopping at the first position where sequences differ. For left direction, it searches forward from index 0 to find the common prefix length, which is used in multiple choice tasks where all prompts share the same context. For right direction, it searches backward from the end to find the common suffix length, which is used in schema tasks where all prompts share the same continuation. This design enables efficient identification of the portion of the sequence that needs to be evaluated for likelihood computation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def stack_sequences(tokens, pad_token_id):\n    bsz, seq_len = len(tokens), max(len(x) for x in tokens)\n    input_ids = torch.full((bsz, seq_len), pad_token_id, dtype=torch.long)\n    for i, x in enumerate(tokens):\n        input_ids[i, :len(x)] = torch.tensor(x, dtype=torch.long)\n    return input_ids\n",
      "highlight": true,
      "voiceover": "This function stacks a list of variable-length token sequences into a single batched tensor by padding shorter sequences to match the longest sequence length. It creates a tensor filled with the pad token ID, with dimensions matching the batch size and maximum sequence length. Then it copies each token sequence into the corresponding row of the tensor, leaving the remaining positions as padding. The padding is applied on the right side of each sequence, which is the standard approach for causal language models. This batching operation enables efficient parallel processing of multiple prompts through the model in a single forward pass, which is essential for evaluation performance when processing many examples or multiple choices per example.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def batch_sequences_mc(tokenizer, prompts):\n    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n    answer_start_idx = find_common_length(tokens, direction='left')\n    start_indices = [answer_start_idx] * len(prompts)\n    end_indices = [len(x) for x in tokens]\n    return tokens, start_indices, end_indices\n",
      "highlight": true,
      "voiceover": "This function prepares token sequences for multiple choice evaluation by tokenizing all prompts and identifying the start and end indices of each answer continuation. In multiple choice tasks, all prompts share the same context but have different continuations (one per choice), so the function finds the common prefix length to determine where the answer continuations begin. It tokenizes all prompts with a BOS token prepended, then uses find_common_length to locate the divergence point where the different answer choices start. The function returns the tokenized sequences along with parallel lists of start and end indices that mark the boundaries of each continuation, which are used later to compute the likelihood of each answer choice. This design enables efficient evaluation by identifying exactly which tokens need to be scored for each choice.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def batch_sequences_schema(tokenizer, prompts):\n    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n    suffix_length = find_common_length(tokens, direction='right')\n    end_indices = [len(x) for x in tokens]\n    start_indices = [ei - suffix_length for ei in end_indices]\n    return tokens, start_indices, end_indices\n",
      "highlight": true,
      "voiceover": "This function prepares token sequences for schema task evaluation by tokenizing all prompts and identifying the start and end indices of each context variation. In schema tasks, all prompts share the same continuation but have different contexts (one per option), so the function finds the common suffix length to determine where the shared continuation begins. It tokenizes all prompts with a BOS token prepended, then uses find_common_length with right direction to locate the common suffix. The function calculates the start indices by subtracting the suffix length from each sequence's end position, effectively marking where each unique context ends and the shared continuation begins. This design enables evaluation by identifying which tokens represent the varying context portions that need to be scored for likelihood given the fixed continuation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def batch_sequences_lm(tokenizer, prompts):\n    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n    tokens_without, tokens_with = tokens\n    start_idx, end_idx = len(tokens_without), len(tokens_with)\n    assert start_idx < end_idx, \"prompt without is supposed to be a prefix of prompt with\"\n    assert tokens_without == tokens_with[:start_idx], \"prompt without is supposed to be a prefix of prompt with\"\n    return [tokens_with], [start_idx], [end_idx]\n",
      "highlight": true,
      "voiceover": "This function prepares token sequences for language modeling evaluation by tokenizing both the context-only and context-with-continuation prompts and identifying the continuation boundaries. In language modeling tasks, the function receives two prompts: one without the continuation and one with it. It tokenizes both with a BOS token prepended, then extracts the start and end indices of the continuation by using the length of the without-continuation prompt as the start index and the length of the with-continuation prompt as the end index. The function validates that the without-continuation tokens form a proper prefix of the with-continuation tokens, ensuring clean token boundaries. It returns only the with-continuation tokens in a single-element list (batch size 1) along with the continuation indices, which are used to evaluate whether the model correctly predicts the continuation tokens given the context. This design ensures accurate likelihood computation for the continuation portion of the sequence.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "def forward_model(model, input_ids):\n    batch_size, seq_len = input_ids.size()\n    outputs = model(input_ids)\n    target_ids = torch.roll(input_ids, shifts=-1, dims=1)\n    losses = torch.nn.functional.cross_entropy(\n        outputs.view(batch_size * seq_len, -1),\n        target_ids.view(batch_size * seq_len),\n        reduction='none'\n    ).view(batch_size, seq_len)\n    losses[:, -1] = float('nan')\n    predictions = outputs.argmax(dim=-1)\n    return losses, predictions\n",
      "highlight": true,
      "voiceover": "This function performs a forward pass through the model to compute autoregressive losses and predictions for each token position. It takes a batch of token sequences and returns per-token cross-entropy losses and argmax predictions. The function first runs the model to get logits for each position, then creates autoregressive targets by rolling the input tensor left by one position, so that each position's target is the next token in the sequence. It computes cross-entropy loss at every position by reshaping the outputs and targets into 2D tensors for efficient batch computation, then reshapes the results back to the original batch-by-sequence dimensions. The last column of losses is set to NaN because there's no autoregressive target for the final position. The function also computes argmax predictions at each position, which can be compared against the actual tokens to check if the model would have generated them correctly. This design provides detailed per-token evaluation metrics that are used by the task-specific evaluation logic to determine correctness.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "def evaluate_example(idx, model, tokenizer, data, device, task_meta):\n    item = data[idx]\n    task_type = task_meta['task_type']\n    num_fewshot = task_meta['num_fewshot']\n    continuation_delimiter = task_meta['continuation_delimiter']\n\n    fewshot_examples = []\n    if num_fewshot > 0:\n        rng = random.Random(1234 + idx)\n        available_indices = [i for i in range(len(data)) if i != idx]\n        fewshot_indices = rng.sample(available_indices, num_fewshot)\n        fewshot_examples = [data[i] for i in fewshot_indices]\n\n    if task_type == 'multiple_choice':\n        prompts = render_prompts_mc(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_mc(tokenizer, prompts)\n    elif task_type == 'schema':\n        prompts = render_prompts_schema(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_schema(tokenizer, prompts)\n    elif task_type == 'language_modeling':\n        prompts = render_prompts_lm(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_lm(tokenizer, prompts)\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n\n    if hasattr(model, 'max_seq_len') and model.max_seq_len is not None:\n        max_tokens = model.max_seq_len\n        new_tokens, new_start_idxs, new_end_idxs = [], [], []\n        for t, s, e in zip(tokens, start_idxs, end_idxs):\n            if len(t) > max_tokens:\n                num_to_crop = len(t) - max_tokens\n                new_tokens.append(t[-max_tokens:])\n                new_start_idxs.append(s - num_to_crop)\n                new_end_idxs.append(e - num_to_crop)\n                assert s - num_to_crop >= 0, \"this should never happen right?\"\n                assert e - num_to_crop >= 0, \"this should never happen right?\"\n            else:\n                new_tokens.append(t)\n                new_start_idxs.append(s)\n                new_end_idxs.append(e)\n        tokens, start_idxs, end_idxs = new_tokens, new_start_idxs, new_end_idxs\n\n    pad_token_id = tokenizer.get_bos_token_id()\n    input_ids = stack_sequences(tokens, pad_token_id)\n    input_ids = input_ids.to(device)\n\n    losses, predictions = forward_model(model, input_ids)\n\n    if task_type == 'language_modeling':\n        si = start_idxs[0]\n        ei = end_idxs[0]\n        predicted_tokens = predictions[0, si-1:ei-1]\n        actual_tokens = input_ids[0, si:ei]\n        is_correct = torch.all(predicted_tokens == actual_tokens).item()\n    elif task_type in ['multiple_choice', 'schema']:\n        mean_losses = [losses[i, si-1:ei-1].mean().item()\n                        for i, (si, ei) in enumerate(zip(start_idxs, end_idxs))]\n        pred_idx = mean_losses.index(min(mean_losses))\n        is_correct = pred_idx == item['gold']\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n\n    return is_correct\n",
      "highlight": true,
      "voiceover": "This function evaluates a single example from a dataset by rendering prompts with few-shot examples, running the model, and determining if the model's prediction is correct. It retrieves the example from the dataset and extracts task configuration including task type, number of few-shot examples, and continuation delimiter. The function samples few-shot examples deterministically using a seeded random number generator based on the example index, excluding the current example from the pool. It then renders prompts and prepares token sequences according to the task type (multiple choice, schema, or language modeling), using the appropriate rendering and batching functions. For models with maximum sequence length constraints, it truncates sequences from the left while adjusting the continuation indices accordingly, ensuring the most relevant tokens (typically at the end) are preserved. The function stacks the token sequences into a batched tensor, runs the model forward pass to get losses and predictions, then evaluates correctness based on task type. For language modeling, it checks if all continuation tokens match the argmax predictions. For multiple choice and schema tasks, it selects the option with the lowest average loss over the continuation tokens and compares it to the gold answer. The function returns a boolean indicating whether the model's prediction was correct, providing a simple interface for evaluating individual examples across different task types.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "new_tokens.append(t[-max_tokens:])",
      "voiceover": "take the last max_tokens tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "new_start_idxs.append(s - num_to_crop)",
      "voiceover": "shift the indices down",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "new_tokens.append(t)",
      "voiceover": "keep unchanged",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/core_eval.py",
      "find": "pad_token_id = tokenizer.get_bos_token_id()",
      "voiceover": "use BOS as pad token is ok",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def evaluate_task(model, tokenizer, data, device, task_meta):\n    rank = dist.get_rank() if dist.is_initialized() else 0\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    correct = torch.zeros(len(data), dtype=torch.float32, device=device)\n    for idx in range(rank, len(data), world_size):\n        is_correct = evaluate_example(idx, model, tokenizer, data, device, task_meta)\n        correct[idx] = float(is_correct)\n    if world_size > 1:\n        dist.barrier()\n        dist.all_reduce(correct, op=dist.ReduceOp.SUM)\n    mean_correct = correct.mean().item()\n    return mean_correct\n",
      "highlight": true,
      "voiceover": "This function orchestrates the evaluation of an entire task across all examples in the dataset, with support for distributed evaluation across multiple processes. It retrieves the current process's rank and world size from PyTorch's distributed module, defaulting to single-process values if distributed training is not initialized. The function creates a tensor to store correctness results for all examples, then distributes the evaluation workload by having each rank process a strided subset of examples (rank 0 processes examples 0, world_size, 2*world_size, etc.). Each rank evaluates its assigned examples by calling evaluate_example and storing the boolean results as floats in the correctness tensor. After all ranks complete their assigned examples, the function synchronizes results across processes using a barrier and all_reduce operation with SUM, which aggregates the correctness values from all ranks into a single tensor. Finally, it computes the mean accuracy across all examples and returns it as a scalar value. This design enables efficient parallel evaluation where the workload is distributed across multiple GPUs or nodes, significantly speeding up evaluation on large datasets while ensuring all examples are evaluated exactly once.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "nanochat/dataloader.py"
    },
    {
      "type": "writeText",
      "content": "from collections import deque\n\nimport torch\nimport pyarrow.parquet as pq\n\nfrom nanochat.common import get_dist_info\nfrom nanochat.dataset import list_parquet_files\nfrom nanochat.tokenizer import get_tokenizer\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "from collections import deque",
      "voiceover": "Double-ended queue for efficient token buffering with fast append and pop operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and GPU memory management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "import pyarrow.parquet as pq",
      "voiceover": "Apache Arrow Parquet library for reading columnar data files efficiently",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "from nanochat.common import get_dist_info",
      "voiceover": "Utility to retrieve distributed training configuration (rank, world size)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "from nanochat.dataset import list_parquet_files",
      "voiceover": "Function to enumerate all parquet data files in the dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "from nanochat.tokenizer import get_tokenizer",
      "voiceover": "Factory function to get the tokenizer instance for text encoding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def tokenizing_distributed_data_loader_with_state(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128, device=\"cuda\", resume_state_dict=None):\n    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    def document_batches():\n        parquet_paths = list_parquet_files()\n        parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n        resume_pq_idx = resume_state_dict[\"pq_idx\"] if resume_state_dict is not None else 0\n        resume_rg_idx = resume_state_dict[\"rg_idx\"] if resume_state_dict is not None else None\n        first_pass = True\n        pq_idx = resume_pq_idx\n        while True:\n            pq_idx = resume_pq_idx if first_pass else 0\n            while pq_idx < len(parquet_paths):\n                filepath = parquet_paths[pq_idx]\n                pf = pq.ParquetFile(filepath)\n                if first_pass and (resume_rg_idx is not None) and (pq_idx == resume_pq_idx):\n                    base_idx = resume_rg_idx // ddp_world_size\n                    base_idx += 1\n                    rg_idx = base_idx * ddp_world_size + ddp_rank\n                    if rg_idx >= pf.num_row_groups:\n                        pq_idx += 1\n                        continue\n                    resume_rg_idx = None\n                else:\n                    rg_idx = ddp_rank\n                while rg_idx < pf.num_row_groups:\n                    rg = pf.read_row_group(rg_idx)\n                    batch = rg.column('text').to_pylist()\n                    for i in range(0, len(batch), tokenizer_batch_size):\n                        yield batch[i:i+tokenizer_batch_size], (pq_idx, rg_idx)\n                    rg_idx += ddp_world_size\n                pq_idx += 1\n            first_pass = False\n    batches = document_batches()\n\n    needed_tokens = B * T + 1\n    tokenizer = get_tokenizer()\n    bos_token = tokenizer.get_bos_token_id()\n    token_buffer = deque()\n    while True:\n        while len(token_buffer) < needed_tokens:\n            doc_batch, (pq_idx, rg_idx) = next(batches)\n            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n            for tokens in token_lists:\n                token_buffer.extend(tokens)\n        tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n        use_cuda_optimizations = device == \"cuda\"\n        scratch = torch.tensor(tokens, dtype=torch.long, pin_memory=use_cuda_optimizations)\n        inputs_cpu = scratch[:-1]\n        targets_cpu = scratch[1:]\n        inputs = inputs_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)\n        targets = targets_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)\n        state_dict = {\"pq_idx\": pq_idx, \"rg_idx\": rg_idx}\n        yield inputs, targets, state_dict\n",
      "highlight": true,
      "voiceover": "This function creates an infinite data loader that streams pretraining text from parquet files, tokenizes it on-the-fly, and yields batches of training data with state tracking for approximate resume capability. It implements a sophisticated streaming pipeline that handles distributed training by partitioning data across multiple processes, where each rank processes a strided subset of row groups from the parquet files. The function supports two splits: train uses all parquet files except the last one, while val uses only the last file for validation. The data loading is designed to be stateless and resumable, returning a state dictionary with each batch that tracks the current parquet file index and row group index, allowing training to be approximately resumed from a checkpoint by skipping already-processed data. The approximation comes from the fact that resuming advances by one full DDP stride to guarantee no data repetition, which may skip a few documents but ensures training correctness. The function uses a nested generator pattern where an inner document_batches generator yields raw text batches from parquet files, and the outer loop tokenizes these batches and accumulates tokens in a deque buffer until enough tokens are available for a complete training batch. The tokenization happens in configurable batch sizes for efficiency, and the final tensors are created with CUDA memory pinning when using GPU devices to enable asynchronous CPU-to-GPU transfers. The function yields tuples of input tensors, target tensors (shifted by one position for autoregressive training), and state dictionaries, providing everything needed for distributed pretraining with resume support.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "pq_idx = resume_pq_idx",
      "voiceover": "we kick off parquet files at the resume index (or by default just 0)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "while True:",
      "voiceover": "iterate infinitely (multi-epoch)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "while pq_idx < len(parquet_paths):",
      "voiceover": "iterate over all parquet files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "base_idx = resume_rg_idx // ddp_world_size",
      "voiceover": "in units of ddp_world_size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "base_idx += 1",
      "voiceover": "advance by 1 so that we definitely don't repeat data after resuming",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "resume_rg_idx = None",
      "voiceover": "set to None as we only want to do this a single time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "batch = rg.column('text').to_pylist()",
      "voiceover": "each batch is a parquet group, e.g. 1024 rows",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "rg_idx += ddp_world_size",
      "voiceover": "advance to the next row group (in DDP)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "pq_idx += 1",
      "voiceover": "advance to the next parquet file",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "needed_tokens = B * T + 1",
      "voiceover": "+1 is because we also need the target at the last token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "token_buffer = deque()",
      "voiceover": "we stream tokens on the right and pop from the left",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "scratch = torch.tensor(tokens, dtype=torch.long, pin_memory=use_cuda_optimizations)",
      "voiceover": "in PyTorch, long=int64",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataloader.py",
      "find": "state_dict = {\"pq_idx\": pq_idx, \"rg_idx\": rg_idx}",
      "voiceover": "we need this in case we wish to approximately resume training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def tokenizing_distributed_data_loader(*args, **kwargs):\n    for inputs, targets, state_dict in tokenizing_distributed_data_loader_with_state(*args, **kwargs):\n        yield inputs, targets\n",
      "highlight": true,
      "voiceover": "This is a convenience wrapper function that provides a simplified interface to the data loader by yielding only the input and target tensors without the state dictionary. It forwards all arguments to tokenizing_distributed_data_loader_with_state and unpacks each yielded tuple, discarding the state dictionary and yielding only the inputs and targets. This function is useful when state tracking and resume capability are not needed, such as during evaluation or when using external checkpointing mechanisms. The wrapper design allows callers to choose between the full-featured version with state tracking or this simpler version based on their needs, without duplicating the core data loading logic.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "nanochat/dataset.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport argparse\nimport time\nimport requests\nimport pyarrow.parquet as pq\nfrom multiprocessing import Pool\n\nfrom nanochat.common import get_base_dir\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "import os",
      "voiceover": "Operating system interface for file path operations and directory management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "import argparse",
      "voiceover": "Command-line argument parsing for the download script",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "import time",
      "voiceover": "Time utilities for implementing exponential backoff during retries",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "import requests",
      "voiceover": "HTTP library for downloading files from remote URLs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "import pyarrow.parquet as pq",
      "voiceover": "Apache Arrow Parquet library for reading columnar data files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "from multiprocessing import Pool",
      "voiceover": "Process pool for parallel file downloads",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "from nanochat.common import get_base_dir",
      "voiceover": "Utility to get the base directory for nanochat data storage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"",
      "voiceover": "The URL on the internet where the data is hosted and downloaded from on demand",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_SHARD = 1822\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "MAX_SHARD = 1822",
      "voiceover": "The last datashard is shard_01822.parquet, defining the total number of available shards",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\"\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\"",
      "voiceover": "Lambda function to convert shard index to filename with zero-padded formatting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "base_dir = get_base_dir()\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get the base directory where nanochat stores all its data and checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "DATA_DIR = os.path.join(base_dir, \"base_data\")\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "DATA_DIR = os.path.join(base_dir, \"base_data\")",
      "voiceover": "Construct the full path to the directory where parquet data files are stored",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.makedirs(DATA_DIR, exist_ok=True)\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "os.makedirs(DATA_DIR, exist_ok=True)",
      "voiceover": "Create the data directory if it doesn't exist, ensuring it's ready for file operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def list_parquet_files(data_dir=None):\n    data_dir = DATA_DIR if data_dir is None else data_dir\n    parquet_files = sorted([\n        f for f in os.listdir(data_dir)\n        if f.endswith('.parquet') and not f.endswith('.tmp')\n    ])\n    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]\n    return parquet_paths\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this dataset management module\nThe base/pretraining dataset is a set of parquet files.\nThis file contains utilities for:\n- iterating over the parquet files and yielding documents from it\n- download the files on demand if they are not on disk\n\nFor details of how the dataset was prepared, see `repackage_data_reference.py`.\n\nThis function scans a data directory and returns the full paths to all parquet files found within it, sorted alphabetically. It takes an optional data_dir parameter that defaults to the module-level DATA_DIR if not provided, allowing flexibility for different data locations. The function filters the directory listing to include only files with the .parquet extension while excluding temporary files ending in .tmp, which may be present during downloads. The files are sorted to ensure consistent ordering across different systems and runs, which is important for reproducible data loading. The function constructs and returns absolute paths by joining the directory path with each filename, making the returned paths ready for use with file I/O operations. This utility is used by the dataloader and other modules to discover available dataset files.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def parquets_iter_batched(split, start=0, step=1):\n    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n    parquet_paths = list_parquet_files()\n    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n    for filepath in parquet_paths:\n        pf = pq.ParquetFile(filepath)\n        for rg_idx in range(start, pf.num_row_groups, step):\n            rg = pf.read_row_group(rg_idx)\n            texts = rg.column('text').to_pylist()\n            yield texts\n",
      "highlight": true,
      "voiceover": "This function creates a generator that iterates through the dataset in batches corresponding to the underlying row groups in the parquet files, which is more efficient than reading individual rows. It supports two splits: train uses all parquet files except the last one, while val uses only the last file for validation. The start and step parameters enable distributed data parallel training by allowing different processes to read disjoint subsets of the data, where each process starts at its rank and advances by the world size. For each parquet file in the selected split, the function opens it using PyArrow and iterates through row groups with the specified stride pattern. Each row group is read and the text column is extracted as a Python list, which is then yielded as a batch. This batched approach reduces I/O overhead and provides natural batch boundaries for efficient processing, making it suitable for streaming large datasets that don't fit in memory.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def download_single_file(index):\n\n    filename = index_to_filename(index)\n    filepath = os.path.join(DATA_DIR, filename)\n    if os.path.exists(filepath):\n        print(f\"Skipping {filepath} (already exists)\")\n        return True\n\n    url = f\"{BASE_URL}/{filename}\"\n    print(f\"Downloading {filename}...\")\n\n    max_attempts = 5\n    for attempt in range(1, max_attempts + 1):\n        try:\n            response = requests.get(url, stream=True, timeout=30)\n            response.raise_for_status()\n            temp_path = filepath + f\".tmp\"\n            with open(temp_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=1024 * 1024):\n                    if chunk:\n                        f.write(chunk)\n            os.rename(temp_path, filepath)\n            print(f\"Successfully downloaded {filename}\")\n            return True\n\n        except (requests.RequestException, IOError) as e:\n            print(f\"Attempt {attempt}/{max_attempts} failed for {filename}: {e}\")\n            for path in [filepath + f\".tmp\", filepath]:\n                if os.path.exists(path):\n                    try:\n                        os.remove(path)\n                    except:\n                        pass\n            if attempt < max_attempts:\n                wait_time = 2 ** attempt\n                print(f\"Waiting {wait_time} seconds before retry...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"Failed to download {filename} after {max_attempts} attempts\")\n                return False\n\n    return False\n",
      "highlight": true,
      "voiceover": "This function downloads a single parquet file from the remote dataset repository with robust error handling and retry logic. It takes a shard index and constructs the local filepath using the index_to_filename function, first checking if the file already exists to avoid redundant downloads. If the file needs to be downloaded, it constructs the remote URL and initiates an HTTP GET request with streaming enabled to handle large files efficiently. The function implements exponential backoff retry logic with up to 5 attempts, where each retry waits for 2^attempt seconds before trying again, handling transient network errors gracefully. During download, the file is first written to a temporary location with a .tmp extension, then atomically renamed to the final location only after successful completion, preventing corruption from partial downloads. If any error occurs during a retry attempt, the function cleans up any partial or temporary files before retrying. The function returns True on successful download or if the file already exists, and False if all retry attempts fail. This design ensures reliable dataset acquisition even in the presence of network instability.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "for chunk in response.iter_content(chunk_size=1024 * 1024):",
      "voiceover": "1MB chunks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Download FineWeb-Edu 100BT dataset shards\")\n    parser.add_argument(\"-n\", \"--num-files\", type=int, default=-1, help=\"Number of shards to download (default: -1), -1 = disable\")\n    parser.add_argument(\"-w\", \"--num-workers\", type=int, default=4, help=\"Number of parallel download workers (default: 4)\")\n    args = parser.parse_args()\n\n    num = MAX_SHARD + 1 if args.num_files == -1 else min(args.num_files, MAX_SHARD + 1)\n    ids_to_download = list(range(num))\n    print(f\"Downloading {len(ids_to_download)} shards using {args.num_workers} workers...\")\n    print(f\"Target directory: {DATA_DIR}\")\n    print()\n    with Pool(processes=args.num_workers) as pool:\n        results = pool.map(download_single_file, ids_to_download)\n\n    successful = sum(1 for success in results if success)\n    print(f\"Done! Downloaded: {successful}/{len(ids_to_download)} shards to {DATA_DIR}\")\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Main block that executes when the script is run directly for dataset downloading",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "parser = argparse.ArgumentParser(description=\"Download FineWeb-Edu 100BT dataset shards\")",
      "voiceover": "Create argument parser for command-line interface",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "parser.add_argument(\"-n\", \"--num-files\", type=int, default=-1, help=\"Number of shards to download (default: -1), -1 = disable\")",
      "voiceover": "Option to limit number of files to download",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "parser.add_argument(\"-w\", \"--num-workers\", type=int, default=4, help=\"Number of parallel download workers (default: 4)\")",
      "voiceover": "Option to control parallelism",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parse command-line arguments into args object",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "num = MAX_SHARD + 1 if args.num_files == -1 else min(args.num_files, MAX_SHARD + 1)",
      "voiceover": "Determine how many shards to download, capping at maximum available",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "ids_to_download = list(range(num))",
      "voiceover": "Create list of shard indices to download",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "print(f\"Downloading {len(ids_to_download)} shards using {args.num_workers} workers...\")",
      "voiceover": "Log download plan",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "print(f\"Target directory: {DATA_DIR}\")",
      "voiceover": "Show where files will be saved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "print()",
      "voiceover": "Print blank line for readability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "with Pool(processes=args.num_workers) as pool:",
      "voiceover": "Create process pool for parallel downloads",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "results = pool.map(download_single_file, ids_to_download)",
      "voiceover": "Download files in parallel, collecting results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "successful = sum(1 for success in results if success)",
      "voiceover": "Count how many downloads succeeded",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/dataset.py",
      "find": "print(f\"Done! Downloaded: {successful}/{len(ids_to_download)} shards to {DATA_DIR}\")",
      "voiceover": "Print final summary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/engine.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import torch\nimport torch.nn.functional as F\nimport signal\nimport warnings\nfrom contextlib import contextmanager\nfrom collections import deque\nfrom nanochat.common import compute_init, autodetect_device_type\nfrom nanochat.checkpoint_manager import load_model\nfrom contextlib import nullcontext\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and model inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "import torch.nn.functional as F",
      "voiceover": "Functional API for operations like softmax used in sampling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "import signal",
      "voiceover": "Signal handling for implementing timeout functionality in calculator tool",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "import warnings",
      "voiceover": "Warning control for suppressing syntax warnings during eval",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "from contextlib import contextmanager",
      "voiceover": "Context manager decorator for creating timeout contexts",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "from collections import deque",
      "voiceover": "Double-ended queue for efficient token forcing in generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "from nanochat.common import compute_init, autodetect_device_type",
      "voiceover": "Utilities for device initialization and detection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Function to load trained models from checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "No-op context manager for conditional autocast usage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@contextmanager\n"
    },
    {
      "type": "writeText",
      "content": "def timeout(duration, formula):\n    def timeout_handler(signum, frame):\n        raise Exception(f\"'{formula}': timed out after {duration} seconds\")\n\n    signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(duration)\n    yield\n    signal.alarm(0)\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this efficient inference engine module\nEngine for efficient inference of our models.\n\nEverything works around token sequences:\n- The user can send token sequences to the engine\n- The engine returns the next token\n\nNotes:\n- The engine knows nothing about tokenization, it's purely token id sequences.\n\nThe whole thing is made as efficient as possible.\n\nThis context manager implements a timeout mechanism using Unix signals to prevent infinite loops or long-running computations during calculator expression evaluation. It sets up a signal handler that raises an exception if the code block doesn't complete within the specified duration. The function uses SIGALRM to schedule an alarm that triggers after the given number of seconds, and the handler raises an exception with a descriptive message including the formula that timed out. After the context block completes successfully, the alarm is cancelled by setting it to 0. This design provides a safety mechanism to prevent malicious or accidentally complex expressions from hanging the inference engine, ensuring responsive generation even when tool use is enabled.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def eval_with_timeout(formula, max_time=3):\n    try:\n        with timeout(max_time, formula):\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", SyntaxWarning)\n                return eval(formula, {\"__builtins__\": {}}, {})\n    except Exception as e:\n        signal.alarm(0)\n        return None\n",
      "highlight": true,
      "voiceover": "This function safely evaluates a Python expression with a timeout constraint and suppressed warnings. It wraps the eval call in the timeout context manager to prevent long-running or infinite computations, and uses warnings.catch_warnings to suppress SyntaxWarning messages that might occur during expression evaluation. The function evaluates the formula in a restricted environment with no builtins, preventing access to dangerous functions or modules. If the evaluation completes successfully within the time limit, it returns the result. If any exception occurs (timeout, syntax error, runtime error, etc.), the function cancels any pending alarm and returns None, allowing the caller to handle failures gracefully. This design provides a safe sandbox for evaluating user-provided expressions in the calculator tool while preventing security risks and ensuring the inference engine remains responsive.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def use_calculator(expr):\n    expr = expr.replace(\",\", \"\")\n\n    if all([x in \"0123456789*+-/.() \" for x in expr]):\n        if \"**\" in expr:\n            return None\n        return eval_with_timeout(expr)\n\n    allowed_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\\\"()._ \"\n    if not all([x in allowed_chars for x in expr]):\n        return None\n\n    dangerous_patterns = ['__', 'import', 'exec', 'eval', 'compile', 'open', 'file',\n                         'input', 'raw_input', 'globals', 'locals', 'vars', 'dir',\n                         'getattr', 'setattr', 'delattr', 'hasattr']\n    expr_lower = expr.lower()\n    if any(pattern in expr_lower for pattern in dangerous_patterns):\n        return None\n\n    if '.count(' not in expr:\n        return None\n\n    return eval_with_timeout(expr)\n",
      "highlight": true,
      "voiceover": "This function provides a safe calculator tool that evaluates Python expressions for both mathematical operations and string methods like count. It implements multiple layers of security to prevent code injection and malicious usage. The function first removes commas from numbers for cleaner parsing, then checks if the expression is a pure mathematical expression containing only digits and basic operators. For pure math expressions, it disallows the power operator to prevent exponential complexity attacks. For non-math expressions, it validates that all characters are in an allowed set (letters, numbers, quotes, parentheses, dots, underscores, and spaces), then checks for dangerous patterns like double underscores, import statements, and various builtin functions that could be exploited. Currently, the function only allows the .count() string method, though it's designed to be extensible for other safe methods. All expressions are evaluated using eval_with_timeout to prevent infinite loops or long computations. This multi-layered security approach ensures the calculator tool can be safely used during model generation without compromising system security or responsiveness.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "if \"**\" in expr:",
      "voiceover": "disallow power operator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class KVCache:\n",
      "highlight": true,
      "voiceover": "This class manages the key-value cache for efficient transformer inference, working in tandem with the GPT model to store and retrieve cached attention keys and values across generation steps. The cache eliminates redundant computation by storing the keys and values from previous tokens, allowing the model to only process new tokens during autoregressive generation. The cache maintains a position counter that automatically advances after the last transformer layer processes each token, ensuring synchronized state across all layers. The cache is designed to handle batched generation where multiple sequences are generated in parallel, and supports dynamic growth when sequences exceed the initially allocated size. The cache shape is (num_layers, 2, batch_size, num_heads, seq_len, head_dim) where the second dimension holds keys (index 0) and values (index 1) separately. This design enables efficient memory management and fast lookup during the attention computation in each transformer layer.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, batch_size, num_heads, seq_len, head_dim, num_layers):\n        self.kv_shape = (num_layers, 2, batch_size, num_heads, seq_len, head_dim)\n        self.kv_cache = None\n        self.pos = 0\n",
      "highlight": true,
      "voiceover": "This initializer sets up the cache structure with the specified dimensions but doesn't allocate memory yet, using lazy initialization to determine dtype and device from the first inserted tensors. It stores the target shape for the cache and initializes the position counter to zero. The lazy allocation strategy ensures the cache is created on the correct device and with the correct dtype without requiring explicit specification.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.pos = 0",
      "voiceover": "current position in time in the cache",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def reset(self):\n        self.pos = 0\n",
      "highlight": true,
      "voiceover": "This method resets the position counter to zero, effectively clearing the cache state without deallocating memory. It's used when starting a new generation sequence with the same cache object.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_pos(self):\n        return self.pos\n",
      "highlight": true,
      "voiceover": "This method returns the current position in the cache, indicating how many tokens have been processed and stored. It's useful for tracking generation progress and debugging.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def prefill(self, other):\n        assert self.kv_cache is None, \"Cannot prefill a non-empty KV cache\"\n        assert other.kv_cache is not None, \"Cannot prefill with a None KV cache\"\n        \n        self_layers, self_kv, self_batch, self_heads, self_seq, self_head_dim = self.kv_shape\n        other_layers, other_kv, other_batch, other_heads, other_seq, other_head_dim = other.kv_shape\n        \n        assert self_layers == other_layers, f\"Layer count mismatch: {self_layers} != {other_layers}\"\n        assert self_kv == other_kv, f\"K/V dimension mismatch: {self_kv} != {other_kv}\"\n        assert self_heads == other_heads, f\"Head count mismatch: {self_heads} != {other_heads}\"\n        assert self_head_dim == other_head_dim, f\"Head dim mismatch: {self_head_dim} != {other_head_dim}\"\n        \n        assert self_batch == other_batch or other_batch == 1, f\"Batch size mismatch: {self_batch} vs {other_batch} (other must be 1 or equal)\"\n        \n        assert self_seq >= other_seq, f\"Sequence length mismatch: {self_seq} < {other_seq}\"\n        \n        dtype, device = other.kv_cache.dtype, other.kv_cache.device\n        self.kv_cache = torch.empty(self.kv_shape, dtype=dtype, device=device)\n        self.kv_cache[:, :, :, :, :other.pos, :] = other.kv_cache\n        self.pos = other.pos\n",
      "highlight": true,
      "voiceover": "This method initializes the current cache by copying data from another KVCache, with optional batch dimension expansion for parallel generation. It's designed for the common pattern where a prompt is processed once with batch size 1, then multiple samples are generated in parallel from that prefilled state. The method performs extensive validation to ensure dimensional compatibility between the source and target caches, checking that layer count, K/V dimension, head count, and head dimension all match exactly. The batch size can differ, with the constraint that the source must have batch size 1 or equal to the target, enabling broadcasting of the prefilled state across multiple generation samples. The sequence length of the target must be at least as long as the source to accommodate the prefilled tokens plus additional generation. After validation, the method allocates the target cache with the same dtype and device as the source, copies the prefilled data up to the source position, and updates the target position to match. This design enables efficient multi-sample generation by avoiding redundant prefill computation for each sample.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def insert_kv(self, layer_idx, k, v):\n        if self.kv_cache is None:\n            self.kv_cache = torch.empty(self.kv_shape, dtype=k.dtype, device=k.device)\n        B, H, T_add, D = k.size()\n        t0, t1 = self.pos, self.pos + T_add\n        if t1 > self.kv_cache.size(4):\n            t_needed = t1 + 1024\n            t_needed = (t_needed + 1023) & ~1023\n            additional_shape = list(self.kv_cache.shape)\n            additional_shape[4] = t_needed - self.kv_cache.size(4)\n            additional_cache = torch.empty(additional_shape, dtype=k.dtype, device=k.device)\n            self.kv_cache = torch.cat([self.kv_cache, additional_cache], dim=4).contiguous()\n            self.kv_shape = self.kv_cache.shape\n        self.kv_cache[layer_idx, 0, :, :, t0:t1, :] = k\n        self.kv_cache[layer_idx, 1, :, :, t0:t1, :] = v\n        key_view = self.kv_cache[layer_idx, 0, :, :, :t1, :]\n        value_view = self.kv_cache[layer_idx, 1, :, :, :t1, :]\n        if layer_idx == self.kv_cache.size(0) - 1:\n            self.pos = t1\n        return key_view, value_view\n",
      "highlight": true,
      "voiceover": "This method inserts new key and value tensors into the cache for a specific transformer layer and returns views of the complete cached keys and values up to the current position. It performs lazy initialization of the cache on the first insertion, using the dtype and device from the provided tensors. The method calculates the insertion range based on the current position and the number of new tokens being added. If the new tokens would exceed the allocated cache size, the method dynamically grows the cache by allocating additional space in chunks of 1024 tokens (rounded up to the nearest multiple of 1024 for memory alignment), then concatenates the new space to the existing cache. After ensuring sufficient capacity, it inserts the new keys and values at the appropriate positions and returns views (not copies) of the full cached sequences up to the current position, which the transformer layer uses for attention computation. The position counter is automatically incremented after the last layer processes, ensuring all layers see the same position during a single forward pass. This design enables efficient memory usage through lazy allocation and dynamic growth while maintaining fast access patterns for the transformer's attention mechanism.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t_needed = t1 + 1024",
      "voiceover": "as much as we need plus buffer of 1024",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t_needed = (t_needed + 1023) & ~1023",
      "voiceover": "then round up to the nearest multiple of 1024",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n@torch.inference_mode()\n"
    },
    {
      "type": "writeText",
      "content": "def sample_next_token(logits, rng, temperature=1.0, top_k=None):\n    assert temperature >= 0.0, \"temperature must be non-negative\"\n    if temperature == 0.0:\n        return torch.argmax(logits, dim=-1, keepdim=True)\n    if top_k is not None:\n        k = min(top_k, logits.size(-1))\n        vals, idx = torch.topk(logits, k, dim=-1)\n        vals = vals / temperature\n        probs = F.softmax(vals, dim=-1)\n        choice = torch.multinomial(probs, num_samples=1, generator=rng)\n        return idx.gather(1, choice)\n    else:\n        logits = logits / temperature\n        probs = F.softmax(logits, dim=-1)\n        return torch.multinomial(probs, num_samples=1, generator=rng)\n",
      "highlight": true,
      "voiceover": "This function samples the next token from model logits using temperature-controlled sampling with optional top-k filtering. It takes logits of shape (B, vocab_size) and returns sampled token indices of shape (B, 1). The temperature parameter controls randomness: temperature 0.0 produces deterministic argmax selection, while higher temperatures increase randomness by flattening the probability distribution. When top_k is specified, the function first selects the k highest-probability tokens, then samples from only those candidates, which helps avoid sampling very low-probability tokens that might produce incoherent text. The function uses the provided random number generator for reproducible sampling. For top-k sampling, it retrieves the top k logits and their indices, applies temperature scaling, converts to probabilities via softmax, samples from the multinomial distribution, and maps the sampled index back to the original vocabulary using gather. For standard sampling without top-k, it applies temperature scaling and softmax to the full logit distribution before sampling. This design provides flexible control over generation quality and diversity through temperature and top-k parameters.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class RowState:\n",
      "highlight": true,
      "voiceover": "This class tracks the per-row state during parallel batch generation, managing token sequences, tool use state, and completion status for each independent generation sample. It maintains the current token sequence being generated, a queue of tokens to force-inject (used for tool outputs), state flags for tracking whether the generation is inside a Python code block, accumulated tokens for the current Python expression, and a completion flag indicating whether this row has finished generating. The class is designed to support tool use where the model can generate Python expressions that are evaluated and their results injected back into the generation stream. The deque for forced tokens enables efficient FIFO insertion of tool output tokens that bypass the normal sampling process. This state management design allows each row in a batch to independently track its generation progress and tool use state, enabling efficient parallel generation of multiple samples with tool use support.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, current_tokens=None):\n        self.current_tokens = current_tokens or []\n        self.forced_tokens = deque()\n        self.in_python_block = False\n        self.python_expr_tokens = []\n        self.completed = False\n",
      "highlight": true,
      "voiceover": "This initializer sets up the state tracking for a single generation row, optionally starting with a provided token sequence. It initializes the current token list, creates an empty deque for forced tokens, sets tool use flags to their initial states, and marks the row as not completed. This initialization prepares the row for generation with tool use support.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.current_tokens = current_tokens or []",
      "voiceover": "Current token sequence for this row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.forced_tokens = deque()",
      "voiceover": "Queue of tokens to force inject",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.in_python_block = False",
      "voiceover": "Whether we are inside a python block",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.python_expr_tokens = []",
      "voiceover": "Tokens of the current python expression",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.completed = False",
      "voiceover": "Whether this row has completed generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class Engine:\n",
      "highlight": true,
      "voiceover": "This class provides an efficient inference engine for GPT models with support for KV caching, parallel batch generation, and tool use. It wraps a model and tokenizer to provide high-level generation APIs that handle the complexities of efficient autoregressive sampling, cache management, and tool integration. The engine implements a sophisticated generation pipeline that performs a single prefill pass on the prompt, then replicates the KV cache for parallel generation of multiple samples, avoiding redundant computation. It supports tool use through a state machine that detects special tokens marking Python code blocks, evaluates the expressions using the calculator tool, and injects the results back into the generation stream. The engine is designed for maximum efficiency, using KV caching to avoid recomputing attention for previous tokens, batched operations for parallel sample generation, and streaming output that yields tokens as they're generated rather than waiting for completion. This design makes it suitable for interactive applications and scenarios requiring multiple diverse samples from a single prompt.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n",
      "highlight": true,
      "voiceover": "This initializer sets up the engine with a model and tokenizer. The tokenizer is needed for tool use to encode and decode Python expressions and their results. The model is stored for inference during generation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "self.tokenizer = tokenizer",
      "voiceover": "needed for tool use",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n    @torch.inference_mode()\n"
    },
    {
      "type": "writeText",
      "content": "    def generate(self, tokens, num_samples=1, max_tokens=None, temperature=1.0, top_k=None, seed=42):\n        assert isinstance(tokens, list) and isinstance(tokens[0], int), \"expecting list of ints\"\n        device = self.model.get_device()\n        rng = torch.Generator(device=device)\n        rng.manual_seed(seed)\n\n        get_special = lambda s: self.tokenizer.encode_special(s)\n        python_start = get_special(\"<|python_start|>\")\n        python_end = get_special(\"<|python_end|>\")\n        output_start = get_special(\"<|output_start|>\")\n        output_end = get_special(\"<|output_end|>\")\n        assistant_end = get_special(\"<|assistant_end|>\")\n        bos = self.tokenizer.get_bos_token_id()\n\n        m = self.model.config\n        kv_model_kwargs = {\"num_heads\": m.n_kv_head, \"head_dim\": m.n_embd // m.n_head, \"num_layers\": m.n_layer}\n        kv_cache_prefill = KVCache(\n            batch_size=1,\n            seq_len=len(tokens),\n            **kv_model_kwargs,\n        )\n        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n        logits = self.model.forward(ids, kv_cache=kv_cache_prefill)\n        logits = logits[:, -1, :]\n        next_ids = sample_next_token(logits, rng, temperature, top_k)\n        sampled_tokens = next_ids[:, 0].tolist()\n\n        kv_length_hint = (len(tokens) + max_tokens) if max_tokens is not None else self.model.config.sequence_len\n        kv_cache_decode = KVCache(\n            batch_size=num_samples,\n            seq_len=kv_length_hint,\n            **kv_model_kwargs,\n        )\n        kv_cache_decode.prefill(kv_cache_prefill)\n        del kv_cache_prefill\n\n        row_states = [RowState(tokens.copy()) for _ in range(num_samples)]\n\n        num_generated = 0\n        first_iteration = True\n        while True:\n            if max_tokens is not None and num_generated >= max_tokens:\n                break\n            if all(state.completed for state in row_states):\n                break\n\n            if first_iteration:\n                sampled_tokens = [sampled_tokens[0]] * num_samples\n                first_iteration = False\n            else:\n                logits = self.model.forward(ids, kv_cache=kv_cache_decode)\n                logits = logits[:, -1, :]\n                next_ids = sample_next_token(logits, rng, temperature, top_k)\n                sampled_tokens = next_ids[:, 0].tolist()\n\n            token_column = []\n            token_masks = []\n            for i, state in enumerate(row_states):\n                is_forced = len(state.forced_tokens) > 0\n                token_masks.append(0 if is_forced else 1)\n                next_token = state.forced_tokens.popleft() if is_forced else sampled_tokens[i]\n                token_column.append(next_token)\n                state.current_tokens.append(next_token)\n                if next_token == assistant_end or next_token == bos:\n                    state.completed = True\n                if next_token == python_start:\n                    state.in_python_block = True\n                    state.python_expr_tokens = []\n                elif next_token == python_end and state.in_python_block:\n                    state.in_python_block = False\n                    if state.python_expr_tokens:\n                        expr = self.tokenizer.decode(state.python_expr_tokens)\n                        result = use_calculator(expr)\n                        if result is not None:\n                            result_tokens = self.tokenizer.encode(str(result))\n                            state.forced_tokens.append(output_start)\n                            state.forced_tokens.extend(result_tokens)\n                            state.forced_tokens.append(output_end)\n                    state.python_expr_tokens = []\n                elif state.in_python_block:\n                    state.python_expr_tokens.append(next_token)\n\n            yield token_column, token_masks\n            num_generated += 1\n            ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)\n",
      "highlight": true,
      "voiceover": "This method implements streaming generation of multiple token sequences in parallel with KV cache replication and tool use support. It takes a prompt token sequence and generates num_samples different continuations, yielding tokens column-by-column as they're generated. The method performs a single-batch prefill of the prompt to populate a KV cache, then replicates that cache across all samples to avoid redundant computation. It initializes per-row state tracking for each sample, including token sequences, forced token queues for tool outputs, and tool use state flags. The main generation loop continues until either the maximum token limit is reached or all rows complete (by generating assistant_end or bos tokens). For each step, the method either uses the prefilled tokens on the first iteration or runs a forward pass to get new logits. It processes each row independently, choosing between sampled tokens and forced tokens from the tool output queue, updating the row state, and handling the tool use state machine. When a Python code block is detected (between python_start and python_end tokens), the method accumulates the expression tokens, evaluates them using the calculator tool, and queues the result tokens for forced injection. The method yields token columns and mask columns (indicating whether each token was sampled or forced) for streaming output, allowing callers to process tokens as they're generated. This design enables efficient parallel generation with tool use while maintaining streaming output for responsive user interfaces.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "assistant_end = get_special(\"<|assistant_end|>\")",
      "voiceover": "if sampled, ends row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "bos = self.tokenizer.get_bos_token_id()",
      "voiceover": "if sampled, ends row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "next_ids = sample_next_token(logits, rng, temperature, top_k)",
      "voiceover": "(B, 1)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "del kv_cache_prefill",
      "voiceover": "no need to keep this memory around",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "sampled_tokens = [sampled_tokens[0]] * num_samples",
      "voiceover": "Broadcast first token to all rows",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "logits = self.model.forward(ids, kv_cache=kv_cache_decode)",
      "voiceover": "(B, T, vocab_size)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "logits = logits[:, -1, :]",
      "voiceover": "(B, vocab_size) at last time step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "next_ids = sample_next_token(logits, rng, temperature, top_k)",
      "voiceover": "(B, 1)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "token_column = []",
      "voiceover": "contains the next token id along each row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "token_masks = []",
      "voiceover": "contains the mask (was it sampled (1) or forced (0)?) along each row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "is_forced = len(state.forced_tokens) > 0",
      "voiceover": "are there tokens waiting to be forced in deque?",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "token_masks.append(0 if is_forced else 1)",
      "voiceover": "mask is 0 if forced, 1 if sampled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def generate_batch(self, tokens, num_samples=1, **kwargs):\n        assistant_end = self.tokenizer.encode_special(\"<|assistant_end|>\")\n        bos = self.tokenizer.get_bos_token_id()\n        results = [tokens.copy() for _ in range(num_samples)]\n        masks = [[0] * len(tokens) for _ in range(num_samples)]\n        completed = [False] * num_samples\n        for token_column, token_masks in self.generate(tokens, num_samples, **kwargs):\n            for i, (token, mask) in enumerate(zip(token_column, token_masks)):\n                if not completed[i]:\n                    if token == assistant_end or token == bos:\n                        completed[i] = True\n                    else:\n                        results[i].append(token)\n                        masks[i].append(mask)\n            if all(completed):\n                break\n        return results, masks\n",
      "highlight": true,
      "voiceover": "This method provides a non-streaming batch generation interface that returns complete token sequences rather than yielding tokens incrementally. It wraps the streaming generate method and accumulates all generated tokens for each sample, excluding terminal tokens (assistant_end and bos) from the results. The method initializes result lists with copies of the input prompt for each sample, along with mask lists tracking which tokens were sampled versus forced. It iterates through the streaming generator, appending tokens to the appropriate result sequences until they're marked as completed by encountering a terminal token. The method stops early if all samples complete before reaching the maximum token limit, improving efficiency. It returns both the complete token sequences and their corresponding masks, providing full information about the generation process. This design offers a simpler API for applications that don't need streaming output, while still leveraging the efficient parallel generation and tool use capabilities of the underlying streaming implementation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    import time\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()\n    device_type = autodetect_device_type()\n    autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n\n    model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")\n    bos_token_id = tokenizer.get_bos_token_id()\n    kwargs = dict(max_tokens=64, temperature=0.0)\n    prompt_tokens = tokenizer.encode(\"The chemical formula of water is\", prepend=bos_token_id)\n    generated_tokens = []\n    torch.cuda.synchronize()\n    t0 = time.time()\n    stream = model.generate(prompt_tokens, **kwargs)\n    with autocast_ctx:\n        for token in stream:\n            generated_tokens.append(token)\n            chunk = tokenizer.decode([token])\n            print(chunk, end=\"\", flush=True)\n    print()\n    torch.cuda.synchronize()\n    t1 = time.time()\n    print(f\"Reference time: {t1 - t0:.2f}s\")\n    reference_ids = generated_tokens\n    generated_tokens = []\n    engine = Engine(model, tokenizer)\n    stream = engine.generate(prompt_tokens, num_samples=1, **kwargs)\n    torch.cuda.synchronize()\n    t0 = time.time()\n    with autocast_ctx:\n        for token_column, token_masks in stream:\n            token = token_column[0]\n            generated_tokens.append(token)\n            chunk = tokenizer.decode([token])\n            print(chunk, end=\"\", flush=True)\n    print()\n    torch.cuda.synchronize()\n    t1 = time.time()\n    print(f\"Engine time: {t1 - t0:.2f}s\")\n    for i in range(len(reference_ids)):\n        if reference_ids[i] != generated_tokens[i]:\n            print(f\"Mismatch at {i}: {reference_ids[i]} != {generated_tokens[i]}\")\n            break\n    print(f\"Match: {reference_ids == generated_tokens}\")\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Main block for testing and comparing Engine.generate against model.generate",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "import time",
      "voiceover": "Time module for measuring generation speed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()",
      "voiceover": "Initialize distributed training configuration and device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "device_type = autodetect_device_type()",
      "voiceover": "Detect the best available device (CUDA, MPS, or CPU)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Use bfloat16 autocast for CUDA, no-op for other devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")",
      "voiceover": "Load the base pretrained model in evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "bos_token_id = tokenizer.get_bos_token_id()",
      "voiceover": "Get the beginning-of-sequence token ID",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "kwargs = dict(max_tokens=64, temperature=0.0)",
      "voiceover": "Use deterministic generation (temperature=0) for exact comparison",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "prompt_tokens = tokenizer.encode(\"The chemical formula of water is\", prepend=bos_token_id)",
      "voiceover": "Encode the test prompt",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "generated_tokens = []",
      "voiceover": "List to accumulate tokens from reference generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "torch.cuda.synchronize()",
      "voiceover": "Synchronize CUDA operations before timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t0 = time.time()",
      "voiceover": "Start timing reference generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "stream = model.generate(prompt_tokens, **kwargs)",
      "voiceover": "Generate using the model's built-in method",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "with autocast_ctx:",
      "voiceover": "Apply autocast context for mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "for token in stream:",
      "voiceover": "Iterate through the streaming generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "generated_tokens.append(token)",
      "voiceover": "Accumulate each generated token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "chunk = tokenizer.decode([token])",
      "voiceover": "Decode token to text for display",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(chunk, end=\"\", flush=True)",
      "voiceover": "Print token immediately without newline",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print()",
      "voiceover": "Print newline after generation completes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "torch.cuda.synchronize()",
      "voiceover": "Synchronize CUDA operations after generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t1 = time.time()",
      "voiceover": "End timing reference generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(f\"Reference time: {t1 - t0:.2f}s\")",
      "voiceover": "Display reference generation time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "reference_ids = generated_tokens",
      "voiceover": "Store reference tokens for comparison",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "generated_tokens = []",
      "voiceover": "Reset list for Engine generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create Engine instance with model and tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "stream = engine.generate(prompt_tokens, num_samples=1, **kwargs)",
      "voiceover": "note: runs in fp32 # Generate using Engine (note: runs in fp32 not bfloat16)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "torch.cuda.synchronize()",
      "voiceover": "Synchronize CUDA operations before timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t0 = time.time()",
      "voiceover": "Start timing Engine generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "with autocast_ctx:",
      "voiceover": "Apply autocast context for mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "for token_column, token_masks in stream:",
      "voiceover": "Iterate through Engine's column-based streaming output",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "token = token_column[0]",
      "voiceover": "only print out the first row # Extract token from first (and only) row",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "generated_tokens.append(token)",
      "voiceover": "Accumulate each generated token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "chunk = tokenizer.decode([token])",
      "voiceover": "Decode token to text for display",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(chunk, end=\"\", flush=True)",
      "voiceover": "Print token immediately without newline",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print()",
      "voiceover": "Print newline after generation completes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "torch.cuda.synchronize()",
      "voiceover": "Synchronize CUDA operations after generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "t1 = time.time()",
      "voiceover": "End timing Engine generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(f\"Engine time: {t1 - t0:.2f}s\")",
      "voiceover": "Display Engine generation time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "for i in range(len(reference_ids)):",
      "voiceover": "Iterate through reference tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "if reference_ids[i] != generated_tokens[i]:",
      "voiceover": "Check for mismatch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(f\"Mismatch at {i}: {reference_ids[i]} != {generated_tokens[i]}\")",
      "voiceover": "Report first mismatch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "break",
      "voiceover": "Stop checking after first mismatch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/engine.py",
      "find": "print(f\"Match: {reference_ids == generated_tokens}\")",
      "voiceover": "Print overall match result",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/execution.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import contextlib\nimport faulthandler\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport signal\nimport tempfile\nfrom dataclasses import dataclass\nfrom typing import Optional\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import contextlib",
      "voiceover": "Context manager utilities for creating custom context managers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import faulthandler",
      "voiceover": "Fault handler for debugging segmentation faults and other crashes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import io",
      "voiceover": "Input/output utilities for capturing stdout and stderr",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import multiprocessing",
      "voiceover": "Multiprocessing library for running code in separate processes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import os",
      "voiceover": "Operating system interface for environment and file operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import platform",
      "voiceover": "Platform identification for OS-specific behavior",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import signal",
      "voiceover": "Signal handling for implementing timeout functionality",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "import tempfile",
      "voiceover": "Temporary file and directory creation for isolated execution",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "from dataclasses import dataclass",
      "voiceover": "Decorator for creating data classes with automatic methods",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "from typing import Optional",
      "voiceover": "Type hints for optional values",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n@dataclass\n"
    },
    {
      "type": "writeText",
      "content": "class ExecutionResult:\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this sandboxed execution module\nSandboxed execution utilities for running Python code that comes out of an LLM.\nAdapted from OpenAI HumanEval code:\nhttps://github.com/openai/human-eval/blob/master/human_eval/execution.py\n\nWhat is covered:\n- Each execution runs in its own process (can be killed if it hangs or crashes)\n- Execution is limited by a timeout to stop infinite loops\n- Memory limits are enforced by default (256MB)\n- stdout and stderr are captured and returned\n- Code runs in a temporary directory that is deleted afterwards\n- Dangerous functions are disabled (examples: os.system, os.kill, shutil.rmtree, subprocess.Popen)\n\nWhat is not covered:\n- Not a true security sandbox\n- Network access is not blocked (e.g. sockets could be opened)\n- Python's dynamic features (e.g. ctypes) could bypass restrictions\n- No kernel-level isolation (no seccomp, no containers, no virtualization)\n\nOverall this sandbox is good for evaluation of generated code and protects against\naccidental destructive behavior, but it is not safe against malicious adversarial code.\n\nThis dataclass encapsulates the result of executing Python code in a sandboxed environment, providing structured access to execution status, output, and error information. It stores a success flag indicating whether the code executed without exceptions, captured stdout and stderr streams as strings, an optional error message describing any exception that occurred, and boolean flags for timeout and memory limit violations. The class includes a custom repr method that provides a concise, readable representation by only including non-default fields in the output, making it easier to quickly understand execution results during debugging or logging. This design provides a clean interface for callers to inspect execution outcomes and handle different failure modes appropriately.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    success: bool\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "success: bool",
      "voiceover": "Whether the code executed successfully without exceptions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    stdout: str\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "stdout: str",
      "voiceover": "Captured standard output from the executed code",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    stderr: str\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "stderr: str",
      "voiceover": "Captured standard error from the executed code",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    error: Optional[str] = None\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "error: Optional[str] = None",
      "voiceover": "Error message if execution failed, None otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    timeout: bool = False\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "timeout: bool = False",
      "voiceover": "Whether execution was terminated due to timeout",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    memory_exceeded: bool = False\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "memory_exceeded: bool = False",
      "voiceover": "Whether execution was terminated due to memory limit",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __repr__(self):\n        parts = []\n        parts.append(f\"ExecutionResult(success={self.success}\")\n        if self.timeout:\n            parts.append(\", timeout=True\")\n        if self.memory_exceeded:\n            parts.append(\", memory_exceeded=True\")\n        if self.error:\n            parts.append(f\", error={self.error!r}\")\n        if self.stdout:\n            parts.append(f\", stdout={self.stdout!r}\")\n        if self.stderr:\n            parts.append(f\", stderr={self.stderr!r}\")\n        parts.append(\")\")\n        return \"\".join(parts)\n",
      "highlight": true,
      "voiceover": "This method generates a custom string representation of the ExecutionResult that only includes non-default fields, making the output more concise and readable. It builds the representation incrementally by appending only the fields that have meaningful values, such as timeout or memory_exceeded flags when True, or error, stdout, and stderr when non-empty. This design improves debugging and logging by focusing attention on the relevant information rather than cluttering output with default values.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@contextlib.contextmanager\n"
    },
    {
      "type": "writeText",
      "content": "def time_limit(seconds: float):\n    def signal_handler(signum, frame):\n        raise TimeoutException(\"Timed out!\")\n\n    signal.setitimer(signal.ITIMER_REAL, seconds)\n    signal.signal(signal.SIGALRM, signal_handler)\n    try:\n        yield\n    finally:\n        signal.setitimer(signal.ITIMER_REAL, 0)\n",
      "highlight": true,
      "voiceover": "This context manager implements a timeout mechanism using Unix signals to limit the execution time of code blocks. It sets up a signal handler that raises a TimeoutException when the specified time limit is exceeded, using SIGALRM and setitimer for precise timing control. The function configures the alarm before yielding control to the code block, then ensures the timer is cancelled in the finally block regardless of how the context exits. This design provides a reliable way to prevent infinite loops or long-running computations in executed code, ensuring the sandbox remains responsive and doesn't hang indefinitely.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@contextlib.contextmanager\n"
    },
    {
      "type": "writeText",
      "content": "def capture_io():\n    stdout_capture = io.StringIO()\n    stderr_capture = io.StringIO()\n    stdin_block = WriteOnlyStringIO()\n    with contextlib.redirect_stdout(stdout_capture):\n        with contextlib.redirect_stderr(stderr_capture):\n            with redirect_stdin(stdin_block):\n                yield stdout_capture, stderr_capture\n",
      "highlight": true,
      "voiceover": "This context manager captures stdout and stderr output while blocking stdin access during code execution. It creates StringIO objects to buffer stdout and stderr, and a WriteOnlyStringIO object that raises exceptions on read attempts to prevent executed code from requesting user input. The function uses contextlib's redirect utilities to temporarily replace the standard streams, then yields the capture objects for the caller to retrieve output after execution. This design ensures executed code cannot interact with the user or read sensitive input, while providing a clean way to collect and inspect all output produced during execution.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@contextlib.contextmanager\n"
    },
    {
      "type": "writeText",
      "content": "def create_tempdir():\n    with tempfile.TemporaryDirectory() as dirname:\n        with chdir(dirname):\n            yield dirname\n",
      "highlight": true,
      "voiceover": "This context manager creates a temporary directory and changes the working directory to it for the duration of code execution, then automatically cleans up both the directory change and the temporary directory afterwards. It uses Python's tempfile.TemporaryDirectory to ensure the directory is deleted even if exceptions occur, and the chdir context manager to safely restore the original working directory. This design isolates executed code in its own filesystem space, preventing it from accidentally or maliciously modifying files in the host system, while ensuring proper cleanup regardless of execution outcome.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class TimeoutException(Exception):\n",
      "highlight": true,
      "voiceover": "This custom exception is raised when code execution exceeds the specified time limit. It's used by the time_limit context manager to signal timeout conditions, allowing the execution framework to distinguish timeouts from other types of exceptions and handle them appropriately in the ExecutionResult.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    pass\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class WriteOnlyStringIO(io.StringIO):\n",
      "highlight": true,
      "voiceover": "This class extends StringIO to create a write-only stream that raises IOError on any read attempt, effectively blocking stdin access for executed code. It overrides all read-related methods (read, readline, readlines) to raise exceptions, and returns False from the readable method to indicate the stream cannot be read. This design prevents executed code from attempting to read user input, which could cause the execution to hang waiting for input that will never arrive, or could be used to exfiltrate information through timing attacks.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def read(self, *args, **kwargs):\n        raise IOError\n",
      "highlight": true,
      "voiceover": "This method raises IOError to prevent reading from stdin, ensuring executed code cannot request user input.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def readline(self, *args, **kwargs):\n        raise IOError\n",
      "highlight": true,
      "voiceover": "This method raises IOError to prevent reading lines from stdin, ensuring executed code cannot request user input.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def readlines(self, *args, **kwargs):\n        raise IOError\n",
      "highlight": true,
      "voiceover": "This method raises IOError to prevent reading multiple lines from stdin, ensuring executed code cannot request user input.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def readable(self, *args, **kwargs):\n        return False\n",
      "highlight": true,
      "voiceover": "This method returns False to indicate the stream is not readable, helping executed code detect that stdin is unavailable rather than hanging on read attempts.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class redirect_stdin(contextlib._RedirectStream):\n",
      "highlight": true,
      "voiceover": "This class provides a context manager for redirecting stdin, following the same pattern as contextlib's redirect_stdout and redirect_stderr. It extends the internal _RedirectStream base class and specifies stdin as the stream to redirect. This design enables the capture_io function to replace stdin with a WriteOnlyStringIO object, preventing executed code from reading user input.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "class redirect_stdin(contextlib._RedirectStream):",
      "voiceover": "type: ignore",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    _stream = \"stdin\"\n"
    },
    {
      "type": "writeText",
      "content": "\n\n@contextlib.contextmanager\n"
    },
    {
      "type": "writeText",
      "content": "def chdir(root):\n    if root == \".\":\n        yield\n        return\n    cwd = os.getcwd()\n    os.chdir(root)\n    try:\n        yield\n    finally:\n        os.chdir(cwd)\n",
      "highlight": true,
      "voiceover": "This context manager temporarily changes the current working directory to the specified root directory, then restores the original directory when the context exits. It handles the special case where root is the current directory (\".\") by yielding immediately without any directory change. For other paths, it saves the current working directory, changes to the new directory, yields control to the code block, and ensures the original directory is restored in the finally block regardless of exceptions. This design provides safe directory navigation that always returns to the original location, preventing executed code from leaving the process in an unexpected directory state.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def reliability_guard(maximum_memory_bytes: Optional[int] = None):\n\n    if platform.uname().system != \"Darwin\":\n        import resource\n        resource.setrlimit(resource.RLIMIT_AS, (maximum_memory_bytes, maximum_memory_bytes))\n        resource.setrlimit(resource.RLIMIT_DATA, (maximum_memory_bytes, maximum_memory_bytes))\n        resource.setrlimit(resource.RLIMIT_STACK, (maximum_memory_bytes, maximum_memory_bytes))\n\n    faulthandler.disable()\n\n    import builtins\n\n    builtins.exit = None\n    builtins.quit = None\n\n    import os\n\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n    os.kill = None\n    os.system = None\n    os.putenv = None\n    os.remove = None\n    os.removedirs = None\n    os.rmdir = None\n    os.fchdir = None\n    os.setuid = None\n    os.fork = None\n    os.forkpty = None\n    os.killpg = None\n    os.rename = None\n    os.renames = None\n    os.truncate = None\n    os.replace = None\n    os.unlink = None\n    os.fchmod = None\n    os.fchown = None\n    os.chmod = None\n    os.chown = None\n    os.chroot = None\n    os.fchdir = None\n    os.lchflags = None\n    os.lchmod = None\n    os.lchown = None\n    os.getcwd = None\n    os.chdir = None\n\n    import shutil\n\n    shutil.rmtree = None\n    shutil.move = None\n    shutil.chown = None\n\n    import subprocess\n\n    subprocess.Popen = None\n\n    __builtins__[\"help\"] = None\n\n    import sys\n\n    sys.modules[\"ipdb\"] = None\n    sys.modules[\"joblib\"] = None\n    sys.modules[\"resource\"] = None\n    sys.modules[\"psutil\"] = None\n    sys.modules[\"tkinter\"] = None\n",
      "highlight": true,
      "voiceover": "This function disables various destructive and dangerous functions to prevent executed code from interfering with the host system or the test environment. It implements multiple layers of protection by setting resource limits for memory usage (on non-macOS systems), disabling the faulthandler to prevent crash dumps, removing dangerous builtin functions like exit and quit, and nullifying numerous os, shutil, and subprocess functions that could be used for destructive operations like killing processes, removing files, or spawning subprocesses. The function also blocks access to certain modules like ipdb, joblib, resource, psutil, and tkinter that could be used to bypass restrictions or cause problems. The memory limits are enforced through resource.setrlimit calls that restrict address space, data segment, and stack size to the specified maximum. This design provides defense-in-depth against accidental or malicious code, though it's explicitly not a complete security sandbox and should not be relied upon for running truly untrusted code. The function is called before executing user code to establish these protections in the subprocess.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "subprocess.Popen = None",
      "voiceover": "type: ignore",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def _unsafe_execute(code: str, timeout: float, maximum_memory_bytes: Optional[int], result_dict):\n    with create_tempdir():\n\n        import os\n        import shutil\n\n        rmtree = shutil.rmtree\n        rmdir = os.rmdir\n        chdir = os.chdir\n        unlink = os.unlink\n\n        reliability_guard(maximum_memory_bytes=maximum_memory_bytes)\n\n        result_dict.update({\n            \"success\": False,\n            \"stdout\": \"\",\n            \"stderr\": \"\",\n            \"timeout\": False,\n            \"memory_exceeded\": False,\n            \"error\": None,\n        })\n\n        try:\n            exec_globals = {}\n            with capture_io() as (stdout_capture, stderr_capture):\n                with time_limit(timeout):\n                    exec(code, exec_globals)\n\n            result_dict.update({\n                \"success\": True,\n                \"stdout\": stdout_capture.getvalue(),\n                \"stderr\": stderr_capture.getvalue(),\n            })\n\n        except TimeoutException:\n            result_dict.update({\n                \"timeout\": True,\n                \"error\": \"Execution timed out\",\n            })\n\n        except MemoryError as e:\n            result_dict.update({\n                \"memory_exceeded\": True,\n                \"error\": f\"Memory limit exceeded: {e}\",\n            })\n\n        except BaseException as e:\n            result_dict.update({\n                \"error\": f\"{type(e).__name__}: {e}\",\n            })\n\n        shutil.rmtree = rmtree\n        os.rmdir = rmdir\n        os.chdir = chdir\n        os.unlink = unlink\n",
      "highlight": true,
      "voiceover": "This function executes Python code in a subprocess with safety guards and writes results to a shared dictionary for inter-process communication. It runs inside a separate process spawned by execute_code, providing process isolation so that crashes or hangs don't affect the parent process. The function first creates a temporary directory and changes to it for filesystem isolation, then saves references to critical os and shutil functions that will be needed for cleanup after the reliability guard disables them. After applying the reliability guard to disable dangerous functions, it initializes the result dictionary with default failure values. The actual code execution happens inside nested context managers that capture I/O and enforce the timeout, using exec with an empty globals dictionary to prevent access to the module's namespace. The function catches and handles three types of exceptions: TimeoutException for time limit violations, MemoryError for memory limit violations, and BaseException for all other errors, updating the result dictionary appropriately for each case. After execution completes or fails, it restores the saved os and shutil functions to enable proper cleanup of the temporary directory. This design provides robust sandboxing with multiple safety layers while ensuring cleanup happens even when executed code misbehaves.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def execute_code(\n    code: str,\n    timeout: float = 5.0,\n    maximum_memory_bytes: Optional[int] = 256 * 1024 * 1024,\n) -> ExecutionResult:\n    manager = multiprocessing.Manager()\n    result_dict = manager.dict()\n\n    p = multiprocessing.Process(\n        target=_unsafe_execute,\n        args=(code, timeout, maximum_memory_bytes, result_dict)\n    )\n    p.start()\n    p.join(timeout=timeout + 1)\n\n    if p.is_alive():\n        p.kill()\n        return ExecutionResult(\n            success=False,\n            stdout=\"\",\n            stderr=\"\",\n            error=\"Execution timed out (process killed)\",\n            timeout=True,\n            memory_exceeded=False,\n        )\n\n    if not result_dict:\n        return ExecutionResult(\n            success=False,\n            stdout=\"\",\n            stderr=\"\",\n            error=\"Execution failed (no result returned)\",\n            timeout=True,\n            memory_exceeded=False,\n        )\n\n    return ExecutionResult(\n        success=result_dict[\"success\"],\n        stdout=result_dict[\"stdout\"],\n        stderr=result_dict[\"stderr\"],\n        error=result_dict[\"error\"],\n        timeout=result_dict[\"timeout\"],\n        memory_exceeded=result_dict[\"memory_exceeded\"],\n    )\n",
      "highlight": true,
      "voiceover": "This function provides the main public API for executing Python code in a sandboxed environment with timeout and memory limits. It creates a multiprocessing Manager to enable inter-process communication through a shared dictionary, then spawns a separate process to run the _unsafe_execute function with the provided code and limits. The function waits for the subprocess to complete with a timeout slightly longer than the code timeout (timeout + 1) to allow the subprocess's own timeout mechanism to trigger first. If the subprocess is still alive after this wait period, it forcibly kills the process and returns a timeout result, providing a safety net in case the subprocess's timeout mechanism fails. If the subprocess completes but returns an empty result dictionary (which shouldn't happen in normal operation), the function returns a failure result indicating no result was returned. Otherwise, it constructs an ExecutionResult from the values in the shared dictionary, providing a clean interface for callers to inspect execution outcomes. This design provides robust process isolation, ensuring that even catastrophic failures in executed code (segfaults, infinite loops, memory exhaustion) cannot crash or hang the parent process, while the multiprocessing infrastructure handles all the complexity of inter-process communication and process management.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "timeout: float = 5.0,",
      "voiceover": "5 seconds default",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/execution.py",
      "find": "maximum_memory_bytes: Optional[int] = 256 * 1024 * 1024,",
      "voiceover": "256MB default",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/gpt.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import math\nfrom functools import partial\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nanochat.common import get_dist_info, print0\nfrom nanochat.muon import Muon, DistMuon\nfrom nanochat.adamw import DistAdamW\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "import math",
      "voiceover": "Mathematical functions for initialization and FLOP estimation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "from functools import partial",
      "voiceover": "Partial function application for creating optimizer factories",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "from dataclasses import dataclass",
      "voiceover": "Decorator for creating configuration data classes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and neural networks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "import torch.nn as nn",
      "voiceover": "Neural network modules and layers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "import torch.nn.functional as F",
      "voiceover": "Functional API for operations like softmax and cross-entropy",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "from nanochat.common import get_dist_info, print0",
      "voiceover": "Utilities for distributed training information and rank-0 printing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "from nanochat.muon import Muon, DistMuon",
      "voiceover": "Muon optimizer for matrix parameters with distributed variant",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "from nanochat.adamw import DistAdamW",
      "voiceover": "Distributed AdamW optimizer for embedding parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@dataclass\n"
    },
    {
      "type": "writeText",
      "content": "class GPTConfig:\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the GPT model architecture and features\nGPT model (rewrite, a lot simpler)\nNotable features:\n- rotary embeddings (and no positional embeddings)\n- QK norm\n- untied weights for token embedding and lm_head\n- relu^2 activation in MLP\n- norm after token embedding\n- no learnable params in rmsnorm\n- no bias in linear layers\n- Group-Query Attention (GQA) support for more efficient inference\n\nThis dataclass defines the configuration parameters for the GPT model architecture, specifying the model's size and structure. It includes the maximum sequence length the model can process, the vocabulary size for token embeddings, the number of transformer layers, the number of attention heads for queries, the number of key-value heads for Group-Query Attention (which can be less than query heads for efficiency), and the embedding dimension. The default values represent a small GPT model suitable for experimentation, and all parameters can be overridden when instantiating the config. This design provides a clean, type-safe way to configure model architecture without scattered magic numbers throughout the code.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    sequence_len: int = 1024\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "sequence_len: int = 1024",
      "voiceover": "Maximum sequence length the model can process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    vocab_size: int = 50304\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "vocab_size: int = 50304",
      "voiceover": "Size of the vocabulary for token embeddings",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    n_layer: int = 12\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "n_layer: int = 12",
      "voiceover": "Number of transformer layers in the model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    n_head: int = 6\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "n_head: int = 6",
      "voiceover": "number of query heads # Number of attention heads for queries",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    n_kv_head: int = 6\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "n_kv_head: int = 6",
      "voiceover": "number of key/value heads (GQA) # Number of key/value heads for Group-Query Attention",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    n_embd: int = 768\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "n_embd: int = 768",
      "voiceover": "Embedding dimension for the model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def norm(x):\n    return F.rms_norm(x, (x.size(-1),))\n",
      "highlight": true,
      "voiceover": "This function applies RMS normalization to the input tensor without any learnable parameters, providing a purely functional normalization operation. It uses PyTorch's built-in rms_norm function with the last dimension as the normalization dimension, which is standard for transformer models where the last dimension represents the feature/embedding dimension. This parameter-free design simplifies the model and reduces the number of trainable parameters while still providing effective normalization for stable training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4\n    d = x.shape[3] // 2\n    x1, x2 = x[..., :d], x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    out = torch.cat([y1, y2], 3)\n    out = out.to(x.dtype)\n    return out\n",
      "highlight": true,
      "voiceover": "This function applies rotary positional embeddings to the input tensor by rotating pairs of dimensions according to precomputed cosine and sine values. It expects a 4D tensor with shape (batch, heads, sequence, features) representing multi-head attention queries or keys. The function splits the last dimension into two halves, then applies a 2D rotation to each pair of corresponding elements using the rotation matrix defined by cos and sin. The rotation is computed as (x1*cos + x2*sin, -x1*sin + x2*cos) for each pair, which implements a rotation in the 2D plane formed by each pair of dimensions. After rotating all pairs, the function concatenates the results back together and ensures the output dtype matches the input dtype for numerical stability. This design provides relative positional information without requiring learned positional embeddings, and the rotation angles vary with position and dimension to encode rich positional information.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "assert x.ndim == 4",
      "voiceover": "multihead attention",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "x1, x2 = x[..., :d], x[..., d:]",
      "voiceover": "split up last time into two halves",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "y1 = x1 * cos + x2 * sin",
      "voiceover": "rotate pairs of dims",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "out = torch.cat([y1, y2], 3)",
      "voiceover": "re-assemble",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "out = out.to(x.dtype)",
      "voiceover": "ensure input/output dtypes match",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class CausalSelfAttention(nn.Module):\n",
      "highlight": true,
      "voiceover": "This class implements causal self-attention with Group-Query Attention (GQA) support, rotary positional embeddings, and QK normalization. It creates separate linear projections for queries, keys, and values, where the number of key-value heads can be less than query heads for efficiency (GQA). The class stores the layer index for KV cache management and validates that the embedding dimension is evenly divisible by the number of heads, and that GQA configuration is valid (n_kv_head divides n_head evenly). The forward pass projects inputs to queries, keys, and values, applies rotary embeddings for positional information, normalizes queries and keys for training stability, handles KV caching for efficient inference, and computes scaled dot-product attention with appropriate masking for causal (autoregressive) generation. The attention implementation handles three cases: training without cache using standard causal attention, inference with single-token queries attending to full cache, and inference with multi-token queries requiring custom masking for prefix attention combined with causal attention within the new tokens. This design provides efficient attention computation for both training and inference while supporting modern techniques like GQA and rotary embeddings.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.n_head = config.n_head\n        self.n_kv_head = config.n_kv_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
      "highlight": true,
      "voiceover": "This initializer sets up the attention mechanism with the specified configuration and layer index. It stores configuration parameters, creates linear projections for queries, keys, and values with appropriate dimensions for GQA, and validates that the configuration is valid. The layer index is used for KV cache management to track which layer's cache to update.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def forward(self, x, cos_sin, kv_cache):\n        B, T, C = x.size()\n\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n\n        cos, sin = cos_sin\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        q, k = norm(q), norm(k)\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if kv_cache is not None:\n            k, v = kv_cache.insert_kv(self.layer_idx, k, v)\n        Tq = q.size(2)\n        Tk = k.size(2)\n\n        enable_gqa = self.n_head != self.n_kv_head\n        if kv_cache is None or Tq == Tk:\n            y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n        elif Tq == 1:\n            y = F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n        else:\n            attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=q.device)\n            prefix_len = Tk - Tq\n            attn_mask[:, :prefix_len] = True\n            attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool, device=q.device))\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)\n\n        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n        y = self.c_proj(y)\n        return y\n",
      "highlight": true,
      "voiceover": "This method performs the forward pass of causal self-attention with rotary embeddings, QK normalization, and optional KV caching. It projects the input to queries, keys, and values, applies rotary positional embeddings to queries and keys for relative position encoding, normalizes queries and keys for training stability, and rearranges dimensions to make heads the batch dimension. If a KV cache is provided, it inserts the current keys and values into the cache and retrieves the full cached sequence. The method then computes scaled dot-product attention with three different strategies depending on the context: during training or when query and key lengths match, it uses standard causal attention; during inference with a single query token, it attends to all cached keys without causal masking; during inference with multiple query tokens, it creates a custom attention mask that allows each query to attend to all prefix tokens plus causally to tokens within the current chunk. After computing attention, it rearranges the output back to the original shape and projects it back to the residual stream. This design handles both training and various inference scenarios efficiently while supporting modern attention techniques.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)",
      "voiceover": "QK rotary embedding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "q, k = norm(q), norm(k)",
      "voiceover": "QK norm",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)",
      "voiceover": "make head be batch dim, i.e. (B, T, H, D) -> (B, H, T, D)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "Tq = q.size(2)",
      "voiceover": "number of queries in this forward pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "Tk = k.size(2)",
      "voiceover": "number of keys/values in total (in the cache + current forward pass)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "enable_gqa = self.n_head != self.n_kv_head",
      "voiceover": "Group Query Attention (GQA): duplicate key/value heads to match query heads if desired",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=q.device)",
      "voiceover": "True = keep, False = mask",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class MLP(nn.Module):\n",
      "highlight": true,
      "voiceover": "This class implements the feedforward network (MLP) component of each transformer block using a two-layer architecture with ReLU² activation. It expands the input dimension by a factor of 4 in the first layer, applies the squared ReLU activation function (which has been shown to work well in transformers), then projects back to the original dimension. The use of ReLU² instead of GELU or other activations is a design choice that provides good performance while being computationally efficient. The class uses linear layers without bias terms, consistent with the overall model design philosophy of simplicity and parameter efficiency.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
      "highlight": true,
      "voiceover": "This initializer creates the two linear layers for the MLP, expanding to 4x the embedding dimension in the hidden layer and projecting back to the original dimension in the output layer.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square()\n        x = self.c_proj(x)\n        return x\n",
      "highlight": true,
      "voiceover": "This method performs the forward pass of the MLP by first expanding the input through the first linear layer, applying squared ReLU activation (ReLU followed by squaring), then projecting back to the original dimension through the second linear layer. The squared ReLU activation provides non-linearity while being simple and efficient to compute.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class Block(nn.Module):\n",
      "highlight": true,
      "voiceover": "This class implements a single transformer block combining causal self-attention and a feedforward network with residual connections and pre-normalization. It follows the pre-norm architecture where normalization is applied before each sub-layer (attention and MLP) rather than after, which has been shown to improve training stability. The block applies attention to the normalized input and adds the result to the original input via a residual connection, then applies the MLP to the normalized result and adds it via another residual connection. This design implements the standard transformer block architecture with modern improvements like pre-normalization.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.attn = CausalSelfAttention(config, layer_idx)\n        self.mlp = MLP(config)\n",
      "highlight": true,
      "voiceover": "This initializer creates the attention and MLP components for the transformer block, passing the layer index to the attention mechanism for KV cache management.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def forward(self, x, cos_sin, kv_cache):\n        x = x + self.attn(norm(x), cos_sin, kv_cache)\n        x = x + self.mlp(norm(x))\n        return x\n",
      "highlight": true,
      "voiceover": "This method performs the forward pass of the transformer block by applying pre-normalization, attention, and MLP with residual connections. It normalizes the input before attention, adds the attention output to the original input, normalizes again before the MLP, and adds the MLP output to create the final block output. This pre-norm residual architecture provides stable gradient flow during training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class GPT(nn.Module):\n",
      "highlight": true,
      "voiceover": "This class implements the complete GPT language model with modern architectural improvements including rotary embeddings, Group-Query Attention, QK normalization, and ReLU² activations. It combines token embeddings, a stack of transformer blocks, and a language modeling head to predict next tokens. The model uses untied weights for the token embedding and lm_head (they don't share parameters), applies normalization after the token embedding and before the lm_head, and uses parameter-free RMS normalization throughout. The class provides methods for weight initialization with careful scaling, optimizer setup with separate learning rates for different parameter groups (using Muon for matrices and AdamW for embeddings), forward pass with optional loss computation, FLOP estimation for performance analysis, and autoregressive generation for inference. The model precomputes rotary embeddings and stores them as non-persistent buffers to avoid saving them in checkpoints. This design implements a modern, efficient GPT architecture suitable for both training and inference.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n            \"h\": nn.ModuleList([Block(config, layer_idx) for layer_idx in range(config.n_layer)]),\n        })\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.rotary_seq_len = config.sequence_len * 10\n        head_dim = config.n_embd // config.n_head\n        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n        self.register_buffer(\"cos\", cos, persistent=False)\n        self.register_buffer(\"sin\", sin, persistent=False)\n",
      "highlight": true,
      "voiceover": "This initializer constructs the GPT model with the specified configuration, creating the token embedding, transformer blocks, language modeling head, and precomputed rotary embeddings. It uses a ModuleDict for the transformer components to maintain clean organization, creates a list of transformer blocks with sequential layer indices, and initializes the lm_head as a separate linear layer (untied from the embedding). The rotary embeddings are precomputed for a sequence length 10x larger than the config specifies to provide headroom for longer sequences, and they're registered as non-persistent buffers so they're not saved in checkpoints but are still moved to the correct device. This initialization creates a complete model ready for weight initialization and training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "self.rotary_seq_len = config.sequence_len * 10",
      "voiceover": "10X over-compute should be enough, TODO make nicer?",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "self.register_buffer(\"cos\", cos, persistent=False)",
      "voiceover": "persistent=False means it's not saved to the checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def init_weights(self):\n        self.apply(self._init_weights)\n        torch.nn.init.zeros_(self.lm_head.weight)\n        for block in self.transformer.h:\n            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n            torch.nn.init.zeros_(block.attn.c_proj.weight)\n        head_dim = self.config.n_embd // self.config.n_head\n        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n        self.cos, self.sin = cos, sin\n        if self.transformer.wte.weight.device.type == \"cuda\":\n            self.transformer.wte.to(dtype=torch.bfloat16)\n",
      "highlight": true,
      "voiceover": "This method initializes all model weights using a carefully designed initialization scheme for stable training. It applies the _init_weights method to all modules for basic initialization, then performs special initialization for specific layers: zeros out the lm_head classifier weights, zeros out the c_proj weights in all blocks (the output projections of attention and MLP) to start with identity-like residual connections, recomputes the rotary embeddings on the correct device, and casts the token embedding to bfloat16 if on CUDA to save memory in both the model and activations. The zero initialization of output projections is important for training stability as it makes the residual connections initially act like identity functions, allowing gradients to flow easily in early training. This initialization strategy is designed to enable stable training from the start while being memory-efficient.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            fan_out = module.weight.size(0)\n            fan_in = module.weight.size(1)\n            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
      "highlight": true,
      "voiceover": "This method initializes weights for individual modules using a variance-scaled normal initialization scheme. For linear layers, it computes a custom standard deviation based on both fan-in and fan-out, using the formula std = (1/√fan_in) * min(1, √(fan_out/fan_in)), which is based on recent research (arxiv.org/pdf/2310.17813) and provides better initialization than standard schemes. This scaling helps maintain appropriate activation magnitudes throughout the network. For embedding layers, it uses standard normal initialization with mean 0 and std 1. Bias terms are initialized to zero if present, though the model doesn't use biases by default. This careful initialization helps ensure stable training dynamics from the beginning.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n        if device is None:\n            device = self.transformer.wte.weight.device\n        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n        freqs = torch.outer(t, inv_freq)\n        cos, sin = freqs.cos(), freqs.sin()\n        cos, sin = cos.bfloat16(), sin.bfloat16()\n        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n        return cos, sin\n",
      "highlight": true,
      "voiceover": "This method precomputes the cosine and sine values for rotary positional embeddings across all positions and dimensions. It autodetects the device from the model's embedding weights if not specified, creates a range of channel indices strided by 2 (since rotary embeddings work on pairs of dimensions), computes inverse frequencies using the formula 1/(base^(channel/head_dim)) where base is typically 10000, creates a range of time steps from 0 to seq_len, and computes the outer product of time steps and inverse frequencies to get rotation angles for each (position, channel) pair. The method then computes cosine and sine of these angles, converts them to bfloat16 for memory efficiency, and adds batch and head dimensions for broadcasting during the forward pass. The base parameter controls the wavelength of the positional encodings, with 10000 being standard but higher values like 100K being explored for longer context. This precomputation is efficient since rotary embeddings are position-dependent but data-independent, allowing them to be computed once and reused.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "cos, sin = cos.bfloat16(), sin.bfloat16()",
      "voiceover": "keep them in bfloat16",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "cos, sin = cos[None, :, None, :], sin[None, :, None, :]",
      "voiceover": "add batch and head dims for later broadcasting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_device(self):\n        return self.transformer.wte.weight.device\n",
      "highlight": true,
      "voiceover": "This method returns the device (CPU, CUDA, etc.) where the model's parameters are located by checking the device of the token embedding weights. This is a convenience method used throughout the codebase to determine where to place tensors for model operations.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def estimate_flops(self):\n        nparams = sum(p.numel() for p in self.parameters())\n        nparams_embedding = self.transformer.wte.weight.numel()\n        l, h, q, t = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.sequence_len\n        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t\n        return num_flops_per_token\n",
      "highlight": true,
      "voiceover": "This method estimates the number of floating-point operations (FLOPs) per token during a forward pass, which is useful for performance analysis and comparing model efficiency. It counts the total number of parameters, subtracts the embedding parameters (which don't contribute to per-token FLOPs in the same way), and uses the formula 6*(nparams - nparams_embedding) + 12*l*h*q*t based on the Chinchilla paper (arxiv.org/abs/2204.02311). The formula accounts for both the matrix multiplications in the transformer (the 6x factor) and the attention operations (the 12*l*h*q*t term). This FLOP estimate helps in understanding computational requirements and optimizing training efficiency.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def setup_optimizers(self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):\n        model_dim = self.config.n_embd\n        ddp, rank, local_rank, world_size = get_dist_info()\n        matrix_params = list(self.transformer.h.parameters())\n        embedding_params = list(self.transformer.wte.parameters())\n        lm_head_params = list(self.lm_head.parameters())\n        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params)\n        dmodel_lr_scale = (model_dim / 768) ** -0.5\n        if rank == 0:\n            print(f\"Scaling the LR for the AdamW parameters ∝1/√({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n        adam_groups = [\n            dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),\n            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n        ]\n        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)\n        AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=True)\n        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)\n        MuonFactory = DistMuon if ddp else Muon\n        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)\n        optimizers = [adamw_optimizer, muon_optimizer]\n        for opt in optimizers:\n            for group in opt.param_groups:\n                group[\"initial_lr\"] = group[\"lr\"]\n        return optimizers\n",
      "highlight": true,
      "voiceover": "This method creates and configures optimizers for different parameter groups using a hybrid optimization strategy. It separates parameters into three groups: matrix parameters (transformer blocks) optimized with Muon, embedding parameters optimized with AdamW, and lm_head parameters also optimized with AdamW. The method scales learning rates proportionally to 1/√dmodel relative to a 768-dimensional baseline, ensuring appropriate learning rates for different model sizes. For matrix parameters, it uses the Muon optimizer (or DistMuon for distributed training) which is specifically designed for matrix-structured parameters. For embedding and lm_head parameters, it uses AdamW (or DistAdamW for distributed training) with carefully tuned beta values (0.8, 0.95) and epsilon (1e-10). The method creates separate parameter groups with different learning rates: unembedding_lr for lm_head, embedding_lr for token embeddings, and matrix_lr for transformer blocks. It combines both optimizers into a list and sets initial_lr for each parameter group to enable learning rate scheduling. This sophisticated optimizer setup is designed to provide optimal training dynamics for different types of parameters in the model.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n        B, T = idx.size()\n\n        assert T <= self.cos.size(1), f\"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}\"\n        assert idx.device == self.cos.device, f\"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}\"\n        assert self.cos.dtype == torch.bfloat16, \"Rotary embeddings must be in bfloat16\"\n        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n        cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]\n\n        x = self.transformer.wte(idx)\n        x = norm(x)\n        for block in self.transformer.h:\n            x = block(x, cos_sin, kv_cache)\n        x = norm(x)\n\n        softcap = 15\n        logits = self.lm_head(x)\n        logits = logits.float()\n        logits = softcap * torch.tanh(logits / softcap)\n\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n            return loss\n        else:\n            return logits\n",
      "highlight": true,
      "voiceover": "This method performs the forward pass of the GPT model, computing logits and optionally the loss. It extracts the batch size and sequence length from the input token indices, retrieves the appropriate rotary embeddings for the current sequence length (with offset if using KV cache), validates that the sequence doesn't exceed the precomputed rotary embedding cache, and ensures all tensors are on the same device. The method then passes the token indices through the embedding layer, applies normalization, processes through all transformer blocks with rotary embeddings and optional KV caching, applies final normalization, and computes logits through the lm_head. The logits are converted to float32 and passed through a tanh-based soft-capping function (logits = 15 * tanh(logits/15)) to smoothly limit the range to [-15, 15], which helps with training stability. If targets are provided, the method computes and returns the cross-entropy loss with optional ignore_index=-1 for padding tokens and configurable reduction (mean or none). If no targets are provided, it returns the logits directly for inference. This design supports both training (with loss computation) and inference (with KV caching) in a single unified forward pass.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]",
      "voiceover": "truncate cache to current sequence length",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "softcap = 15",
      "voiceover": "smoothly cap the logits to the range [-softcap, softcap]",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "logits = self.lm_head(x)",
      "voiceover": "(B, T, vocab_size) <- very big tensor, large amount of memory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "logits = logits.float()",
      "voiceover": "switch to fp32 for logit softcap and loss computation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "logits = softcap * torch.tanh(logits / softcap)",
      "voiceover": "squash the logits",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n    @torch.inference_mode()\n"
    },
    {
      "type": "writeText",
      "content": "    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):\n        assert isinstance(tokens, list)\n        device = self.get_device()\n        rng = None\n        if temperature > 0:\n            rng = torch.Generator(device=device)\n            rng.manual_seed(seed)\n        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n        for _ in range(max_tokens):\n            logits = self.forward(ids)\n            logits = logits[:, -1, :]\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            if temperature > 0:\n                logits = logits / temperature\n                probs = F.softmax(logits, dim=-1)\n                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n            else:\n                next_ids = torch.argmax(logits, dim=-1, keepdim=True)\n            ids = torch.cat((ids, next_ids), dim=1)\n            token = next_ids.item()\n            yield token\n",
      "highlight": true,
      "voiceover": "This method implements naive autoregressive streaming inference for text generation, designed to be simple and easy to understand. It assumes batch size 1 and works with Python lists and integers for simplicity. The method validates that tokens is a list, gets the model's device, creates a random number generator with the specified seed if temperature is positive (for reproducible sampling), and converts the input tokens to a tensor. For each generation step up to max_tokens, it runs a forward pass to get logits for all positions, extracts the logits for the last position, optionally applies top-k filtering by masking out all but the k highest logits, applies temperature scaling if temperature is positive, samples from the resulting distribution (or takes argmax if temperature is 0 for deterministic generation), appends the sampled token to the sequence, and yields the token for streaming output. This design provides a simple, readable implementation of autoregressive generation suitable for basic inference, though the Engine class provides a more efficient implementation with KV caching for production use.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "ids = torch.tensor([tokens], dtype=torch.long, device=device)",
      "voiceover": "add batch dim",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "logits = self.forward(ids)",
      "voiceover": "(B, T, vocab_size)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/gpt.py",
      "find": "logits = logits[:, -1, :]",
      "voiceover": "(B, vocab_size)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/loss_eval.py"
    },
    {
      "type": "writeText",
      "content": "import math\nimport torch\nimport torch.distributed as dist\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "import math",
      "voiceover": "Mathematical functions for logarithm conversion in bits-per-byte calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and model evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "import torch.distributed as dist",
      "voiceover": "Distributed training utilities for synchronizing metrics across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "def evaluate_bpb(model, batches, steps, token_bytes):\n    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n    total_bytes = torch.tensor(0, dtype=torch.int64, device=model.get_device())\n    batch_iter = iter(batches)\n    for _ in range(steps):\n        x, y = next(batch_iter)\n        loss2d = model(x, y, loss_reduction='none')\n        loss2d = loss2d.view(-1)\n        y = y.view(-1)\n        if (y.int() < 0).any():\n            valid = y >= 0\n            y_safe = torch.where(valid, y, torch.zeros_like(y))\n            num_bytes2d = torch.where(\n                valid,\n                token_bytes[y_safe],\n                torch.zeros_like(y, dtype=token_bytes.dtype)\n            )\n            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n            total_bytes += num_bytes2d.sum()\n        else:\n            num_bytes2d = token_bytes[y]\n            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n            total_bytes += num_bytes2d.sum()\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    if world_size > 1:\n        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n    total_nats = total_nats.item()\n    total_bytes = total_bytes.item()\n    if total_bytes == 0:\n        return float('inf')\n    bpb = total_nats / (math.log(2) * total_bytes)\n    return bpb\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this evaluation utilities module\nA number of functions that help with evaluating a base model.\n\nThis function evaluates model performance using bits-per-byte (bpb) instead of the naive mean loss, providing a tokenization-independent metric that allows fair comparison across different vocabulary sizes. The function computes the total loss (in nats) and total bytes across multiple evaluation steps, then divides to get loss per byte, which is converted to bits by dividing by log(2). The key innovation is that each token's loss is weighted by the number of UTF-8 bytes that token represents, ensuring that longer tokens (which represent more information) contribute proportionally more to the metric. The function handles three important cases: normal tokens are weighted by their byte length, special tokens (like BOS) are excluded by having 0 bytes in the token_bytes tensor, and actively masked tokens (with ignore_index like -1) are also excluded from the metric. The implementation includes a fast path for batches without masked tokens and a careful path that handles negative indices safely when masked tokens are present. In distributed training, the function synchronizes total_nats and total_bytes across all ranks using all_reduce before computing the final metric. This design provides a robust, tokenization-independent evaluation metric that accurately reflects model performance on the underlying byte-level data.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "loss2d = model(x, y, loss_reduction='none')",
      "voiceover": "(B, T)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "loss2d = loss2d.view(-1)",
      "voiceover": "flatten",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "y = y.view(-1)",
      "voiceover": "flatten",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/loss_eval.py",
      "find": "if (y.int() < 0).any():",
      "voiceover": "mps does not currently have kernel for < 0 for int64, only int32",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/muon.py"
    },
    {
      "type": "writeText",
      "content": "import torch\nfrom torch import Tensor\nimport torch.distributed as dist\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "import torch",
      "voiceover": "PyTorch library for tensor operations and optimizer base class",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "from torch import Tensor",
      "voiceover": "Type hint for PyTorch tensors",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "import torch.distributed as dist",
      "voiceover": "Distributed training utilities for multi-GPU synchronization",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@torch.compile\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "@torch.compile",
      "voiceover": "JIT compile this function for performance optimization",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n\n    assert G.ndim >= 2\n    a, b, c = (3.4445, -4.7750,  2.0315)\n    X = G.bfloat16()\n    if G.size(-2) > G.size(-1):\n        X = X.mT\n\n    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n    for _ in range(steps):\n        A = X @ X.mT\n        B = b * A + c * A @ A\n        X = a * X + B @ X\n\n    if G.size(-2) > G.size(-1):\n        X = X.mT\n    return X\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the Muon optimizer implementation\nMuon optimizer from Keller et al.\nAlso a lot of borrowing of ideas from modded-nanogpt.\n\nThis function computes the zero-power (matrix sign) of a gradient matrix using the Newton-Schulz iteration with a quintic polynomial approximation. The zero-power operation is central to the Muon optimizer's preconditioning strategy, transforming gradients to have more uniform eigenvalues for better optimization. The function handles batched inputs (ndim >= 2) for efficiency, converts to bfloat16 for memory and speed, and transposes tall matrices to wide format for better numerical stability. It normalizes the spectral norm to at most 1 to ensure convergence, then performs the specified number of Newton-Schulz iterations using the quintic formula with coefficients (a=3.4445, b=-4.7750, c=2.0315) optimized for fast convergence. Each iteration computes A = X @ X^T, then B = b*A + c*A@A (the quintic term), and finally X = a*X + B@X. After convergence, the function transposes back if needed and returns the result. This batched implementation enables efficient processing of multiple parameter matrices simultaneously, and the torch.compile decorator ensures the function is JIT-compiled for maximum performance.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "assert G.ndim >= 2",
      "voiceover": "batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "B = b * A + c * A @ A",
      "voiceover": "quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class Muon(torch.optim.Optimizer):\n",
      "highlight": true,
      "voiceover": "This class implements the Muon optimizer for single-process training, providing momentum-based optimization with Newton-Schulz preconditioning for matrix-structured parameters. The optimizer groups parameters by their total number of elements (numel) to enable efficient batched processing of same-sized parameters. During initialization, it creates separate parameter groups for each unique parameter size, allowing the Newton-Schulz iteration to process multiple parameters simultaneously. The optimizer supports standard momentum with optional Nesterov acceleration, and applies the zero-power transformation to gradients before the update step. The learning rate is scaled by sqrt(max(1, rows/cols)) to account for the aspect ratio of parameter matrices, providing better adaptation to different matrix shapes. This single-process implementation is simpler than DistMuon but doesn't support distributed training, making it suitable for single-GPU or CPU training scenarios.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n        params: list[Tensor] = [*params]\n        param_groups = []\n        for size in {p.numel() for p in params}:\n            group = dict(params=[p for p in params if p.numel() == size])\n            param_groups.append(group)\n        super().__init__(param_groups, defaults)\n",
      "highlight": true,
      "voiceover": "This initializer sets up the Muon optimizer with the specified hyperparameters and groups parameters by their total element count. It converts the parameter iterable to a list, creates a set of unique parameter sizes, and builds parameter groups where each group contains all parameters of the same size. This grouping strategy enables efficient batched Newton-Schulz computation. The defaults dictionary stores the learning rate, momentum coefficient, Nesterov flag, and number of Newton-Schulz iteration steps.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "    def step(self):\n        for group in self.param_groups:\n            params: list[Tensor] = group[\"params\"]\n            for p in params:\n                g = p.grad\n                assert g is not None\n                state = self.state[p]\n                if \"momentum_buffer\" not in state:\n                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n                buf: Tensor = state[\"momentum_buffer\"]\n                buf.lerp_(g, 1 - group[\"momentum\"])\n                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n                p.add_(g, alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
      "highlight": true,
      "voiceover": "This method performs a single optimization step for all parameters, applying momentum, optional Nesterov acceleration, Newton-Schulz preconditioning, and the parameter update. For each parameter, it retrieves the gradient, initializes or updates the momentum buffer using linear interpolation (lerp), optionally applies Nesterov momentum by interpolating between the gradient and momentum buffer, applies the zero-power transformation via Newton-Schulz iteration to precondition the gradient, and finally updates the parameter with a learning rate scaled by the square root of the aspect ratio. The aspect ratio scaling (sqrt(max(1, rows/cols))) helps normalize updates for matrices of different shapes, ensuring more uniform optimization across parameters.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class DistMuon(torch.optim.Optimizer):\n",
      "highlight": true,
      "voiceover": "This class implements the distributed version of the Muon optimizer, designed for efficient multi-GPU training with gradient averaging and parameter synchronization across ranks. Unlike the single-process Muon, DistMuon groups parameters by their exact shape (not just size) to ensure compatibility across ranks, and requires all parameters to be 2D matrices. During initialization, it validates that all parameters are 2D, groups them by shape with deterministic ordering, and creates a zero buffer for each group to use as padding during collective operations. The optimizer implements a sophisticated two-phase update strategy: first, it uses reduce-scatter to average gradients across ranks with each rank owning computation for a subset of parameters; second, after computing the Muon update on owned parameters, it uses all-gather to replicate updated parameters back to all ranks. This design minimizes communication overhead by overlapping computation with communication through asynchronous operations, and ensures all ranks have consistent parameter values after each step. The implementation carefully handles edge cases like parameter groups that don't divide evenly by world_size using zero-buffer padding.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, params, lr: float = 0.02, momentum: float = 0.95,\n                 nesterov: bool = True, ns_steps: int = 5):\n        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n        params = list(params)\n        assert all(p.ndim == 2 for p in params), \"Muon expects 2D parameters only\"\n        rank = dist.get_rank()\n        shapes = sorted({p.shape for p in params})\n        param_groups = []\n        for shape in shapes:\n            group_params = [p for p in params if p.shape == shape]\n            device, dtype = group_params[0].device, group_params[0].dtype\n            assert all(p.device == device for p in group_params)\n            assert all(p.dtype == dtype for p in group_params)\n            if rank == 0:\n                print(f\"Muon: Grouping {len(group_params)} params of shape {shape}, device {device}, dtype {dtype}\")\n            param_groups.append(dict(params=group_params, zero_buffer=torch.zeros_like(group_params[0])))\n        super().__init__(param_groups, defaults)\n",
      "highlight": true,
      "voiceover": "This initializer sets up the distributed Muon optimizer by grouping parameters by shape, validating they're all 2D, and creating zero buffers for padding during collective operations. It gets the current rank, converts parameters to a list, validates all are 2D matrices (required for Muon), creates a sorted set of unique shapes for deterministic ordering across ranks, and for each shape creates a parameter group containing all parameters of that shape along with a zero buffer of the same shape. The zero buffer is used to pad incomplete groups during reduce-scatter and all-gather operations. The method prints grouping information from rank 0 for debugging.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "shapes = sorted({p.shape for p in params})",
      "voiceover": "sort to ensure consistent / deterministic ordering",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n    @torch.no_grad()\n"
    },
    {
      "type": "writeText",
      "content": "    def step(self):\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n\n        assert all(p.grad is not None for group in self.param_groups for p in group[\"params\"]), \"All params must have grads\"\n\n        all_reduce_futures = []\n        for group in self.param_groups:\n            params = group[\"params\"]\n            zero_buffer = group[\"zero_buffer\"]\n            for base_i in range(0, len(params), world_size):\n                owner_idx = base_i + rank\n                rs_input = [p.grad for p in params[base_i:base_i + world_size]]\n                rs_input.extend([zero_buffer] * (world_size - len(rs_input)))\n                rs_output = params[owner_idx].grad if owner_idx < len(params) else torch.empty_like(zero_buffer)\n                work = dist.reduce_scatter(rs_output, rs_input, op=dist.ReduceOp.AVG, async_op=True).get_future()\n                all_reduce_futures.append(work)\n\n        future_idx = 0\n        all_gather_futures = []\n        for group in self.param_groups:\n            params = group[\"params\"]\n            zero_buffer = group[\"zero_buffer\"]\n            for base_i in range(0, len(params), world_size):\n                owner_idx = base_i + rank\n                all_reduce_futures[future_idx].wait()\n                future_idx += 1\n                if owner_idx < len(params):\n                    p = params[owner_idx]\n                    g = p.grad\n                    state = self.state[p]\n                    if \"momentum_buffer\" not in state:\n                        state[\"momentum_buffer\"] = torch.zeros_like(g)\n                    buf: Tensor = state[\"momentum_buffer\"]\n                    buf.lerp_(g, 1.0 - group[\"momentum\"])\n                    g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n                    g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n                    scale = (max(1.0, p.size(-2) / p.size(-1)) ** 0.5)\n                    p.add_(g, alpha=-group[\"lr\"] * scale)\n                ag_input = params[owner_idx] if owner_idx < len(params) else zero_buffer\n                ag_output = params[base_i:base_i + world_size]\n                ag_output.extend([torch.empty_like(zero_buffer) for _ in range(world_size - len(ag_output))])\n                work = dist.all_gather(ag_output, ag_input, async_op=True).get_future()\n                all_gather_futures.append(work)\n\n        torch.futures.collect_all(all_gather_futures).wait()\n",
      "highlight": true,
      "voiceover": "This method performs a distributed optimization step using a two-phase strategy to minimize communication overhead while ensuring all ranks have consistent parameters. In the first phase, it launches asynchronous reduce-scatter operations to average gradients across ranks, where parameters are processed in groups of world_size and each rank becomes the compute owner for one parameter in each group (determined by base_i + rank). The reduce-scatter averages the gradients and distributes them so each rank receives the averaged gradient for its owned parameter. In the second phase, as each reduce-scatter completes, the owning rank computes the Muon update (momentum, Nesterov, Newton-Schulz preconditioning, and parameter update with aspect-ratio-scaled learning rate), then immediately launches an asynchronous all-gather to replicate the updated parameter to all other ranks. This overlapping of computation and communication maximizes GPU utilization. The method carefully handles edge cases where the number of parameters doesn't divide evenly by world_size using zero-buffer padding, and uses futures to track asynchronous operations, waiting for all all-gather operations to complete before returning. This design ensures efficient distributed training with minimal communication overhead and consistent parameters across all ranks.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "owner_idx = base_i + rank",
      "voiceover": "calculate the index of the param that this rank owns",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "all_reduce_futures[future_idx].wait()",
      "voiceover": "possibly later we could use wait_any polling instead",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "g = p.grad",
      "voiceover": "now averaged across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/muon.py",
      "find": "ag_output.extend([torch.empty_like(zero_buffer) for _ in range(world_size - len(ag_output))])",
      "voiceover": "pad",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/report.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport re\nimport shutil\nimport subprocess\nimport socket\nimport datetime\nimport platform\nimport psutil\nimport torch\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import os",
      "voiceover": "Operating system interface for file and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import re",
      "voiceover": "Regular expressions for pattern matching in report content",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import shutil",
      "voiceover": "High-level file operations for copying report files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import subprocess",
      "voiceover": "Subprocess management for running shell commands",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import socket",
      "voiceover": "Network interface for getting hostname information",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import datetime",
      "voiceover": "Date and time utilities for timestamps and duration calculations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import platform",
      "voiceover": "Platform identification for system information",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import psutil",
      "voiceover": "Process and system utilities for CPU and memory information",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import torch",
      "voiceover": "PyTorch library for GPU information and CUDA availability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def run_command(cmd):\n\n    try:\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            return result.stdout.strip()\n        return None\n    except:\n        return None\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the purpose of this report generation utilities module\nUtilities for generating training report cards. More messy code than usual, will fix.\n\nThis function executes a shell command with a timeout and returns its output, providing a safe wrapper for running git and other system commands. It runs the command with shell=True for convenience, captures both stdout and stderr, uses text mode for string output, and enforces a 5-second timeout to prevent hanging. If the command succeeds (returncode 0), it returns the stripped stdout; otherwise it returns None. The function also catches all exceptions and returns None, making it safe to use for commands that might fail. This design provides a simple, robust interface for executing system commands during report generation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_git_info():\n    info = {}\n    info['commit'] = run_command(\"git rev-parse --short HEAD\") or \"unknown\"\n    info['branch'] = run_command(\"git rev-parse --abbrev-ref HEAD\") or \"unknown\"\n\n    status = run_command(\"git status --porcelain\")\n    info['dirty'] = bool(status) if status is not None else False\n\n    info['message'] = run_command(\"git log -1 --pretty=%B\") or \"\"\n    info['message'] = info['message'].split('\\n')[0][:80]\n\n    return info\n",
      "highlight": true,
      "voiceover": "This function collects git repository information for the training report, including commit hash, branch name, dirty status, and commit message. It uses git commands to retrieve the short commit hash and current branch name, checks if the repository has uncommitted changes using git status --porcelain (dirty flag), and extracts the first line of the most recent commit message truncated to 80 characters. The function returns a dictionary with all this information, using \"unknown\" as fallback values when git commands fail. This design captures the code state at the time of training, enabling reproducibility and tracking of experiments.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "info['message'] = info['message'].split('\\n')[0][:80]",
      "voiceover": "First line, truncated",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_gpu_info():\n    \n    if not torch.cuda.is_available():\n        return {\"available\": False}\n\n    num_devices = torch.cuda.device_count()\n    info = {\n        \"available\": True,\n        \"count\": num_devices,\n        \"names\": [],\n        \"memory_gb\": []\n    }\n\n    for i in range(num_devices):\n        props = torch.cuda.get_device_properties(i)\n        info[\"names\"].append(props.name)\n        info[\"memory_gb\"].append(props.total_memory / (1024**3))\n\n    info[\"cuda_version\"] = torch.version.cuda or \"unknown\"\n\n    return info\n",
      "highlight": true,
      "voiceover": "This function gathers comprehensive GPU information for the training report, including availability, device count, names, memory, and CUDA version. It first checks if CUDA is available, returning a minimal dict with available=False if not. For available GPUs, it queries each device for its properties, extracting the device name and total memory in gigabytes. It also retrieves the CUDA version from PyTorch. The function returns a dictionary containing all this information, which is used to document the hardware environment and estimate training costs. This design provides complete GPU configuration details for reproducibility and cost tracking.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_system_info():\n    \n    info = {}\n\n    info['hostname'] = socket.gethostname()\n    info['platform'] = platform.system()\n    info['python_version'] = platform.python_version()\n    info['torch_version'] = torch.__version__\n\n    info['cpu_count'] = psutil.cpu_count(logical=False)\n    info['cpu_count_logical'] = psutil.cpu_count(logical=True)\n    info['memory_gb'] = psutil.virtual_memory().total / (1024**3)\n\n    info['user'] = os.environ.get('USER', 'unknown')\n    info['nanochat_base_dir'] = os.environ.get('NANOCHAT_BASE_DIR', 'out')\n    info['working_dir'] = os.getcwd()\n\n    return info\n",
      "highlight": true,
      "voiceover": "This function collects comprehensive system information for the training report, including hostname, platform, Python version, PyTorch version, CPU count, memory, user, and working directory. It uses various system libraries to gather this information: socket for hostname, platform for OS and Python version, psutil for CPU count (both physical and logical cores) and total memory in gigabytes, and os.environ for user and NANOCHAT_BASE_DIR environment variables. The function returns a dictionary with all this information, providing a complete picture of the software and hardware environment. This design ensures the training report captures all relevant system details for reproducibility.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def estimate_cost(gpu_info, runtime_hours=None):\n   \n\n    default_rate = 2.0\n    gpu_hourly_rates = {\n        \"H100\": 3.00,\n        \"A100\": 1.79,\n        \"V100\": 0.55,\n    }\n\n    if not gpu_info.get(\"available\"):\n        return None\n\n    hourly_rate = None\n    gpu_name = gpu_info[\"names\"][0] if gpu_info[\"names\"] else \"unknown\"\n    for gpu_type, rate in gpu_hourly_rates.items():\n        if gpu_type in gpu_name:\n            hourly_rate = rate * gpu_info[\"count\"]\n            break\n\n    if hourly_rate is None:\n        hourly_rate = default_rate * gpu_info[\"count\"]\n\n    return {\n        \"hourly_rate\": hourly_rate,\n        \"gpu_type\": gpu_name,\n        \"estimated_total\": hourly_rate * runtime_hours if runtime_hours else None\n    }\n",
      "highlight": true,
      "voiceover": "This function estimates the cost of training based on GPU type and runtime hours using Lambda Cloud pricing as a reference. It maintains a dictionary of hourly rates for common GPU types (H100, A100, V100) and a default rate for unknown GPUs. The function attempts to identify the GPU type by matching the device name against known types, calculates the hourly rate by multiplying the per-GPU rate by the number of GPUs, and optionally computes the total estimated cost if runtime_hours is provided. It returns None if no GPUs are available, otherwise returns a dictionary with hourly_rate, gpu_type, and estimated_total. This design helps track the financial cost of experiments and compare cost-effectiveness of different approaches.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "hourly_rate = default_rate * gpu_info[\"count\"]",
      "voiceover": "Default estimate",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def generate_header():\n   \n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    git_info = get_git_info()\n    gpu_info = get_gpu_info()\n    sys_info = get_system_info()\n    cost_info = estimate_cost(gpu_info)\n\n    header = f\"\"\"\n\nGenerated: {timestamp}\n\n- Branch: {git_info['branch']}\n- Commit: {git_info['commit']} {\"(dirty)\" if git_info['dirty'] else \"(clean)\"}\n- Message: {git_info['message']}\n\n- Platform: {sys_info['platform']}\n- CPUs: {sys_info['cpu_count']} cores ({sys_info['cpu_count_logical']} logical)\n- Memory: {sys_info['memory_gb']:.1f} GB\n- GPU Memory: {total_vram:.1f} GB total\n- CUDA Version: {gpu_info['cuda_version']}\n    header += f\"\"\"\n- Python: {sys_info['python_version']}\n- PyTorch: {sys_info['torch_version']}\n\n- Characters: {num_chars:,}\n- Lines: {num_lines:,}\n- Files: {num_files:,}\n- Tokens (approx): {num_tokens:,}\n- Dependencies (uv.lock lines): {uv_lock_lines:,}\n",
      "highlight": true,
      "voiceover": "This function generates a comprehensive markdown header for the training report containing environment information, hardware details, software versions, and codebase bloat metrics. It collects information from multiple sources: git info for code versioning, GPU info for hardware specs, system info for platform details, and cost estimates for budget tracking. The function constructs a formatted markdown string with sections for Git Information (branch, commit, dirty status, message), Hardware (platform, CPUs, memory, GPUs, CUDA version, hourly rate), and Software (Python and PyTorch versions). It also runs the files-to-prompt command to package all source code and compute bloat metrics including character count, line count, file count, approximate token count (chars/4), and dependency count from uv.lock. The function returns the complete header as a formatted markdown string ready to be written to the report. This design provides a comprehensive snapshot of the training environment for reproducibility and cost tracking.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "num_tokens = num_chars // 4",
      "voiceover": "assume approximately 4 chars per token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def slugify(text):\n   \n    return text.lower().replace(\" \", \"-\")\n",
      "highlight": true,
      "voiceover": "This function converts text to a URL-friendly slug format by converting to lowercase and replacing spaces with hyphens. It's used to generate consistent filenames for report sections from their human-readable titles. This simple transformation ensures section names like \"Base Model Training\" become \"base-model-training.md\" for filesystem compatibility.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "EXPECTED_FILES = [\n    \"tokenizer-training.md\",\n    \"tokenizer-evaluation.md\",\n    \"base-model-training.md\",\n    \"base-model-loss.md\",\n    \"base-model-evaluation.md\",\n    \"midtraining.md\",\n    \"chat-evaluation-mid.md\",\n    \"chat-sft.md\",\n    \"chat-evaluation-sft.md\",\n    \"chat-rl.md\",\n    \"chat-evaluation-rl.md\",\n]\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "EXPECTED_FILES = [",
      "voiceover": "the expected files and their order # List of expected report section files in the order they should appear in the final report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "chat_metrics = [\"ARC-Easy\", \"ARC-Challenge\", \"MMLU\", \"GSM8K\", \"HumanEval\", \"ChatCORE\"]\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "chat_metrics = [\"ARC-Easy\", \"ARC-Challenge\", \"MMLU\", \"GSM8K\", \"HumanEval\", \"ChatCORE\"]",
      "voiceover": "the metrics we're currently interested in # List of chat evaluation metrics to extract and display in the summary table",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def extract(section, keys):\n    if not isinstance(keys, list):\n        keys = [keys]\n    out = {}\n    for line in section.split(\"\\n\"):\n        for key in keys:\n            if key in line:\n                out[key] = line.split(\":\")[1].strip()\n    return out\n",
      "highlight": true,
      "voiceover": "This function extracts metric values from a report section by searching for lines containing the specified keys and parsing the values after the colon. It accepts either a single key or a list of keys for convenience, iterates through all lines in the section, and for each matching key extracts the value by splitting on colon and stripping whitespace. The function returns a dictionary mapping keys to their extracted values. This simple parsing approach works well for the markdown format used in report sections where metrics are listed as \"- metric_name: value\".",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "keys = [keys]",
      "voiceover": "convenience",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def extract_timestamp(content, prefix):\n    for line in content.split('\\n'):\n        if line.startswith(prefix):\n            time_str = line.split(\":\", 1)[1].strip()\n            try:\n                return datetime.datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n            except:\n                pass\n    return None\n",
      "highlight": true,
      "voiceover": "This function extracts and parses a timestamp from report content by searching for a line with the specified prefix and parsing the datetime string. It iterates through lines looking for the prefix, splits on the first colon to get the timestamp string, attempts to parse it using the standard format \"%Y-%m-%d %H:%M:%S\", and returns the datetime object if successful or None if parsing fails. This function is used to extract start and end times from report sections for calculating total wall clock time.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class Report:\n",
      "highlight": true,
      "voiceover": "This class manages the generation and organization of training reports by logging individual sections to markdown files and assembling them into a comprehensive final report. It maintains a report directory where section files are stored, provides methods to log data for different training stages (tokenizer, base model, chat, etc.), generates the final consolidated report with metrics tables and timing information, and resets the report state for new training runs. The class handles formatting of different data types (strings, dicts, floats, large ints), extracts key metrics from sections for summary tables, calculates total wall clock time from timestamps, and copies the final report to the current directory for convenience. This design provides a structured way to track and document all aspects of a training run in a single comprehensive markdown document.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, report_dir):\n        os.makedirs(report_dir, exist_ok=True)\n        self.report_dir = report_dir\n",
      "highlight": true,
      "voiceover": "This initializer creates the report directory if it doesn't exist and stores the path for later use. It ensures the directory structure is ready for logging report sections.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def log(self, section, data):\n        slug = slugify(section)\n        file_name = f\"{slug}.md\"\n        file_path = os.path.join(self.report_dir, file_name)\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"\n            f.write(f\"timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n            for item in data:\n                if not item:\n                    continue\n                if isinstance(item, str):\n                    f.write(item)\n                else:\n                    for k, v in item.items():\n                        if isinstance(v, float):\n                            vstr = f\"{v:.4f}\"\n                        elif isinstance(v, int) and v >= 10000:\n                            vstr = f\"{v:,.0f}\"\n                        else:\n                            vstr = str(v)\n                        f.write(f\"- {k}: {vstr}\\n\")\n            f.write(\"\\n\")\n        return file_path\n",
      "highlight": true,
      "voiceover": "This method logs a section of data to a markdown file in the report directory, formatting the data appropriately based on type. It converts the section name to a slug for the filename, creates a markdown file with a section header and timestamp, and processes the data list by skipping falsy values, writing strings directly, and formatting dictionaries as markdown lists with appropriate number formatting (4 decimal places for floats, comma separators for large ints). The method returns the file path of the created section file. This design allows flexible logging of different data types while maintaining consistent markdown formatting.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def generate(self):\n    \n        report_dir = self.report_dir\n        report_file = os.path.join(report_dir, \"report.md\")\n        print(f\"Generating report to {report_file}\")\n        final_metrics = {}\n        start_time = None\n        end_time = None\n        with open(report_file, \"w\", encoding=\"utf-8\") as out_file:\n            header_file = os.path.join(report_dir, \"header.md\")\n            if os.path.exists(header_file):\n                with open(header_file, \"r\", encoding=\"utf-8\") as f:\n                    header_content = f.read()\n                    out_file.write(header_content)\n                    start_time = extract_timestamp(header_content, \"Run started:\")\n                    bloat_data = re.search(r\"\n                    bloat_data = bloat_data.group(1) if bloat_data else \"\"\n            else:\n                start_time = None\n                bloat_data = \"[bloat data missing]\"\n                print(f\"Warning: {header_file} does not exist. Did you forget to run `nanochat reset`?\")\n            for file_name in EXPECTED_FILES:\n                section_file = os.path.join(report_dir, file_name)\n                if not os.path.exists(section_file):\n                    print(f\"Warning: {section_file} does not exist, skipping\")\n                    continue\n                with open(section_file, \"r\", encoding=\"utf-8\") as in_file:\n                    section = in_file.read()\n                if \"rl\" not in file_name:\n                    end_time = extract_timestamp(section, \"timestamp:\")\n                if file_name == \"base-model-evaluation.md\":\n                    final_metrics[\"base\"] = extract(section, \"CORE\")\n                if file_name == \"chat-evaluation-mid.md\":\n                    final_metrics[\"mid\"] = extract(section, chat_metrics)\n                if file_name == \"chat-evaluation-sft.md\":\n                    final_metrics[\"sft\"] = extract(section, chat_metrics)\n                if file_name == \"chat-evaluation-rl.md\":\n                    final_metrics[\"rl\"] = extract(section, \"GSM8K\")\n                out_file.write(section)\n                out_file.write(\"\\n\")\n            out_file.write(\"\n            out_file.write(bloat_data)\n            out_file.write(\"\\n\\n\")\n            all_metrics = set()\n            for stage_metrics in final_metrics.values():\n                all_metrics.update(stage_metrics.keys())\n            all_metrics = sorted(all_metrics, key=lambda x: (x != \"CORE\", x == \"ChatCORE\", x))\n            stages = [\"base\", \"mid\", \"sft\", \"rl\"]\n            metric_width = 15\n            value_width = 8\n            header = f\"| {'Metric'.ljust(metric_width)} |\"\n            for stage in stages:\n                header += f\" {stage.upper().ljust(value_width)} |\"\n            out_file.write(header + \"\\n\")\n            separator = f\"|{'-' * (metric_width + 2)}|\"\n            for stage in stages:\n                separator += f\"{'-' * (value_width + 2)}|\"\n            out_file.write(separator + \"\\n\")\n            for metric in all_metrics:\n                row = f\"| {metric.ljust(metric_width)} |\"\n                for stage in stages:\n                    value = final_metrics.get(stage, {}).get(metric, \"-\")\n                    row += f\" {str(value).ljust(value_width)} |\"\n                out_file.write(row + \"\\n\")\n            out_file.write(\"\\n\")\n            if start_time and end_time:\n                duration = end_time - start_time\n                total_seconds = int(duration.total_seconds())\n                hours = total_seconds // 3600\n                minutes = (total_seconds % 3600) // 60\n                out_file.write(f\"Total wall clock time: {hours}h{minutes}m\\n\")\n            else:\n                out_file.write(\"Total wall clock time: unknown\\n\")\n        print(f\"Copying report.md to current directory for convenience\")\n        shutil.copy(report_file, \"report.md\")\n        return report_file\n",
      "highlight": true,
      "voiceover": "This method generates the final consolidated training report by assembling all section files, extracting key metrics, and creating a summary table with timing information. It opens the output report file, reads and writes the header if it exists (extracting start time and bloat data), iterates through all expected section files in order (skipping missing ones with warnings), extracts timestamps from each section (using the last non-RL timestamp as end time), extracts key metrics from specific sections (base model CORE, mid/sft chat metrics, RL GSM8K), appends each section to the output, creates a summary section with bloat metrics and a formatted metrics table comparing all training stages, calculates and writes total wall clock time from start to end timestamps, and finally copies the report to the current directory for convenience. The metrics table uses custom ordering (CORE first, ChatCORE last) and fixed column widths for clean formatting. This design produces a comprehensive, well-organized report that captures the entire training pipeline from tokenizer to final model.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "final_metrics = {}",
      "voiceover": "the most important final metrics we'll add as table at the end",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "start_time = None",
      "voiceover": "will cause us to not write the total wall clock time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "final_metrics[\"rl\"] = extract(section, \"GSM8K\")",
      "voiceover": "RL only evals GSM8K",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def reset(self):\n    \n        for file_name in EXPECTED_FILES:\n            file_path = os.path.join(self.report_dir, file_name)\n            if os.path.exists(file_path):\n                os.remove(file_path)\n        report_file = os.path.join(self.report_dir, \"report.md\")\n        if os.path.exists(report_file):\n            os.remove(report_file)\n        header_file = os.path.join(self.report_dir, \"header.md\")\n        header = generate_header()\n        start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        with open(header_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(header)\n            f.write(f\"Run started: {start_time}\\n\\n---\\n\\n\")\n        print(f\"Reset report and wrote header to {header_file}\")\n",
      "highlight": true,
      "voiceover": "This method resets the report state by removing all section files, the final report, and generating a fresh header with the current timestamp. It iterates through all expected section files and removes them if they exist, removes the consolidated report.md file if it exists, generates a new header with current environment information, writes it to header.md with a \"Run started\" timestamp and separator, and prints confirmation. This design prepares the report directory for a new training run while preserving the directory structure.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class DummyReport:\n",
      "highlight": true,
      "voiceover": "This class provides a no-op implementation of the Report interface for non-rank-0 processes in distributed training. It implements log and reset methods that do nothing, allowing all ranks to call report methods without actually writing files. This design ensures only rank 0 generates reports while other ranks can safely call report methods without conditional logic at every call site.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def log(self, *args, **kwargs):\n        pass\n",
      "highlight": true,
      "voiceover": "This method does nothing, providing a no-op implementation for non-rank-0 processes.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    def reset(self, *args, **kwargs):\n        pass\n",
      "highlight": true,
      "voiceover": "This method does nothing, providing a no-op implementation for non-rank-0 processes.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_report():\n    from nanochat.common import get_base_dir, get_dist_info\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    if ddp_rank == 0:\n        report_dir = os.path.join(get_base_dir(), \"report\")\n        return Report(report_dir)\n    else:\n        return DummyReport()\n",
      "highlight": true,
      "voiceover": "This function returns a Report instance for rank 0 or a DummyReport for other ranks in distributed training, ensuring only the primary process generates reports. It imports distributed training utilities, checks the current rank, and returns either a real Report instance pointing to the report directory in the base directory (for rank 0) or a DummyReport instance (for other ranks). This design simplifies report usage by allowing all ranks to call report methods without conditional logic.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Generate or reset nanochat training reports.\")\n    parser.add_argument(\"command\", nargs=\"?\", default=\"generate\", choices=[\"generate\", \"reset\"], help=\"Operation to perform (default: generate)\")\n    args = parser.parse_args()\n    if args.command == \"generate\":\n        get_report().generate()\n    elif args.command == \"reset\":\n        get_report().reset()\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Main block for command-line usage of the report generation system",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "import argparse",
      "voiceover": "Argument parser for command-line interface",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "parser = argparse.ArgumentParser(description=\"Generate or reset nanochat training reports.\")",
      "voiceover": "Create argument parser with description",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "parser.add_argument(\"command\", nargs=\"?\", default=\"generate\", choices=[\"generate\", \"reset\"], help=\"Operation to perform (default: generate)\")",
      "voiceover": "Add command argument with choices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parse command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "if args.command == \"generate\":",
      "voiceover": "If generate command specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "get_report().generate()",
      "voiceover": "Generate the consolidated report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "elif args.command == \"reset\":",
      "voiceover": "If reset command specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/report.py",
      "find": "get_report().reset()",
      "voiceover": "Reset the report state for a new run",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "nanochat/tokenizer.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport copy\nfrom functools import lru_cache\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import os",
      "voiceover": "Operating system interface for file and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import copy",
      "voiceover": "Deep copy utilities for avoiding mutation of conversation objects",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from functools import lru_cache",
      "voiceover": "Least-recently-used cache decorator for memoizing special token encoding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "SPECIAL_TOKENS = [\n    \"<|bos|>\",\n    \"<|user_start|>\",\n    \"<|user_end|>\",\n    \"<|assistant_start|>\",\n    \"<|assistant_end|>\",\n    \"<|python_start|>\",\n    \"<|python_end|>\",\n    \"<|output_start|>\",\n    \"<|output_end|>\",\n]\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "SPECIAL_TOKENS = [",
      "voiceover": "List of special tokens used for document delimiting and conversation formatting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "\"<|user_start|>\",",
      "voiceover": "user messages",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "\"<|assistant_start|>\",",
      "voiceover": "assistant messages",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "\"<|python_start|>\",",
      "voiceover": "assistant invokes python REPL tool",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "\"<|output_start|>\",",
      "voiceover": "python REPL outputs back to assistant",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"",
      "voiceover": "NOTE: this split pattern deviates from GPT-4 in that we use \\p{N}{1,2} instead of \\p{N}{1,3} I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes. I haven't validated that this is actually a good idea, TODO.",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"",
      "voiceover": "Regex pattern for splitting text before BPE, modified from GPT-4 to use 1-2 digit numbers instead of 1-3",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "from tokenizers import Tokenizer as HFTokenizer\nfrom tokenizers import pre_tokenizers, decoders, Regex\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from tokenizers import Tokenizer as HFTokenizer # HuggingFace tokenizer base class",
      "voiceover": "Section separator indicating HuggingFace tokenizer implementation # ----------------------------------------------------------------------------- Generic GPT-4-style tokenizer based on HuggingFace Tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from tokenizers import Tokenizer as HFTokenizer",
      "voiceover": "HuggingFace tokenizer base class",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from tokenizers import pre_tokenizers, decoders, Regex",
      "voiceover": "Tokenizer components for pre-processing, decoding, and regex patterns",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from tokenizers.models import BPE",
      "voiceover": "Byte Pair Encoding model implementation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "from tokenizers.trainers import BpeTrainer",
      "voiceover": "BPE training algorithm",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class HuggingFaceTokenizer:\n",
      "highlight": true,
      "voiceover": "# Module-level docstring explaining the BPE tokenizer implementations\nBPE Tokenizer in the style of GPT-4.\n\nTwo implementations are available:\n1) HuggingFace Tokenizer that can do both training and inference but is really confusing\n2) Our own RustBPE Tokenizer for training and tiktoken for efficient inference\n\nThis class provides a GPT-4-style BPE tokenizer implementation using the HuggingFace tokenizers library, supporting both training and inference. It wraps a HuggingFace Tokenizer object configured with byte-level BPE, no normalization, GPT-4-style regex pre-tokenization, and byte-level decoding. The class provides factory methods to create tokenizers from pretrained models, local directories, or by training from scratch on text iterators. It supports encoding text to token IDs with optional prepend/append tokens, decoding token IDs back to text, handling special tokens, and saving trained tokenizers to disk. The implementation uses byte_fallback=True to ensure all bytes can be encoded, applies the GPT-4 split pattern (modified for smaller vocab sizes), and includes all special tokens for conversation formatting. This design provides a complete tokenizer solution but is noted as \"really confusing\" compared to the RustBPE alternative.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n",
      "highlight": true,
      "voiceover": "This initializer wraps a HuggingFace Tokenizer object, storing it for use in encoding, decoding, and other operations.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @classmethod\n"
    },
    {
      "type": "writeText",
      "content": "    def from_pretrained(cls, hf_path):\n        tokenizer = HFTokenizer.from_pretrained(hf_path)\n        return cls(tokenizer)\n",
      "highlight": true,
      "voiceover": "This class method creates a tokenizer from a HuggingFace pretrained model by loading the tokenizer configuration and vocabulary from the HuggingFace hub. It takes a model identifier like \"gpt2\" and returns a wrapped tokenizer instance ready for use.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @classmethod\n"
    },
    {
      "type": "writeText",
      "content": "    def from_directory(cls, tokenizer_dir):\n        tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n        tokenizer = HFTokenizer.from_file(tokenizer_path)\n        return cls(tokenizer)\n",
      "highlight": true,
      "voiceover": "This class method creates a tokenizer from a local directory by loading the tokenizer.json file. It expects the directory to contain a saved tokenizer configuration and returns a wrapped tokenizer instance.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @classmethod\n"
    },
    {
      "type": "writeText",
      "content": "    def train_from_iterator(cls, text_iterator, vocab_size):\n        tokenizer = HFTokenizer(BPE(\n            byte_fallback=True,\n            unk_token=None,\n            fuse_unk=False,\n        ))\n        tokenizer.normalizer = None\n        gpt4_split_regex = Regex(SPLIT_PATTERN)\n        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior=\"isolated\", invert=False),\n            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n        ])\n        tokenizer.decoder = decoders.ByteLevel()\n        tokenizer.post_processor = None\n        trainer = BpeTrainer(\n            vocab_size=vocab_size,\n            show_progress=True,\n            min_frequency=0,\n            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n            special_tokens=SPECIAL_TOKENS,\n        )\n        tokenizer.train_from_iterator(text_iterator, trainer)\n        return cls(tokenizer)\n",
      "highlight": true,
      "voiceover": "This class method trains a new BPE tokenizer from scratch using an iterator of text strings. It configures a HuggingFace Tokenizer with byte-level BPE (with byte_fallback=True for complete coverage), no normalization, GPT-4-style regex pre-tokenization using the modified split pattern, byte-level decoding, and no post-processing. The method creates a BPE trainer with the specified vocab size, no minimum frequency requirement, byte-level alphabet initialization, and all special tokens. It then trains the tokenizer on the provided text iterator and returns a wrapped instance. This design provides complete control over tokenizer training while using the HuggingFace infrastructure.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "byte_fallback=True,",
      "voiceover": "needed!",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "gpt4_split_regex = Regex(SPLIT_PATTERN)",
      "voiceover": "huggingface demands that you wrap it in Regex!!",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "min_frequency=0,",
      "voiceover": "no minimum frequency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_vocab_size(self):\n        return self.tokenizer.get_vocab_size()\n",
      "highlight": true,
      "voiceover": "This method returns the total vocabulary size of the tokenizer including all regular and special tokens.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_special_tokens(self):\n        special_tokens_map = self.tokenizer.get_added_tokens_decoder()\n        special_tokens = [w.content for w in special_tokens_map.values()]\n        return special_tokens\n",
      "highlight": true,
      "voiceover": "This method returns a list of all special token strings by extracting them from the tokenizer's added tokens decoder. It retrieves the special tokens map and extracts the content of each token.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def id_to_token(self, id):\n        return self.tokenizer.id_to_token(id)\n",
      "highlight": true,
      "voiceover": "This method converts a single token ID to its corresponding token string, useful for debugging and visualization.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def _encode_one(self, text, prepend=None, append=None):\n        assert isinstance(text, str)\n        ids = []\n        if prepend is not None:\n            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n            ids.append(prepend_id)\n        ids.extend(self.tokenizer.encode(text, add_special_tokens=False).ids)\n        if append is not None:\n            append_id = append if isinstance(append, int) else self.encode_special(append)\n            ids.append(append_id)\n        return ids\n",
      "highlight": true,
      "voiceover": "This method encodes a single string to token IDs with optional prepend and append tokens. It validates that the input is a string, optionally prepends a token (either as a string special token or direct ID), encodes the main text without adding special tokens automatically, and optionally appends a token. The method returns a list of token IDs. This design provides fine-grained control over token sequences for training and inference.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def encode_special(self, text):\n        return self.tokenizer.token_to_id(text)\n",
      "highlight": true,
      "voiceover": "This method encodes a single special token string to its token ID using exact string matching. It's used to get IDs for special tokens like BOS, user_start, etc.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_bos_token_id(self):\n        bos = self.encode_special(\"<|bos|>\")\n        return bos\n",
      "highlight": true,
      "voiceover": "This method returns the token ID for the beginning-of-sequence (BOS) special token, which is prepended to documents during training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def encode(self, text, *args, **kwargs):\n        if isinstance(text, str):\n            return self._encode_one(text, *args, **kwargs)\n        elif isinstance(text, list):\n            return [self._encode_one(t, *args, **kwargs) for t in text]\n        else:\n            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
      "highlight": true,
      "voiceover": "This method encodes text to token IDs, handling both single strings and lists of strings. For single strings it calls _encode_one, for lists it maps _encode_one over each element, and for other types it raises an error. This design provides a flexible encoding interface.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __call__(self, *args, **kwargs):\n        return self.encode(*args, **kwargs)\n",
      "highlight": true,
      "voiceover": "This method makes the tokenizer callable, delegating to the encode method for convenient usage.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def decode(self, ids):\n        return self.tokenizer.decode(ids, skip_special_tokens=False)\n",
      "highlight": true,
      "voiceover": "This method decodes a list of token IDs back to a string, preserving special tokens in the output.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def save(self, tokenizer_dir):\n        os.makedirs(tokenizer_dir, exist_ok=True)\n        tokenizer_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n        self.tokenizer.save(tokenizer_path)\n        print(f\"Saved tokenizer to {tokenizer_path}\")\n",
      "highlight": true,
      "voiceover": "This method saves the tokenizer to disk by creating the directory if needed, writing the tokenizer configuration to tokenizer.json, and printing confirmation. This allows trained tokenizers to be persisted and reloaded later.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "import pickle\nimport rustbpe\nimport tiktoken\n"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import pickle # Serialization for saving/loading tiktoken encodings",
      "voiceover": "Section separator indicating RustBPE tokenizer implementation # ----------------------------------------------------------------------------- Tokenizer based on rustbpe + tiktoken combo",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import pickle",
      "voiceover": "Serialization for saving/loading tiktoken encodings",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import rustbpe",
      "voiceover": "Rust-based BPE training library for fast tokenizer training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "import tiktoken",
      "voiceover": "OpenAI's efficient tokenization library for inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class RustBPETokenizer:\n",
      "highlight": true,
      "voiceover": "This class provides a high-performance BPE tokenizer implementation using rustbpe for training and tiktoken for inference, offering better performance than the HuggingFace alternative. It trains tokenizers using the rustbpe library (written in Rust for speed), then constructs a tiktoken Encoding object for efficient inference. The class provides factory methods to create tokenizers from training iterators, local directories, or pretrained tiktoken encodings. It supports encoding text to token IDs with optional prepend/append tokens and multi-threaded batch encoding, decoding token IDs back to text, rendering conversations with special tokens and supervision masks for finetuning, visualizing tokenization with color-coded output, and saving trained tokenizers as pickle files. The implementation uses LRU caching for special token encoding, handles both string and list inputs efficiently, and provides specialized methods for conversation formatting including system message merging, tool call handling, and supervision mask generation. This design combines the speed of Rust training with the efficiency of tiktoken inference for optimal performance.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, enc, bos_token):\n        self.enc = enc\n        self.bos_token_id = self.encode_special(bos_token)\n",
      "highlight": true,
      "voiceover": "This initializer wraps a tiktoken Encoding object and stores the BOS token ID by encoding the specified BOS token string. It prepares the tokenizer for efficient inference operations.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @classmethod\n"
    },
    {
      "type": "writeText",
      "content": "    def from_directory(cls, tokenizer_dir):\n        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n        with open(pickle_path, \"rb\") as f:\n            enc = pickle.load(f)\n        return cls(enc, \"<|bos|>\")\n",
      "highlight": true,
      "voiceover": "Initializes the tokenizer from a local directory containing a tokenizer.pkl file.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_vocab_size(self):\n        return self.enc.n_vocab\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_special_tokens(self):\n        return self.enc.special_tokens_set\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def id_to_token(self, id):\n        return self.enc.decode([id])\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n    @lru_cache(maxsize=32)\n"
    },
    {
      "type": "writeText",
      "content": "    def encode_special(self, text):\n        return self.enc.encode_single_token(text)\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_bos_token_id(self):\n        return self.bos_token_id\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def encode(self, text, prepend=None, append=None, num_threads=8):\n\n        if prepend is not None:\n            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n        if append is not None:\n            append_id = append if isinstance(append, int) else self.encode_special(append)\n\n        if isinstance(text, str):\n            ids = self.enc.encode_ordinary(text)\n            if prepend is not None:\n                ids.insert(0, prepend_id)\n            if append is not None:\n                ids.append(append_id)\n        elif isinstance(text, list):\n            ids = self.enc.encode_ordinary_batch(text, num_threads=num_threads)\n            if prepend is not None:\n                for ids_row in ids:\n                    ids_row.insert(0, prepend_id)\n            if append is not None:\n                for ids_row in ids:\n                    ids_row.append(append_id)\n        else:\n            raise ValueError(f\"Invalid input type: {type(text)}\")\n\n        return ids\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "ids.insert(0, prepend_id)",
      "voiceover": "TODO: slightly inefficient here? :( hmm",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "ids_row.insert(0, prepend_id)",
      "voiceover": "TODO: same",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __call__(self, *args, **kwargs):\n        return self.encode(*args, **kwargs)\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def decode(self, ids):\n        return self.enc.decode(ids)\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def save(self, tokenizer_dir):\n        os.makedirs(tokenizer_dir, exist_ok=True)\n        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n        with open(pickle_path, \"wb\") as f:\n            pickle.dump(self.enc, f)\n        print(f\"Saved tokenizer encoding to {pickle_path}\")\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def render_conversation(self, conversation, max_tokens=2048):\n    \n        ids, mask = [], []\n        def add_tokens(token_ids, mask_val):\n            if isinstance(token_ids, int):\n                token_ids = [token_ids]\n            ids.extend(token_ids)\n            mask.extend([mask_val] * len(token_ids))\n\n        if conversation[\"messages\"][0][\"role\"] == \"system\":\n            conversation = copy.deepcopy(conversation)\n            messages = conversation[\"messages\"]\n            assert messages[1][\"role\"] == \"user\", \"System message must be followed by a user message\"\n            messages[1][\"content\"] = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n            messages = messages[1:]\n        else:\n            messages = conversation[\"messages\"]\n        assert len(messages) >= 1, f\"Conversation has less than 1 message: {messages}\"\n\n        bos = self.get_bos_token_id()\n        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n\n        add_tokens(bos, 0)\n        for i, message in enumerate(messages):\n\n            must_be_from = \"user\" if i % 2 == 0 else \"assistant\"\n            assert message[\"role\"] == must_be_from, f\"Message {i} is from {message['role']} but should be from {must_be_from}\"\n\n            content = message[\"content\"]\n\n            if message[\"role\"] == \"user\":\n                assert isinstance(content, str), \"User messages are simply expected to be strings\"\n                value_ids = self.encode(content)\n                add_tokens(user_start, 0)\n                add_tokens(value_ids, 0)\n                add_tokens(user_end, 0)\n            elif message[\"role\"] == \"assistant\":\n                add_tokens(assistant_start, 0)\n                if isinstance(content, str):\n                    value_ids = self.encode(content)\n                    add_tokens(value_ids, 1)\n                elif isinstance(content, list):\n                    for part in content:\n                        value_ids = self.encode(part[\"text\"])\n                        if part[\"type\"] == \"text\":\n                            add_tokens(value_ids, 1)\n                        elif part[\"type\"] == \"python\":\n                            add_tokens(python_start, 1)\n                            add_tokens(value_ids, 1)\n                            add_tokens(python_end, 1)\n                        elif part[\"type\"] == \"python_output\":\n                            add_tokens(output_start, 0)\n                            add_tokens(value_ids, 0)\n                            add_tokens(output_end, 0)\n                        else:\n                            raise ValueError(f\"Unknown part type: {part['type']}\")\n                else:\n                    raise ValueError(f\"Unknown content type: {type(content)}\")\n                add_tokens(assistant_end, 1)\n\n        ids = ids[:max_tokens]\n        mask = mask[:max_tokens]\n        return ids, mask\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "conversation = copy.deepcopy(conversation)",
      "voiceover": "avoid mutating the original",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def visualize_tokenization(self, ids, mask, with_token_id=False):\n        \n        RED = '\\033[91m'\n        GREEN = '\\033[92m'\n        RESET = '\\033[0m'\n        GRAY = '\\033[90m'\n        tokens = []\n        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n            token_str = self.decode([token_id])\n            color = GREEN if mask_val == 1 else RED\n            tokens.append(f\"{color}{token_str}{RESET}\")\n            if with_token_id:\n                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n        return '|'.join(tokens)\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def render_for_completion(self, conversation):\n\n        conversation = copy.deepcopy(conversation)\n        messages = conversation[\"messages\"]\n        assert messages[-1][\"role\"] == \"assistant\", \"Last message must be from the Assistant\"\n        messages.pop()\n\n        ids, mask = self.render_conversation(conversation)\n\n        assistant_start = self.encode_special(\"<|assistant_start|>\")\n        ids.append(assistant_start)\n        return ids\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "conversation = copy.deepcopy(conversation)",
      "voiceover": "avoid mutating the original",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "nanochat/tokenizer.py",
      "find": "messages.pop()",
      "voiceover": "remove the last message (of the Assistant) inplace",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def get_tokenizer():\n    from nanochat.common import get_base_dir\n    base_dir = get_base_dir()\n    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n    return RustBPETokenizer.from_directory(tokenizer_dir)\n",
      "highlight": true
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_token_bytes(device=\"cpu\"):\n    import torch\n    from nanochat.common import get_base_dir\n    base_dir = get_base_dir()\n    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n    assert os.path.exists(token_bytes_path), f\"Token bytes not found at {token_bytes_path}? It gets written by tok_train.py\"\n    with open(token_bytes_path, \"rb\") as f:\n        token_bytes = torch.load(f, map_location=device)\n    return token_bytes\n",
      "highlight": true
    },
    {
      "type": "openFile",
      "path": "scripts/base_eval.py"
    },
    {
      "type": "writeText",
      "content": "import os\nimport csv\nimport time\nimport json\nimport yaml\nimport shutil\nimport random\nimport zipfile\nimport tempfile\nfrom contextlib import nullcontext\n\nimport torch\n\nfrom nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, autodetect_device_type, download_file_with_lock\nfrom nanochat.tokenizer import HuggingFaceTokenizer\nfrom nanochat.checkpoint_manager import load_model\nfrom nanochat.core_eval import evaluate_task\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import os",
      "voiceover": "Operating system interface for file and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import csv",
      "voiceover": "CSV file reading for evaluation metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import time",
      "voiceover": "Time utilities for measuring evaluation duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import json",
      "voiceover": "JSON parsing for loading evaluation data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import yaml",
      "voiceover": "YAML parsing for loading evaluation configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import shutil",
      "voiceover": "High-level file operations for moving extracted bundles",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import random",
      "voiceover": "Random number generation for shuffling evaluation data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import zipfile",
      "voiceover": "ZIP archive extraction for eval bundle",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import tempfile",
      "voiceover": "Temporary directory creation for safe extraction",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as fallback for autocast",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import torch",
      "voiceover": "PyTorch library for model inference and autocast",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, autodetect_device_type, download_file_with_lock",
      "voiceover": "Common utilities for distributed setup, cleanup, printing, directory management, device detection, and file downloading",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from nanochat.tokenizer import HuggingFaceTokenizer",
      "voiceover": "HuggingFace tokenizer wrapper for encoding text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Checkpoint loading utility for nanochat models",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from nanochat.core_eval import evaluate_task",
      "voiceover": "Core evaluation function for running individual tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\"",
      "voiceover": "~162MB of data needed to evaluate the CORE metric # URL for downloading the evaluation bundle containing CORE benchmark data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def place_eval_bundle(file_path):\n    base_dir = get_base_dir()\n    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(tmpdir)\n        extracted_bundle_dir = os.path.join(tmpdir, \"eval_bundle\")\n        shutil.move(extracted_bundle_dir, eval_bundle_dir)\n    print0(f\"Placed eval_bundle directory at {eval_bundle_dir}\")\n",
      "highlight": true,
      "voiceover": "This function extracts the downloaded evaluation bundle ZIP file and places it in the base directory. It creates a temporary directory for safe extraction, unzips the eval_bundle.zip file into the temp directory, moves the extracted eval_bundle directory to the base directory, and prints confirmation. This postprocessing function is called by download_file_with_lock after the bundle is downloaded, ensuring the evaluation data is properly organized for use.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def evaluate_model(model, tokenizer, device, max_per_task=-1):\n\n    base_dir = get_base_dir()\n    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n    if not os.path.exists(eval_bundle_dir):\n        download_file_with_lock(EVAL_BUNDLE_URL, \"eval_bundle.zip\", postprocess_fn=place_eval_bundle)\n    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n    eval_meta_data = os.path.join(eval_bundle_dir, \"eval_meta_data.csv\")\n    with open(config_path, 'r', encoding='utf-8') as f:\n        config = yaml.safe_load(f)\n    tasks = config['icl_tasks']\n\n    random_baselines = {}\n    with open(eval_meta_data, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            task_name = row['Eval Task']\n            random_baseline = row['Random baseline']\n            random_baselines[task_name] = float(random_baseline)\n\n    results = {}\n    centered_results = {}\n    for task in tasks:\n        start_time = time.time()\n        label = task['label']\n        task_meta = {\n            'task_type': task['icl_task_type'],\n            'dataset_uri': task['dataset_uri'],\n            'num_fewshot': task['num_fewshot'][0],\n            'continuation_delimiter': task.get('continuation_delimiter', ' ')\n        }\n        print0(f\"Evaluating: {label} ({task_meta['num_fewshot']}-shot, type: {task_meta['task_type']})... \", end='')\n\n        data_path = os.path.join(data_base_path, task_meta['dataset_uri'])\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data = [json.loads(line.strip()) for line in f]\n        shuffle_rng = random.Random(1337)\n        shuffle_rng.shuffle(data)\n        if max_per_task > 0:\n            data = data[:max_per_task]\n\n        accuracy = evaluate_task(model, tokenizer, data, device, task_meta)\n\n        results[label] = accuracy\n        random_baseline = random_baselines[label]\n        centered_result = (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)\n        centered_results[label] = centered_result\n        end_time = time.time()\n        print0(f\"accuracy: {accuracy:.4f} | centered: {centered_result:.4f} | time: {end_time - start_time:.2f}s\")\n\n    core_metric = sum(centered_results.values()) / len(centered_results)\n    out = {\n        \"results\": results,\n        \"centered_results\": centered_results,\n        \"core_metric\": core_metric\n    }\n    return out\n",
      "highlight": true,
      "voiceover": "This function evaluates a language model on the CORE benchmark by running it on multiple in-context learning tasks and computing centered accuracy metrics. It downloads the evaluation bundle if needed, loads the task configuration from core.yaml, reads random baseline values from eval_meta_data.csv for centering the results, iterates through all tasks in the configuration, loads and shuffles the data for each task (optionally limiting to max_per_task examples for debugging), runs the evaluation using evaluate_task, computes centered results by normalizing accuracy relative to random baseline, calculates the overall CORE metric as the average of centered results, and returns a dictionary with raw accuracies, centered results, and the final CORE metric. The centering formula (accuracy - random_baseline) / (1.0 - random_baseline) normalizes performance between random guessing and perfect accuracy, making results comparable across tasks with different numbers of choices.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class ModelWrapper:\n",
      "highlight": true,
      "voiceover": "This class provides a lightweight wrapper around HuggingFace models to provide a consistent interface for evaluation. It stores the model and optional maximum sequence length, and implements a __call__ method that extracts logits from the model's output. This wrapper normalizes the interface between HuggingFace models (which return complex output objects) and the evaluation code (which expects just logits), making it easier to evaluate different model types with the same evaluation pipeline.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, model, max_seq_len=None):\n        self.model = model\n        self.max_seq_len = max_seq_len\n",
      "highlight": true,
      "voiceover": "This initializer stores the HuggingFace model and optional maximum sequence length for later use during inference.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __call__(self, input_ids):\n        outputs = self.model(input_ids)\n        logits = outputs.logits\n        return logits\n",
      "highlight": true,
      "voiceover": "This method runs inference on the model and extracts logits from the output. It takes input token IDs, passes them through the model, and returns just the logits tensor, discarding other output fields that HuggingFace models typically return.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def load_hf_model(hf_path: str, device):\n    print0(f\"Loading model from: {hf_path}\")\n    from transformers import AutoModelForCausalLM\n    model = AutoModelForCausalLM.from_pretrained(hf_path)\n    model.to(device)\n    model.eval()\n    max_seq_len = 1024 if \"openai-community/gpt2\" in hf_path else None\n    model = ModelWrapper(model, max_seq_len=max_seq_len)\n    tokenizer = HuggingFaceTokenizer.from_pretrained(hf_path)\n    return model, tokenizer\n",
      "highlight": true,
      "voiceover": "This function loads a HuggingFace model and tokenizer from a pretrained path for evaluation. It imports AutoModelForCausalLM, loads the model from the HuggingFace hub or local path, moves it to the specified device, sets it to evaluation mode, wraps it in ModelWrapper with a max sequence length of 1024 for GPT-2 models (to match their training context), loads the corresponding HuggingFace tokenizer, and returns both the wrapped model and tokenizer. This function provides a convenient way to evaluate any HuggingFace model using the nanochat evaluation pipeline.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--hf-path', type=str, default=None, help='HuggingFace model path to evaluate')\n    parser.add_argument('--max-per-task', type=int, default=-1, help='Max examples per task to evaluate (-1 = disable)')\n    args = parser.parse_args()\n\n    device_type = autodetect_device_type()\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n    autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n\n    if args.hf_path is not None:\n        hf_path = args.hf_path\n        print0(f\"Loading huggingface model from: {hf_path}\")\n        model, tokenizer = load_hf_model(hf_path, device)\n        model_name = hf_path\n        model_slug = hf_path.replace(\"/\", \"-\")\n    else:\n        model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")\n        model_name = f\"base_model (step {meta['step']})\"\n        model_slug = f\"base_model_{meta['step']:06d}\"\n\n    with autocast_ctx:\n        out = evaluate_model(model, tokenizer, device, max_per_task=args.max_per_task)\n\n    core_metric = None\n    centered_results = {}\n    if ddp_rank == 0:\n        base_dir = get_base_dir()\n        output_csv_path = os.path.join(base_dir, \"base_eval\", f\"{model_slug}.csv\")\n        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n        results = out[\"results\"]\n        centered_results = out[\"centered_results\"]\n        core_metric = out[\"core_metric\"]\n        with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n            f.write(f\"{'Task':<35}, {'Accuracy':<10}, {'Centered':<10}\\n\")\n            for label in results:\n                f.write(f\"{label:<35}, {results[label]:<10.6f}, {centered_results[label]:<10.6f}\\n\")\n            f.write(f\"{'CORE':<35}, {'':<10}, {core_metric:<10.6f}\\n\")\n        print0(\"=\"*80)\n        print0(f\"Model: {model_name}\")\n        print0(\"=\"*80)\n        with open(output_csv_path, 'r', encoding='utf-8') as f:\n            print0(f.read())\n\n    from nanochat.report import get_report\n    get_report().log(section=\"Base model evaluation\", data=[\n        {\n            \"Model\": model_name,\n            \"CORE metric\": core_metric,\n        },\n        centered_results,\n    ])\n\n    compute_cleanup()\n",
      "highlight": true,
      "voiceover": "This function serves as the main entry point for base model evaluation, orchestrating the entire evaluation pipeline from model loading to results reporting. It parses command-line arguments for optional HuggingFace model path and max examples per task, initializes distributed training and mixed precision contexts, loads either a HuggingFace model or a local nanochat model based on arguments, runs the CORE benchmark evaluation with optional autocast for bfloat16 precision on CUDA, writes results to a CSV file on rank 0 including per-task accuracies and centered results, prints the results table to console, logs the evaluation to the training report, and cleans up distributed resources. This design supports both evaluating external HuggingFace models for comparison and tracking the progress of nanochat models during training.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "import argparse",
      "voiceover": "Argument parser for command-line interface",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "parser = argparse.ArgumentParser()",
      "voiceover": "Create argument parser",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "parser.add_argument('--hf-path', type=str, default=None, help='HuggingFace model path to evaluate')",
      "voiceover": "Optional HuggingFace model path argument",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "parser.add_argument('--max-per-task', type=int, default=-1, help='Max examples per task to evaluate (-1 = disable)')",
      "voiceover": "Optional limit on examples per task for debugging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parse command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "model_name = hf_path",
      "voiceover": "just for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "model_slug = hf_path.replace(\"/\", \"-\")",
      "voiceover": "for the output csv file",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "model_name = f\"base_model (step {meta['step']})\"",
      "voiceover": "just for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "model_slug = f\"base_model_{meta['step']:06d}\"",
      "voiceover": "for the output csv file",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report utility for logging evaluation results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "centered_results,",
      "voiceover": "the full table",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    main()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Main block for command-line execution of the evaluation script",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_eval.py",
      "find": "main()",
      "voiceover": "Run the main evaluation function",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/base_loss.py"
    },
    {
      "type": "writeText",
      "content": "import os\nfrom contextlib import nullcontext\nimport torch\nfrom nanochat.checkpoint_manager import load_model\nfrom nanochat.common import compute_init, print0, compute_cleanup, autodetect_device_type\nfrom nanochat.dataloader import tokenizing_distributed_data_loader\nfrom nanochat.tokenizer import get_token_bytes\nfrom nanochat.loss_eval import evaluate_bpb\nfrom nanochat.engine import Engine\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "import os",
      "voiceover": "Operating system interface for file and directory operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as fallback for autocast",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "import torch",
      "voiceover": "PyTorch library for model inference and autocast",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Checkpoint loading utility for nanochat models",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.common import compute_init, print0, compute_cleanup, autodetect_device_type",
      "voiceover": "Common utilities for distributed setup, printing, cleanup, and device detection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.dataloader import tokenizing_distributed_data_loader",
      "voiceover": "Distributed data loader for streaming tokenized text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.tokenizer import get_token_bytes",
      "voiceover": "Utility to get byte lengths of tokens for bits-per-byte calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.loss_eval import evaluate_bpb",
      "voiceover": "Function to evaluate bits-per-byte metric on a dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for generating text samples from the model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "device_batch_size = 32\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "device_batch_size = 32",
      "voiceover": "Configuration # Configuration section for loss evaluation parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "device_batch_size = 32",
      "voiceover": "Number of sequences per device in each batch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "split_tokens = 20*524288\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "split_tokens = 20*524288",
      "voiceover": "number of tokens to evaluate per split # Total tokens to evaluate per split (train/val), approximately 10M tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_tag = None\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "model_tag = None",
      "voiceover": "optional model tag for the output directory name # Optional tag to identify specific model variant",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_step = None\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "model_step = None",
      "voiceover": "optional model step for the output directory name # Optional step number to load specific checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_type = \"\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "device_type = \"\"",
      "voiceover": "cuda|cpu|mps (empty => autodetect) # Device type for computation, empty string triggers autodetection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "exec(open(os.path.join('nanochat', 'configurator.py')).read())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "exec(open(os.path.join('nanochat', 'configurator.py')).read())",
      "voiceover": "overrides from command line or config file # Execute configurator to allow command-line overrides of the above configuration variables",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "device_type = autodetect_device_type() if device_type == \"\" else device_type\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "device_type = autodetect_device_type() if device_type == \"\" else device_type",
      "voiceover": "Load the base model and the tokenizer # Model and tokenizer loading section",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "device_type = autodetect_device_type() if device_type == \"\" else device_type",
      "voiceover": "Autodetect device if not specified, otherwise use provided device type",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initialize distributed training environment and get device information",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\", model_tag=model_tag, step=model_step)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\", model_tag=model_tag, step=model_step)",
      "voiceover": "Load base model checkpoint with tokenizer and metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "sequence_len = meta[\"model_config\"][\"sequence_len\"]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "sequence_len = meta[\"model_config\"][\"sequence_len\"]",
      "voiceover": "could be arbitrary really # Extract sequence length from model config for data loading",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Create autocast context for bfloat16 precision on CUDA, or no-op context otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "tokens_per_step = device_batch_size * sequence_len * ddp_world_size\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "tokens_per_step = device_batch_size * sequence_len * ddp_world_size",
      "voiceover": "Evaluate the loss on each split # Loss evaluation section for train and validation splits",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "tokens_per_step = device_batch_size * sequence_len * ddp_world_size",
      "voiceover": "Calculate total tokens processed per step across all devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assert split_tokens % tokens_per_step == 0, \"split_tokens must be divisible by tokens_per_step\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "assert split_tokens % tokens_per_step == 0, \"split_tokens must be divisible by tokens_per_step\"",
      "voiceover": "Ensure split_tokens divides evenly into steps for clean evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "steps = split_tokens // tokens_per_step\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "steps = split_tokens // tokens_per_step",
      "voiceover": "Calculate number of evaluation steps needed to process all split tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes = get_token_bytes(device=device)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "token_bytes = get_token_bytes(device=device)",
      "voiceover": "Load byte lengths for each token in vocabulary for bits-per-byte calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "bpb_results = {}\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "bpb_results = {}",
      "voiceover": "Dictionary to store bits-per-byte results for each split",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for split_name in [\"train\", \"val\"]:\n    loader = tokenizing_distributed_data_loader(device_batch_size, sequence_len, split_name, device=device)\n    with autocast_ctx:\n        bpb = evaluate_bpb(model, loader, steps, token_bytes)\n    print0(f\"{split_name} bpb: {bpb:.4f}\")\n    bpb_results[split_name] = bpb\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "for split_name in [\"train\", \"val\"]:",
      "voiceover": "Iterate through train and validation splits",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "loader = tokenizing_distributed_data_loader(device_batch_size, sequence_len, split_name, device=device)",
      "voiceover": "Create distributed data loader for current split",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use autocast for mixed precision if on CUDA",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "bpb = evaluate_bpb(model, loader, steps, token_bytes)",
      "voiceover": "Evaluate bits-per-byte metric on the split",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "print0(f\"{split_name} bpb: {bpb:.4f}\")",
      "voiceover": "Print result from rank 0 only",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "bpb_results[split_name] = bpb",
      "voiceover": "Store result for later reporting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "samples = []\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "samples = []",
      "voiceover": "Master process also samples from the model # Sampling section to generate text completions from the model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "samples = []",
      "voiceover": "List to store generated text samples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "if ddp_rank == 0:\n    prompts = [\n        \"The capital of France is\",\n        \"The chemical symbol of gold is\",\n        \"If yesterday was Friday, then tomorrow will be\",\n        \"The opposite of hot is\",\n        \"The planets of the solar system are:\",\n        \"My favorite color is\",\n        \"If 5*x + 3 = 13, then x is\",\n    ]\n    engine = Engine(model, tokenizer)\n    for prompt in prompts:\n        tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n        with autocast_ctx:\n            sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n        sample_str = tokenizer.decode(sample[0])\n        print0(sample_str)\n        samples.append(sample_str)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "if ddp_rank == 0:",
      "voiceover": "Only rank 0 generates samples to avoid duplication",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "prompts = [",
      "voiceover": "List of diverse prompts to test model's knowledge and reasoning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create inference engine for text generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "for prompt in prompts:",
      "voiceover": "Generate completion for each prompt",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "tokens = tokenizer(prompt, prepend=\"<|bos|>\")",
      "voiceover": "Tokenize prompt with BOS token prepended",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use autocast for mixed precision if on CUDA",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)",
      "voiceover": "Generate greedy completion with max 16 tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "sample_str = tokenizer.decode(sample[0])",
      "voiceover": "Decode generated tokens back to text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "print0(sample_str)",
      "voiceover": "Print the complete prompt and completion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "samples.append(sample_str)",
      "voiceover": "Store sample for reporting",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.report import get_report # Import report utility for logging evaluation results",
      "voiceover": "Log to report # Reporting section to log results to training report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report utility for logging evaluation results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Base model loss\", data=[\n    {\n        \"train bpb\": bpb_results[\"train\"],\n        \"val bpb\": bpb_results[\"val\"],\n    },\n    {f\"sample {i}\": sample for i, sample in enumerate(samples)},\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "get_report().log(section=\"Base model loss\", data=[",
      "voiceover": "Log loss evaluation results and samples to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "\"train bpb\": bpb_results[\"train\"],",
      "voiceover": "Training set bits-per-byte",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "\"val bpb\": bpb_results[\"val\"],",
      "voiceover": "Validation set bits-per-byte",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "{f\"sample {i}\": sample for i, sample in enumerate(samples)},",
      "voiceover": "All generated samples with indices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "compute_cleanup()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "compute_cleanup()",
      "voiceover": "Cleanup # Cleanup section to properly shut down distributed training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_loss.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed training resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/base_train.py"
    },
    {
      "type": "writeText",
      "content": "import os\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "import os",
      "voiceover": "Operating system interface for environment variables and file paths",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"",
      "voiceover": "Configure PyTorch CUDA memory allocator to use expandable segments for better memory management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "import time\nfrom contextlib import nullcontext\n\nimport wandb\nimport torch\n\nfrom nanochat.gpt import GPT, GPTConfig\nfrom nanochat.dataloader import tokenizing_distributed_data_loader, tokenizing_distributed_data_loader_with_state\nfrom nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, print_banner, get_base_dir, autodetect_device_type\nfrom nanochat.tokenizer import get_tokenizer, get_token_bytes\nfrom nanochat.checkpoint_manager import save_checkpoint, load_checkpoint\nfrom nanochat.loss_eval import evaluate_bpb\nfrom nanochat.engine import Engine\nfrom scripts.base_eval import evaluate_model\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "import time",
      "voiceover": "Time-related functions for measuring training duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as a placeholder when autocast is disabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "import wandb",
      "voiceover": "Weights & Biases for experiment tracking and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.gpt import GPT, GPTConfig",
      "voiceover": "GPT model architecture and configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.dataloader import tokenizing_distributed_data_loader, tokenizing_distributed_data_loader_with_state",
      "voiceover": "Data loading utilities for distributed training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, print_banner, get_base_dir, autodetect_device_type",
      "voiceover": "Common utilities for distributed computing and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.tokenizer import get_tokenizer, get_token_bytes",
      "voiceover": "Tokenizer for text encoding/decoding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.checkpoint_manager import save_checkpoint, load_checkpoint",
      "voiceover": "Checkpoint saving and loading utilities",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.loss_eval import evaluate_bpb",
      "voiceover": "Evaluation function for bits-per-byte metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for text generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from scripts.base_eval import evaluate_model",
      "voiceover": "Model evaluation function for CORE metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print_banner()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print_banner()",
      "voiceover": "Display the nanochat banner at startup",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "run = \"dummy\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "run = \"dummy\"",
      "voiceover": "----------------------------------------------------------------------------- User settings",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "run = \"dummy\"",
      "voiceover": "Wandb run name (\"dummy\" disables wandb logging)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_type = \"\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "device_type = \"\"",
      "voiceover": "Device type: cuda|cpu|mps (empty string auto-detects in order: CUDA > MPS > CPU)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "depth = 20\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "depth = 20",
      "voiceover": "Number of transformer layers (other architecture params are derived from this)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "max_seq_len = 2048\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "max_seq_len = 2048",
      "voiceover": "Maximum sequence length (context window size)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_iterations = -1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_iterations = -1",
      "voiceover": "Explicit number of training steps (-1 = calculate from other params)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "target_flops = -1.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "target_flops = -1.0",
      "voiceover": "Target FLOPs to determine num_iterations (-1 = disabled, use other method)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "target_param_data_ratio = 20\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "target_param_data_ratio = 20",
      "voiceover": "Data-to-parameter ratio for calculating num_iterations (Chinchilla optimal is ~20)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_batch_size = 32\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "device_batch_size = 32",
      "voiceover": "Batch size per device (tune to avoid OOM)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "total_batch_size = 524288\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "total_batch_size = 524288",
      "voiceover": "Total batch size across all devices in tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "embedding_lr = 0.2\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "embedding_lr = 0.2",
      "voiceover": "Learning rate for embedding layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "unembedding_lr = 0.004\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "unembedding_lr = 0.004",
      "voiceover": "Learning rate for output projection layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "weight_decay = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "weight_decay = 0.0",
      "voiceover": "Weight decay for embedding/unembedding parameters (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "matrix_lr = 0.02\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "matrix_lr = 0.02",
      "voiceover": "Learning rate for linear layer weight matrices (Muon optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "grad_clip = 1.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "grad_clip = 1.0",
      "voiceover": "Gradient clipping threshold (0.0 = disabled)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "warmup_ratio = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "warmup_ratio = 0.0",
      "voiceover": "Fraction of training for learning rate warmup",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "warmdown_ratio = 0.2\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "warmdown_ratio = 0.2",
      "voiceover": "Fraction of training for learning rate warmdown (cosine decay)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "final_lr_frac = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "final_lr_frac = 0.0",
      "voiceover": "Final learning rate as fraction of initial LR (0.0 = decay to zero)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "resume_from_step = -1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "resume_from_step = -1",
      "voiceover": "Resume training from this checkpoint step (-1 = start from scratch)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_every = 250\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "eval_every = 250",
      "voiceover": "Evaluate validation loss every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_tokens = 20*524288\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "eval_tokens = 20*524288",
      "voiceover": "Number of tokens to use for validation evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "core_metric_every = 2000\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "core_metric_every = 2000",
      "voiceover": "Evaluate CORE benchmark metric every N steps (-1 = disabled)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "core_metric_max_per_task = 500\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "core_metric_max_per_task = 500",
      "voiceover": "Maximum examples per task when evaluating CORE metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "sample_every = 2000\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "sample_every = 2000",
      "voiceover": "Generate text samples every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "save_every = -1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "save_every = -1",
      "voiceover": "Save checkpoint every N steps (-1 = only save at end)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_tag = \"\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model_tag = \"\"",
      "voiceover": "Custom tag for checkpoint directory name (empty = use \"d{depth}\")",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]",
      "voiceover": "Extract all configuration variable names for CLI override",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "exec(open(os.path.join('nanochat', 'configurator.py')).read())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "exec(open(os.path.join('nanochat', 'configurator.py')).read())",
      "voiceover": "Execute configurator to allow command-line and config file overrides",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "user_config = {k: globals()[k] for k in config_keys}\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "user_config = {k: globals()[k] for k in config_keys}",
      "voiceover": "Capture final configuration values for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "device_type = autodetect_device_type() if device_type == \"\" else device_type\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "device_type = autodetect_device_type() if device_type == \"\" else device_type",
      "voiceover": "Auto-detect device if not specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initialize distributed data parallel (DDP) and get device info",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "master_process = ddp_rank == 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "master_process = ddp_rank == 0",
      "voiceover": "Master process (rank 0) handles logging, checkpointing, and sampling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Enable mixed precision training on CUDA, no-op otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None",
      "voiceover": "Function to synchronize CUDA operations (no-op on CPU/MPS)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0",
      "voiceover": "Function to get peak memory usage (0 on CPU/MPS)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "use_dummy_wandb = run == \"dummy\" or not master_process\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "use_dummy_wandb = run == \"dummy\" or not master_process",
      "voiceover": "Disable wandb if run is \"dummy\" or not master process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=run, config=user_config)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=run, config=user_config)",
      "voiceover": "Initialize wandb or dummy logger",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "tokenizer = get_tokenizer()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tokenizer = get_tokenizer()",
      "voiceover": "Load the tokenizer for encoding/decoding text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes = get_token_bytes(device=device)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "token_bytes = get_token_bytes(device=device)",
      "voiceover": "Get byte representation of each token for bits-per-byte calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "vocab_size = tokenizer.get_vocab_size()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "vocab_size = tokenizer.get_vocab_size()",
      "voiceover": "Get vocabulary size for model configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Vocab size: {vocab_size:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Vocab size: {vocab_size:,}\")",
      "voiceover": "Print vocab size (only on master process)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "num_layers = depth\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_layers = depth",
      "voiceover": "Number of transformer layers equals depth parameter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_dim = depth * 64\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model_dim = depth * 64",
      "voiceover": "Model dimension (aspect ratio of 64, typically 64-128 for larger models)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_heads = max(1, (model_dim + 127) // 128)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_heads = max(1, (model_dim + 127) // 128)",
      "voiceover": "Number of attention heads (head dimension of 128, using ceiling division)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_kv_heads = num_heads\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_kv_heads = num_heads",
      "voiceover": "Number of key-value heads (equals num_heads, so GQA is disabled)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"num_layers: {num_layers}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"num_layers: {num_layers}\")",
      "voiceover": "Print number of layers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"model_dim: {model_dim}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"model_dim: {model_dim}\")",
      "voiceover": "Print model dimension",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"num_heads: {num_heads}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"num_heads: {num_heads}\")",
      "voiceover": "Print number of attention heads",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"num_kv_heads: {num_kv_heads}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"num_kv_heads: {num_kv_heads}\")",
      "voiceover": "Print number of key-value heads",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "tokens_per_fwdbwd = device_batch_size * max_seq_len\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tokens_per_fwdbwd = device_batch_size * max_seq_len",
      "voiceover": "Tokens processed per forward-backward pass on a single device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size",
      "voiceover": "Total tokens per forward-backward pass across all devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assert total_batch_size % world_tokens_per_fwdbwd == 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "assert total_batch_size % world_tokens_per_fwdbwd == 0",
      "voiceover": "Ensure total batch size is divisible by world tokens per iteration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd",
      "voiceover": "Calculate gradient accumulation steps needed to reach target batch size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")",
      "voiceover": "Print tokens per device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")",
      "voiceover": "Print total tokens per micro-batch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")",
      "voiceover": "Print gradient accumulation steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)",
      "voiceover": "Model configuration dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "with torch.device(\"meta\"):\n    model_config = GPTConfig(**model_config_kwargs)\n    model = GPT(model_config)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "with torch.device(\"meta\"):",
      "voiceover": "Create model on meta device (no memory allocation)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model_config = GPTConfig(**model_config_kwargs)",
      "voiceover": "Create model configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model = GPT(model_config)",
      "voiceover": "Instantiate GPT model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model.to_empty(device=device)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.to_empty(device=device)",
      "voiceover": "Move model to target device with uninitialized parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model.init_weights()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.init_weights()",
      "voiceover": "Initialize model weights",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "base_dir = get_base_dir()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get base directory for checkpoints and data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "output_dirname = model_tag if model_tag else f\"d{depth}\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "output_dirname = model_tag if model_tag else f\"d{depth}\"",
      "voiceover": "Checkpoint directory name (e.g., \"d20\")",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)",
      "voiceover": "Full path to checkpoint directory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "resuming = resume_from_step != -1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "resuming = resume_from_step != -1",
      "voiceover": "Check if resuming from a checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "if resuming:\n    print0(f\"Resuming optimization from step {resume_from_step}\")\n    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, resume_from_step, device, load_optimizer=True, rank=ddp_rank)\n    model.load_state_dict(model_data, strict=True, assign=True)\n    del model_data\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if resuming:",
      "voiceover": "If resuming training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Resuming optimization from step {resume_from_step}\")",
      "voiceover": "Log resume step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, resume_from_step, device, load_optimizer=True, rank=ddp_rank)",
      "voiceover": "Load checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.load_state_dict(model_data, strict=True, assign=True)",
      "voiceover": "Load model weights",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "del model_data",
      "voiceover": "Free memory after loading",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "orig_model = model\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "orig_model = model",
      "voiceover": "Keep reference to uncompiled model for saving and variable-shape operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model = torch.compile(model, dynamic=False)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model = torch.compile(model, dynamic=False)",
      "voiceover": "Compile model for faster execution (dynamic=False since input shapes are fixed)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_params = sum(p.numel() for p in model.parameters())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_params = sum(p.numel() for p in model.parameters())",
      "voiceover": "Count total number of parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Number of parameters: {num_params:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Number of parameters: {num_params:,}\")",
      "voiceover": "Print parameter count",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_flops_per_token = model.estimate_flops()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_flops_per_token = model.estimate_flops()",
      "voiceover": "Estimate FLOPs per token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")",
      "voiceover": "Print FLOPs estimate",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0",
      "voiceover": "Ensure at least one training horizon method is specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "if num_iterations > 0:\n    print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\nelif target_flops > 0:\n    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))\n    print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")\nelif target_param_data_ratio > 0:\n    target_tokens = target_param_data_ratio * num_params\n    num_iterations = target_tokens // total_batch_size\n    print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")\nelse:\n    raise ValueError(\"No training horizon specified\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if num_iterations > 0:",
      "voiceover": "If explicit iteration count is provided",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Using user-provided number of iterations: {num_iterations:,}\")",
      "voiceover": "Use it directly",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "elif target_flops > 0:",
      "voiceover": "If target FLOPs is specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))",
      "voiceover": "Calculate iterations from FLOPs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")",
      "voiceover": "Log calculated iterations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "elif target_param_data_ratio > 0:",
      "voiceover": "If data-to-parameter ratio is specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "target_tokens = target_param_data_ratio * num_params",
      "voiceover": "Calculate target tokens from ratio",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "num_iterations = target_tokens // total_batch_size",
      "voiceover": "Calculate iterations from tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")",
      "voiceover": "Log calculated iterations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "else:",
      "voiceover": "No training horizon specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "raise ValueError(\"No training horizon specified\")",
      "voiceover": "Raise error",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "total_tokens = total_batch_size * num_iterations\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "total_tokens = total_batch_size * num_iterations",
      "voiceover": "Calculate total training tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Total number of training tokens: {total_tokens:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Total number of training tokens: {total_tokens:,}\")",
      "voiceover": "Print total tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Tokens : Params ratio: {total_batch_size * num_iterations / num_params:.2f}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Tokens : Params ratio: {total_batch_size * num_iterations / num_params:.2f}\")",
      "voiceover": "Print data-to-parameter ratio (Chinchilla optimal is ~20)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")",
      "voiceover": "Print total FLOPs estimate",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)",
      "voiceover": "----------------------------------------------------------------------------- Initialize the Optimizer (Muon for Linear layers, AdamW for embedding and lm_head)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)",
      "voiceover": "Create optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "adamw_optimizer, muon_optimizer = optimizers\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "adamw_optimizer, muon_optimizer = optimizers",
      "voiceover": "Unpack AdamW and Muon optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if resuming:\n    for opt, dat in zip(optimizers, optimizer_data):\n        opt.load_state_dict(dat)\n    del optimizer_data\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if resuming:",
      "voiceover": "If resuming from checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for opt, dat in zip(optimizers, optimizer_data):",
      "voiceover": "For each optimizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "opt.load_state_dict(dat)",
      "voiceover": "Load optimizer state",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "del optimizer_data",
      "voiceover": "Free memory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "tokens_dir = os.path.join(base_dir, \"tokenized_data\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tokens_dir = os.path.join(base_dir, \"tokenized_data\")",
      "voiceover": "----------------------------------------------------------------------------- Initialize the DataLoaders for train/val",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tokens_dir = os.path.join(base_dir, \"tokenized_data\")",
      "voiceover": "Directory containing tokenized data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "dataloader_resume_state_dict = None if not resuming else meta_data[\"dataloader_state_dict\"]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "dataloader_resume_state_dict = None if not resuming else meta_data[\"dataloader_state_dict\"]",
      "voiceover": "Get dataloader state if resuming",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "train_loader = tokenizing_distributed_data_loader_with_state(device_batch_size, max_seq_len, split=\"train\", device=device, resume_state_dict=dataloader_resume_state_dict)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "train_loader = tokenizing_distributed_data_loader_with_state(device_batch_size, max_seq_len, split=\"train\", device=device, resume_state_dict=dataloader_resume_state_dict)",
      "voiceover": "Create training dataloader",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "build_val_loader = lambda: tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"val\", device=device)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "build_val_loader = lambda: tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"val\", device=device)",
      "voiceover": "Lambda to create validation dataloader",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "x, y, dataloader_state_dict = next(train_loader)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "x, y, dataloader_state_dict = next(train_loader)",
      "voiceover": "Load first training batch (prefetch)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def get_lr_multiplier(it):\n    warmup_iters = round(warmup_ratio * num_iterations)\n    warmdown_iters = round(warmdown_ratio * num_iterations)\n    if it < warmup_iters:\n        return (it + 1) / warmup_iters\n    elif it <= num_iterations - warmdown_iters:\n        return 1.0\n    else:\n        progress = (num_iterations - it) / warmdown_iters\n        return progress * 1.0 + (1 - progress) * final_lr_frac\n",
      "highlight": true,
      "voiceover": "Computes the learning rate multiplier for a given training iteration. This function implements a three-phase learning rate schedule consisting of warmup, constant, and warmdown phases. During warmup, the learning rate linearly increases from zero to the base learning rate over a specified fraction of total iterations. The constant phase maintains the full learning rate for the middle portion of training. Finally, the warmdown phase linearly decreases the learning rate from the base rate to a final fraction of the base rate over the remaining iterations. This schedule helps stabilize training at the beginning and end while maintaining full learning capacity during the main training phase. The multiplier is applied to the base learning rates configured for each optimizer.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_muon_momentum(it):\n    frac = min(it / 300, 1)\n    momentum = (1 - frac) * 0.85 + frac * 0.95\n    return momentum\n",
      "highlight": true,
      "voiceover": "Computes the momentum value for the Muon optimizer at a given training iteration. This function implements a momentum warmup schedule that gradually increases the momentum coefficient from 0.85 to 0.95 over the first 300 iterations. The momentum value is linearly interpolated based on the iteration count, capped at iteration 300 after which it remains at 0.95. This warmup helps stabilize the Muon optimizer at the beginning of training by starting with lower momentum and gradually increasing it to the target value. The Muon optimizer uses this momentum for its Newton-like updates on the linear layer weight matrices.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if not resuming:\n    step = 0\n    min_val_bpb = float(\"inf\")\n    smooth_train_loss = 0\n    total_training_time = 0\nelse:\n    step = meta_data[\"step\"]\n    loop_state = meta_data[\"loop_state\"]\n    val_bpb = meta_data[\"val_bpb\"]\n    min_val_bpb = loop_state[\"min_val_bpb\"]\n    smooth_train_loss = loop_state[\"smooth_train_loss\"]\n    total_training_time = loop_state[\"total_training_time\"]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if not resuming:",
      "voiceover": "If starting fresh training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "step = 0",
      "voiceover": "Initialize step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "min_val_bpb = float(\"inf\")",
      "voiceover": "Initialize minimum validation bits-per-byte",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "smooth_train_loss = 0",
      "voiceover": "Initialize exponential moving average of training loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "total_training_time = 0",
      "voiceover": "Initialize total training time counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "else:",
      "voiceover": "If resuming from checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "step = meta_data[\"step\"]",
      "voiceover": "Restore step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "loop_state = meta_data[\"loop_state\"]",
      "voiceover": "Get saved loop state",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "val_bpb = meta_data[\"val_bpb\"]",
      "voiceover": "Restore last validation loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "min_val_bpb = loop_state[\"min_val_bpb\"]",
      "voiceover": "Restore minimum validation loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "smooth_train_loss = loop_state[\"smooth_train_loss\"]",
      "voiceover": "Restore smoothed training loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "total_training_time = loop_state[\"total_training_time\"]",
      "voiceover": "Restore total training time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "while True:\n    last_step = step == num_iterations\n    flops_so_far = num_flops_per_token * total_batch_size * step\n\n    if last_step or step % eval_every == 0:\n        model.eval()\n        val_loader = build_val_loader()\n        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n        with autocast_ctx:\n            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n        if val_bpb < min_val_bpb:\n            min_val_bpb = val_bpb\n        wandb_run.log({\n            \"step\": step,\n            \"total_training_flops\": flops_so_far,\n            \"total_training_time\": total_training_time,\n            \"val/bpb\": val_bpb,\n        })\n        model.train()\n\n    results = {}\n    if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\n        model.eval()\n        with autocast_ctx:\n            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n        print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")\n        wandb_run.log({\n            \"step\": step,\n            \"total_training_flops\": flops_so_far,\n            \"core_metric\": results[\"core_metric\"],\n            \"centered_results\": results[\"centered_results\"],\n        })\n        model.train()\n\n    if master_process and (last_step or (step > 0 and step % sample_every == 0)):\n        model.eval()\n        prompts = [\n            \"The capital of France is\",\n            \"The chemical symbol of gold is\",\n            \"If yesterday was Friday, then tomorrow will be\",\n            \"The opposite of hot is\",\n            \"The planets of the solar system are:\",\n            \"My favorite color is\",\n            \"If 5*x + 3 = 13, then x is\",\n        ]\n        engine = Engine(orig_model, tokenizer)\n        for prompt in prompts:\n            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n            with autocast_ctx:\n                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n            print0(tokenizer.decode(sample[0]))\n        model.train()\n\n    if last_step or (step > 0 and step != resume_from_step and save_every > 0 and step % save_every == 0):\n        save_checkpoint(\n            checkpoint_dir,\n            step,\n            orig_model.state_dict(),\n            [opt.state_dict() for opt in optimizers],\n            {\n                \"step\": step,\n                \"val_bpb\": val_bpb,\n                \"model_config\": model_config_kwargs,\n                \"user_config\": user_config,\n                \"device_batch_size\": device_batch_size,\n                \"max_seq_len\": max_seq_len,\n                \"dataloader_state_dict\": dataloader_state_dict,\n                \"loop_state\": {\n                    \"min_val_bpb\": min_val_bpb,\n                    \"smooth_train_loss\": smooth_train_loss,\n                    \"total_training_time\": total_training_time,\n                },\n            },\n            rank=ddp_rank,\n        )\n\n    if last_step:\n        break\n\n    synchronize()\n    t0 = time.time()\n    for micro_step in range(grad_accum_steps):\n        with autocast_ctx:\n            loss = model(x, y)\n        train_loss = loss.detach()\n        loss = loss / grad_accum_steps\n        loss.backward()\n        x, y, dataloader_state_dict = next(train_loader)\n    grad_clip_enabled = grad_clip > 0.0\n    if grad_clip_enabled:\n        grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n        grad_norm = grad_norm_tensor.item()\n    lrm = get_lr_multiplier(step)\n    for opt in optimizers:\n        for group in opt.param_groups:\n            group[\"lr\"] = group[\"initial_lr\"] * lrm\n    muon_momentum = get_muon_momentum(step)\n    for group in muon_optimizer.param_groups:\n        group[\"momentum\"] = muon_momentum\n    for opt in optimizers:\n        opt.step()\n    model.zero_grad(set_to_none=True)\n    synchronize()\n    t1 = time.time()\n    dt = t1 - t0\n\n    ema_beta = 0.9\n    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item()\n    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))\n    pct_done = 100 * step / num_iterations\n    tok_per_sec = int(total_batch_size / dt)\n    flops_per_sec = num_flops_per_token * total_batch_size / dt\n    promised_flops_per_sec_h100 = 989e12 * ddp_world_size\n    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100\n    if step > 10:\n        total_training_time += dt\n    print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\n    print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n    if step % 100 == 0:\n        log_data = {\n            \"step\": step,\n            \"total_training_flops\": flops_so_far,\n            \"total_training_time\": total_training_time,\n            \"train/loss\": debiased_smooth_loss,\n            \"train/lrm\": lrm,\n            \"train/dt\": dt,\n            \"train/tok_per_sec\": tok_per_sec,\n            \"train/mfu\": mfu,\n        }\n        if grad_clip_enabled:\n            log_data[\"train/grad_norm\"] = grad_norm\n        wandb_run.log(log_data)\n\n    step += 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "while True:",
      "voiceover": "----------------------------------------------------------------------------- Training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "while True:",
      "voiceover": "Main training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "last_step = step == num_iterations",
      "voiceover": "Check if this is the final step (loop runs one extra time for final eval/save)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "flops_so_far = num_flops_per_token * total_batch_size * step",
      "voiceover": "Calculate total FLOPs completed so far",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if last_step or step % eval_every == 0:",
      "voiceover": "Evaluate validation loss periodically and at end",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "val_loader = build_val_loader()",
      "voiceover": "Create validation dataloader",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)",
      "voiceover": "Calculate number of evaluation steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)",
      "voiceover": "Evaluate bits-per-byte on validation set",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")",
      "voiceover": "Print validation loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if val_bpb < min_val_bpb:",
      "voiceover": "Update minimum validation loss if improved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "min_val_bpb = val_bpb",
      "voiceover": "Save new minimum",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "wandb_run.log({",
      "voiceover": "Log validation metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "results = {}",
      "voiceover": "Initialize results dictionary for CORE metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):",
      "voiceover": "Evaluate CORE benchmark periodically if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)",
      "voiceover": "Evaluate CORE metric using uncompiled model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")",
      "voiceover": "Print CORE metric score",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "wandb_run.log({",
      "voiceover": "Log CORE metric to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if master_process and (last_step or (step > 0 and step % sample_every == 0)):",
      "voiceover": "Generate text samples periodically on master process only",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "prompts = [",
      "voiceover": "List of prompts to test model's knowledge and reasoning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "engine = Engine(orig_model, tokenizer)",
      "voiceover": "Create inference engine with uncompiled model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for prompt in prompts:",
      "voiceover": "Generate completion for each prompt",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tokens = tokenizer(prompt, prepend=\"<|bos|>\")",
      "voiceover": "Tokenize prompt with BOS token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)",
      "voiceover": "Generate greedy completion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(tokenizer.decode(sample[0]))",
      "voiceover": "Print generated text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if last_step or (step > 0 and step != resume_from_step and save_every > 0 and step % save_every == 0):",
      "voiceover": "Save checkpoint at end or periodically",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "save_checkpoint(",
      "voiceover": "Save model, optimizer, and metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "orig_model.state_dict(),",
      "voiceover": "Model parameters (uncompiled model)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "[opt.state_dict() for opt in optimizers],",
      "voiceover": "Optimizer states for both AdamW and Muon",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "{",
      "voiceover": "Metadata dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "\"loop_state\": {",
      "voiceover": "Training loop state for resumption",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if last_step:",
      "voiceover": "Exit loop after final evaluation and checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "break",
      "voiceover": "Terminate training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "synchronize()",
      "voiceover": "Synchronize all devices before timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "t0 = time.time()",
      "voiceover": "Start timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for micro_step in range(grad_accum_steps):",
      "voiceover": "Gradient accumulation loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "loss = model(x, y)",
      "voiceover": "Forward pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "train_loss = loss.detach()",
      "voiceover": "Detach loss for logging (prevent gradient tracking)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "loss = loss / grad_accum_steps",
      "voiceover": "Normalize loss by accumulation steps (since .backward() sums gradients)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "loss.backward()",
      "voiceover": "Backward pass (accumulate gradients)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "x, y, dataloader_state_dict = next(train_loader)",
      "voiceover": "Prefetch next batch while GPU computes gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "grad_clip_enabled = grad_clip > 0.0",
      "voiceover": "Check if gradient clipping is enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if grad_clip_enabled:",
      "voiceover": "Apply gradient clipping if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)",
      "voiceover": "Clip gradients and get norm",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "grad_norm = grad_norm_tensor.item()",
      "voiceover": "Convert gradient norm to Python float (CPU-GPU sync point)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "lrm = get_lr_multiplier(step)",
      "voiceover": "Get learning rate multiplier for current step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for opt in optimizers:",
      "voiceover": "Update learning rate for all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "group[\"lr\"] = group[\"initial_lr\"] * lrm",
      "voiceover": "Apply learning rate multiplier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "muon_momentum = get_muon_momentum(step)",
      "voiceover": "Get momentum for Muon optimizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for group in muon_optimizer.param_groups:",
      "voiceover": "Update Muon momentum",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "group[\"momentum\"] = muon_momentum",
      "voiceover": "Set momentum value",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "for opt in optimizers:",
      "voiceover": "Step all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "opt.step()",
      "voiceover": "Update parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "model.zero_grad(set_to_none=True)",
      "voiceover": "Clear gradients (set to None for memory efficiency)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "synchronize()",
      "voiceover": "Synchronize all devices after optimization step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "t1 = time.time()",
      "voiceover": "End timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "dt = t1 - t0",
      "voiceover": "Calculate step duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "ema_beta = 0.9",
      "voiceover": "Exponential moving average decay factor for smoothing training loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item()",
      "voiceover": "Update EMA of training loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))",
      "voiceover": "Debias EMA (correct for initialization bias)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "pct_done = 100 * step / num_iterations",
      "voiceover": "Calculate training progress percentage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "tok_per_sec = int(total_batch_size / dt)",
      "voiceover": "Calculate tokens processed per second",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "flops_per_sec = num_flops_per_token * total_batch_size / dt",
      "voiceover": "Calculate FLOPs per second",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "promised_flops_per_sec_h100 = 989e12 * ddp_world_size",
      "voiceover": "Theoretical peak FLOPs for H100 GPUs (bfloat16, no sparsity)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "mfu = 100 * flops_per_sec / promised_flops_per_sec_h100",
      "voiceover": "Calculate model FLOPs utilization (MFU) percentage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if step > 10:",
      "voiceover": "Skip first 10 steps for timing (warmup/compilation)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "total_training_time += dt",
      "voiceover": "Accumulate total training time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"",
      "voiceover": "Format gradient norm string if clipping enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")",
      "voiceover": "Print training progress",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if step % 100 == 0:",
      "voiceover": "Log to wandb every 100 steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "log_data = {",
      "voiceover": "Training metrics dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "if grad_clip_enabled:",
      "voiceover": "Add gradient norm if clipping is enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "log_data[\"train/grad_norm\"] = grad_norm",
      "voiceover": "Include gradient norm in logs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "wandb_run.log(log_data)",
      "voiceover": "Log metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "step += 1",
      "voiceover": "Increment step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")",
      "voiceover": "Print peak GPU memory usage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Total training time: {total_training_time/60:.2f}m\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Total training time: {total_training_time/60:.2f}m\")",
      "voiceover": "Print total training time in minutes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "print0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")",
      "voiceover": "Print best validation loss achieved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Base model training\", data=[\n    user_config,\n    {\n        \"Number of parameters\": num_params,\n        \"Number of FLOPs per token\": f\"{num_flops_per_token:e}\",\n        \"Calculated number of iterations\": num_iterations,\n        \"Number of training tokens\": total_tokens,\n        \"Tokens : Params ratio\": total_batch_size * num_iterations / num_params,\n        \"DDP world size\": ddp_world_size,\n        \"warmup_ratio\": warmup_ratio,\n        \"warmdown_ratio\": warmdown_ratio,\n        \"final_lr_frac\": final_lr_frac,\n    },\n    {\n        \"Minimum validation bpb\": min_val_bpb,\n        \"Final validation bpb\": val_bpb,\n        \"CORE metric estimate\": results.get(\"core_metric\", None),\n        \"MFU %\": f\"{mfu:.2f}%\",\n        \"Total training flops\": f\"{flops_so_far:e}\",\n        \"Total training time\": f\"{total_training_time/60:.2f}m\",\n        \"Peak memory usage\": f\"{get_max_memory() / 1024 / 1024:.2f}MiB\",\n    }\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "get_report().log(section=\"Base model training\", data=[",
      "voiceover": "Log training summary to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "user_config,",
      "voiceover": "User configuration (CLI arguments)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "{",
      "voiceover": "Training setup statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "{",
      "voiceover": "Training outcome statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "wandb_run.finish()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "wandb_run.finish()",
      "voiceover": "Finalize wandb run and upload any remaining logs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "compute_cleanup()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/base_train.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed computing resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/chat_cli.py"
    },
    {
      "type": "writeText",
      "content": "import argparse\nimport torch\nfrom nanochat.common import compute_init, autodetect_device_type\nfrom contextlib import nullcontext\nfrom nanochat.engine import Engine\nfrom nanochat.checkpoint_manager import load_model\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "import argparse",
      "voiceover": "Command-line argument parsing for chat configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "from nanochat.common import compute_init, autodetect_device_type",
      "voiceover": "Utilities for device initialization and auto-detection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as placeholder when autocast is disabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for efficient text generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Utility to load trained model checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "parser = argparse.ArgumentParser(description='Chat with the model')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser = argparse.ArgumentParser(description='Chat with the model')",
      "voiceover": "Create argument parser for CLI configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-i', '--source', type=str, default=\"sft\", help=\"Source of the model: sft|mid|rl\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-i', '--source', type=str, default=\"sft\", help=\"Source of the model: sft|mid|rl\")",
      "voiceover": "Model source: supervised fine-tuning, mid-training, or reinforcement learning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')",
      "voiceover": "Optional model tag identifier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')",
      "voiceover": "Optional checkpoint step number",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-p', '--prompt', type=str, default='', help='Prompt the model, get a single response back')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-p', '--prompt', type=str, default='', help='Prompt the model, get a single response back')",
      "voiceover": "Single-shot prompt mode (non-interactive)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-t', '--temperature', type=float, default=0.6, help='Temperature for generation')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-t', '--temperature', type=float, default=0.6, help='Temperature for generation')",
      "voiceover": "Sampling temperature (higher = more random)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-k', '--top-k', type=int, default=50, help='Top-k sampling parameter')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-k', '--top-k', type=int, default=50, help='Top-k sampling parameter')",
      "voiceover": "Limit sampling to top-k most likely tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')",
      "voiceover": "Device selection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])",
      "voiceover": "Precision for model inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "args = parser.parse_args()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parse command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type",
      "voiceover": "Auto-detect device if not specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initialize compute environment and get device info",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16",
      "voiceover": "Convert dtype string to PyTorch dtype",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Enable mixed precision on CUDA, no-op otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)",
      "voiceover": "Load model checkpoint and tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "bos = tokenizer.get_bos_token_id()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "bos = tokenizer.get_bos_token_id()",
      "voiceover": "Get beginning-of-sequence token ID",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "user_start, user_end = tokenizer.encode_special(\"<|user_start|>\"), tokenizer.encode_special(\"<|user_end|>\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "user_start, user_end = tokenizer.encode_special(\"<|user_start|>\"), tokenizer.encode_special(\"<|user_end|>\")",
      "voiceover": "Get special tokens marking user message boundaries",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assistant_start, assistant_end = tokenizer.encode_special(\"<|assistant_start|>\"), tokenizer.encode_special(\"<|assistant_end|>\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "assistant_start, assistant_end = tokenizer.encode_special(\"<|assistant_start|>\"), tokenizer.encode_special(\"<|assistant_end|>\")",
      "voiceover": "Get special tokens marking assistant message boundaries",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "engine = Engine(model, tokenizer)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create inference engine for efficient token generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "print(\"\\nNanoChat Interactive Mode\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"\\nNanoChat Interactive Mode\")",
      "voiceover": "Print welcome header",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(\"-\" * 50)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"-\" * 50)",
      "voiceover": "Print separator line",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(\"Type 'quit' or 'exit' to end the conversation\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"Type 'quit' or 'exit' to end the conversation\")",
      "voiceover": "Print exit instructions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(\"Type 'clear' to start a new conversation\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"Type 'clear' to start a new conversation\")",
      "voiceover": "Print clear instructions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(\"-\" * 50)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"-\" * 50)",
      "voiceover": "Print separator line",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "conversation_tokens = [bos]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens = [bos]",
      "voiceover": "Initialize conversation history with beginning-of-sequence token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "while True:\n    if args.prompt:\n        user_input = args.prompt\n    else:\n        try:\n            user_input = input(\"\\nUser: \").strip()\n        except (EOFError, KeyboardInterrupt):\n            print(\"\\nGoodbye!\")\n            break\n\n    if user_input.lower() in ['quit', 'exit']:\n        print(\"Goodbye!\")\n        break\n\n    if user_input.lower() == 'clear':\n        conversation_tokens = [bos]\n        print(\"Conversation cleared.\")\n        continue\n\n    if not user_input:\n        continue\n\n    conversation_tokens.append(user_start)\n    conversation_tokens.extend(tokenizer.encode(user_input))\n    conversation_tokens.append(user_end)\n\n    conversation_tokens.append(assistant_start)\n    generate_kwargs = {\n        \"num_samples\": 1,\n        \"max_tokens\": 256,\n        \"temperature\": args.temperature,\n        \"top_k\": args.top_k,\n    }\n    response_tokens = []\n    print(\"\\nAssistant: \", end=\"\", flush=True)\n    with autocast_ctx:\n        for token_column, token_masks in engine.generate(conversation_tokens, **generate_kwargs):\n            token = token_column[0]\n            response_tokens.append(token)\n            token_text = tokenizer.decode([token])\n            print(token_text, end=\"\", flush=True)\n    print()\n    if response_tokens[-1] != assistant_end:\n        response_tokens.append(assistant_end)\n    conversation_tokens.extend(response_tokens)\n\n    if args.prompt:\n        break\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "while True:",
      "voiceover": "Main chat loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if args.prompt:",
      "voiceover": "If single-shot prompt mode is enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "user_input = args.prompt",
      "voiceover": "Use prompt from command-line argument",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "else:",
      "voiceover": "Interactive mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "try:",
      "voiceover": "Attempt to get user input",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "user_input = input(\"\\nUser: \").strip()",
      "voiceover": "Read user input from console and strip whitespace",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "except (EOFError, KeyboardInterrupt):",
      "voiceover": "Handle Ctrl+D or Ctrl+C",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"\\nGoodbye!\")",
      "voiceover": "Print goodbye message",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "break",
      "voiceover": "Exit chat loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if user_input.lower() in ['quit', 'exit']:",
      "voiceover": "Check for exit commands",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"Goodbye!\")",
      "voiceover": "Print goodbye message",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "break",
      "voiceover": "Exit chat loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if user_input.lower() == 'clear':",
      "voiceover": "Check for clear command",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens = [bos]",
      "voiceover": "Reset conversation to initial state",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"Conversation cleared.\")",
      "voiceover": "Confirm conversation reset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "continue",
      "voiceover": "Skip to next iteration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if not user_input:",
      "voiceover": "If input is empty",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "continue",
      "voiceover": "Skip to next iteration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens.append(user_start)",
      "voiceover": "Add user message start token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens.extend(tokenizer.encode(user_input))",
      "voiceover": "Encode and add user message text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens.append(user_end)",
      "voiceover": "Add user message end token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens.append(assistant_start)",
      "voiceover": "Add assistant message start token to prompt generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "generate_kwargs = {",
      "voiceover": "Generation parameters dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "\"num_samples\": 1,",
      "voiceover": "Generate single response",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "\"max_tokens\": 256,",
      "voiceover": "Maximum tokens to generate",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "\"temperature\": args.temperature,",
      "voiceover": "Sampling temperature from arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "\"top_k\": args.top_k,",
      "voiceover": "Top-k sampling parameter from arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "response_tokens = []",
      "voiceover": "Initialize list to collect generated tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(\"\\nAssistant: \", end=\"\", flush=True)",
      "voiceover": "Print assistant prefix without newline",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "for token_column, token_masks in engine.generate(conversation_tokens, **generate_kwargs):",
      "voiceover": "Generate tokens one at a time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "token = token_column[0]",
      "voiceover": "Extract token from batch dimension (num_samples=1)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "response_tokens.append(token)",
      "voiceover": "Add token to response",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "token_text = tokenizer.decode([token])",
      "voiceover": "Decode token to text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print(token_text, end=\"\", flush=True)",
      "voiceover": "Print token immediately for streaming effect",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "print()",
      "voiceover": "Print newline after generation completes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if response_tokens[-1] != assistant_end:",
      "voiceover": "If generation ended without assistant end token (e.g., hit max_tokens)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "response_tokens.append(assistant_end)",
      "voiceover": "Manually append assistant end token to maintain proper format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "conversation_tokens.extend(response_tokens)",
      "voiceover": "Add assistant response to conversation history",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "if args.prompt:",
      "voiceover": "If in single-shot prompt mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_cli.py",
      "find": "break",
      "voiceover": "Exit after single response",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/chat_eval.py"
    },
    {
      "type": "writeText",
      "content": "import argparse\nfrom functools import partial\nfrom contextlib import nullcontext\n\nimport torch\nimport torch.distributed as dist\n\nfrom nanochat.common import compute_init, compute_cleanup, get_dist_info, print0, autodetect_device_type\nfrom nanochat.checkpoint_manager import load_model\nfrom nanochat.engine import Engine\n\nfrom tasks.humaneval import HumanEval\nfrom tasks.mmlu import MMLU\nfrom tasks.arc import ARC\nfrom tasks.gsm8k import GSM8K\nfrom tasks.spellingbee import SpellingBee\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "import argparse",
      "voiceover": "Command-line argument parsing for evaluation configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from functools import partial",
      "voiceover": "Partial function application for creating task constructors with preset arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as placeholder when autocast is disabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "import torch.distributed as dist",
      "voiceover": "PyTorch distributed training utilities for multi-GPU evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, get_dist_info, print0, autodetect_device_type",
      "voiceover": "Common utilities for distributed computing and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Utility to load trained model checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for efficient text generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from tasks.humaneval import HumanEval",
      "voiceover": "HumanEval code generation benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from tasks.mmlu import MMLU",
      "voiceover": "MMLU (Massive Multitask Language Understanding) benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from tasks.arc import ARC",
      "voiceover": "ARC (AI2 Reasoning Challenge) benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from tasks.gsm8k import GSM8K",
      "voiceover": "GSM8K grade school math benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from tasks.spellingbee import SpellingBee",
      "voiceover": "Spelling bee benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def run_generative_eval(task_object, tokenizer, model, engine, num_samples, max_new_tokens, temperature, top_k, max_problems=None):\n\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    device = model.get_device()\n\n    num_problems = len(task_object) if max_problems is None else min(len(task_object), max_problems)\n\n    num_passed, total = 0, 0\n    for i in range(ddp_rank, num_problems, ddp_world_size):\n        conversation = task_object[i]\n\n        encoded_prompt = tokenizer.render_for_completion(conversation)\n        results, _ = engine.generate_batch(\n            encoded_prompt,\n            num_samples=num_samples,\n            max_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n        )\n        prefix_length = len(encoded_prompt)\n        completions = [tokenizer.decode(result_tokens[prefix_length:]) for result_tokens in results]\n        outcomes = [task_object.evaluate(conversation, completion) for completion in completions]\n        passed = any(outcomes)\n\n        total += 1\n        num_passed += int(passed)\n\n        print(f\"\\r\\033[KRank {ddp_rank} | {num_passed}/{total} ({100*num_passed/total:.2f}%)\", end='', flush=True)\n\n    print()\n\n    if ddp:\n        num_passed_tensor = torch.tensor([num_passed], dtype=torch.long, device=device)\n        total_tensor = torch.tensor([total], dtype=torch.long, device=device)\n        dist.all_reduce(num_passed_tensor, op=dist.ReduceOp.SUM)\n        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n        num_passed = num_passed_tensor.item()\n        total = total_tensor.item()\n\n    print0(\"=\" * 50)\n    print0(f\"Final: {num_passed}/{total} ({100*num_passed/total:.2f}%)\")\n\n    return num_passed/total\n",
      "highlight": true,
      "voiceover": "Evaluates the model on generative tasks where the model must generate free-form text responses. This function processes problems one at a time across distributed ranks, generating multiple candidate completions for each problem using sampling. For each problem, it tokenizes the prompt, generates completions using the inference engine with specified sampling parameters, decodes the results, and evaluates whether any of the generated completions satisfy the task's success criteria using a pass-at-k approach. The evaluation is distributed across multiple GPUs if available, with each rank processing a subset of problems in a round-robin fashion. Results are aggregated across all ranks at the end to compute the overall accuracy. This approach is suitable for open-ended tasks like code generation or math problem solving where exact string matching is not required.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def run_categorical_eval(task_object, tokenizer, model, batch_size, max_problems=None):\n\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    device = model.get_device()\n    bos = tokenizer.get_bos_token_id()\n\n    num_problems = len(task_object) if max_problems is None else min(len(task_object), max_problems)\n    ceil_div = lambda x, y: -(-x // y)\n    num_batches = ceil_div(num_problems, batch_size)\n\n    letter_to_id_cache = {}\n    num_passed, total = 0, 0\n    for i in range(ddp_rank, num_batches, ddp_world_size):\n        i0, i1 = i * batch_size, min((i + 1) * batch_size, num_problems)\n\n        conversations = [task_object[ii] for ii in range(i0, i1)]\n        prompt_ids = [tokenizer.render_for_completion(conversation) for conversation in conversations]\n        max_length = max(len(ids) for ids in prompt_ids)\n        answer_time_positions = [len(ids) - 1 for ids in prompt_ids]\n        padded_prompt_ids = [ids + [bos] * (max_length - len(ids)) for ids in prompt_ids]\n        prompt_ids = torch.tensor(padded_prompt_ids, dtype=torch.long, device=device)\n\n        with torch.no_grad():\n            logits = model(prompt_ids)\n\n        for idx, conversation in enumerate(conversations):\n            letters = conversation['letters']\n            letter_ids = []\n            for letter in letters:\n                if not letter in letter_to_id_cache:\n                    encoded_letter = tokenizer.encode(letter)\n                    assert len(encoded_letter) == 1, \"Each letter must be a single token\"\n                    letter_to_id_cache[letter] = encoded_letter[0]\n                letter_ids.append(letter_to_id_cache[letter])\n            answer_pos = answer_time_positions[idx]\n            focus_logits = logits[idx, answer_pos, letter_ids]\n            argmax_letter_id = focus_logits.argmax(dim=-1).item()\n            predicted_letter = letters[argmax_letter_id]\n            outcome = task_object.evaluate(conversation, predicted_letter)\n            num_passed += int(outcome)\n            total += 1\n\n    if ddp:\n        num_passed_tensor = torch.tensor([num_passed], dtype=torch.long, device=device)\n        total_tensor = torch.tensor([total], dtype=torch.long, device=device)\n        dist.all_reduce(num_passed_tensor, op=dist.ReduceOp.SUM)\n        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)\n        num_passed = num_passed_tensor.item()\n        total = total_tensor.item()\n\n    average = num_passed/total\n    print0(f\"Final: {num_passed}/{total} ({100*average:.2f}%)\")\n    return average\n",
      "highlight": true,
      "voiceover": "Evaluates the model on categorical tasks where the answer is one of a fixed set of choices, typically multiple-choice questions. This function is more efficient than generative evaluation because it doesn't require sampling or text generation. Instead, it processes problems in batches, runs a single forward pass through the model to get logits, and examines only the logits corresponding to the available answer choices at the answer position. For each problem, it identifies which answer choice has the highest logit value and checks if it matches the correct answer. The evaluation leverages batching for efficiency by padding prompts to the same length and processing multiple problems simultaneously. This approach is ideal for benchmarks like MMLU and ARC where answers are single letters representing multiple-choice options, making evaluation both faster and more reliable than generating and parsing free-form text.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "bos = tokenizer.get_bos_token_id()",
      "voiceover": "use BOS as pad token is ok, these positions are ignored",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "letter_to_id_cache = {}",
      "voiceover": "many letters will repeat often, let's save the tokenizer some work",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "prompt_ids = [tokenizer.render_for_completion(conversation) for conversation in conversations]",
      "voiceover": "TODO: remake the way this works",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "answer_time_positions = [len(ids) - 1 for ids in prompt_ids]",
      "voiceover": "where the last token is (and the predicted answer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "logits = model(prompt_ids)",
      "voiceover": "(B, T, V)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def run_chat_eval(task_name, model, tokenizer, engine,\n                   batch_size=1, num_samples=1, max_new_tokens=512, temperature=0.0, top_k=50,\n                   max_problems=None):\n    task_module = {\n        'HumanEval': HumanEval,\n        'MMLU': partial(MMLU, subset=\"all\", split=\"test\"),\n        'ARC-Easy': partial(ARC, subset=\"ARC-Easy\", split=\"test\"),\n        'ARC-Challenge': partial(ARC, subset=\"ARC-Challenge\", split=\"test\"),\n        'GSM8K': partial(GSM8K, subset=\"main\", split=\"test\"),\n        'SpellingBee': partial(SpellingBee, size=256, split=\"test\"),\n    }[task_name]\n    task_object = task_module()\n    if task_object.eval_type == 'generative':\n        acc = run_generative_eval(task_object, tokenizer, model, engine, num_samples, max_new_tokens, temperature, top_k, max_problems=max_problems)\n    elif task_object.eval_type == 'categorical':\n        acc = run_categorical_eval(task_object, tokenizer, model, batch_size, max_problems=max_problems)\n    else:\n        raise ValueError(f\"Unsupported task evaluation type: {task_object.eval_type}\")\n    return acc\n",
      "highlight": true,
      "voiceover": "Orchestrates the evaluation of a chat model on a specified benchmark task. This function serves as the main entry point for running evaluations, handling task selection and routing to the appropriate evaluation method. It creates the task object based on the task name using a dictionary mapping task names to their corresponding classes and configuration parameters. The function then determines whether the task requires generative evaluation (for open-ended tasks like code generation) or categorical evaluation (for multiple-choice tasks) based on the task object's eval_type attribute, and dispatches to the appropriate evaluation function with the provided parameters. This design allows for a unified interface to evaluate different types of tasks while using specialized evaluation logic optimized for each task type. The function returns the accuracy score for the evaluated task.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--source', type=str, required=True, help=\"Source of the model: sft|mid|rl\")\n    parser.add_argument('-a', '--task-name', type=str, default=None, help=\"Task name. Default = all tasks. Use | to split multiple tasks.\")\n    parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])\n    parser.add_argument('-t', '--temperature', type=float, default=0.0)\n    parser.add_argument('-m', '--max-new-tokens', type=int, default=512)\n    parser.add_argument('-n', '--num-samples', type=int, default=1)\n    parser.add_argument('-k', '--top-k', type=int, default=50)\n    parser.add_argument('-b', '--batch-size', type=int, default=8, help='Batch size for categorical evaluation')\n    parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')\n    parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')\n    parser.add_argument('-x', '--max-problems', type=int, default=None, help='Max problems to evaluate')\n    parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')\n    args = parser.parse_args()\n\n    device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n    ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16\n    autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n\n    model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n    engine = Engine(model, tokenizer)\n\n    all_tasks = ['ARC-Easy', 'ARC-Challenge', 'MMLU', 'GSM8K', 'HumanEval', 'SpellingBee']\n    baseline_accuracies = {\n        'ARC-Easy': 0.25,\n        'ARC-Challenge': 0.25,\n        'MMLU': 0.25,\n        'GSM8K': 0.0,\n        'HumanEval': 0.0,\n        'SpellingBee': 0.0,\n    }\n    task_names = all_tasks if args.task_name is None else args.task_name.split('|')\n\n    results = {}\n    for task_name in task_names:\n        with autocast_ctx:\n            acc = run_chat_eval(\n                task_name,\n                model, tokenizer, engine,\n                batch_size=args.batch_size,\n                num_samples=args.num_samples,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                top_k=args.top_k,\n                max_problems=args.max_problems,\n            )\n            results[task_name] = acc\n            print0(f\"{task_name} accuracy: {100 * acc:.2f}%\")\n\n    from nanochat.report import get_report\n    all_tasks_were_evaluated = all(task_name in results for task_name in all_tasks)\n    chatcore_metric_dict = {}\n    if all_tasks_were_evaluated:\n        centered_mean = 0\n        for task_name, acc in results.items():\n            baseline_acc = baseline_accuracies.get(task_name, 0.0)\n            centered_acc = (acc - baseline_acc) / (1.0 - baseline_acc)\n            centered_mean += centered_acc\n        chatcore_metric = centered_mean / len(results)\n        chatcore_metric_dict = {\"ChatCORE metric\": chatcore_metric}\n    get_report().log(section=\"Chat evaluation \" + args.source, data=[\n        vars(args),\n        results,\n        chatcore_metric_dict,\n    ])\n\n    compute_cleanup()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "-----------------------------------------------------------------------------",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Main execution block when script is run directly",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser = argparse.ArgumentParser()",
      "voiceover": "Create argument parser for CLI configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-i', '--source', type=str, required=True, help=\"Source of the model: sft|mid|rl\")",
      "voiceover": "Model source: supervised fine-tuning, mid-training, or reinforcement learning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-a', '--task-name', type=str, default=None, help=\"Task name. Default = all tasks. Use | to split multiple tasks.\")",
      "voiceover": "Task selection (pipe-separated for multiple tasks)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])",
      "voiceover": "Precision for model inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-t', '--temperature', type=float, default=0.0)",
      "voiceover": "Sampling temperature (0.0 = greedy/deterministic)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-m', '--max-new-tokens', type=int, default=512)",
      "voiceover": "Maximum tokens to generate per completion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-n', '--num-samples', type=int, default=1)",
      "voiceover": "Number of completions to generate per problem (for pass@k)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-k', '--top-k', type=int, default=50)",
      "voiceover": "Top-k sampling parameter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-b', '--batch-size', type=int, default=8, help='Batch size for categorical evaluation')",
      "voiceover": "Batch size for categorical tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')",
      "voiceover": "Optional model tag identifier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')",
      "voiceover": "Optional checkpoint step number",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('-x', '--max-problems', type=int, default=None, help='Max problems to evaluate')",
      "voiceover": "Limit number of problems for quick testing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')",
      "voiceover": "Device selection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parse command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type",
      "voiceover": "Auto-detect device if not specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initialize distributed environment and get device info",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16",
      "voiceover": "Convert dtype string to PyTorch dtype",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Enable mixed precision on CUDA, no-op otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)",
      "voiceover": "Load model checkpoint and tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create inference engine for text generation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "all_tasks = ['ARC-Easy', 'ARC-Challenge', 'MMLU', 'GSM8K', 'HumanEval', 'SpellingBee']",
      "voiceover": "List of all available benchmark tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "baseline_accuracies = {",
      "voiceover": "Random baseline accuracies for each task (used for centered metric calculation)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'ARC-Easy': 0.25,",
      "voiceover": "Multiple choice 1 of 4 => 25%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'ARC-Challenge': 0.25,",
      "voiceover": "Multiple choice 1 of 4 => 25%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'MMLU': 0.25,",
      "voiceover": "Multiple choice 1 of 4 => 25%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'GSM8K': 0.0,",
      "voiceover": "Open-ended => 0%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'HumanEval': 0.0,",
      "voiceover": "Open-ended => 0%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "'SpellingBee': 0.0,",
      "voiceover": "Open-ended => 0%",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "task_names = all_tasks if args.task_name is None else args.task_name.split('|')",
      "voiceover": "Determine which tasks to evaluate (all or user-specified)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "results = {}",
      "voiceover": "Dictionary to store accuracy results for each task",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "for task_name in task_names:",
      "voiceover": "Evaluate each task sequentially",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision if enabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "acc = run_chat_eval(",
      "voiceover": "Run evaluation for current task",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "results[task_name] = acc",
      "voiceover": "Store accuracy result",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "print0(f\"{task_name} accuracy: {100 * acc:.2f}%\")",
      "voiceover": "Print task accuracy",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "all_tasks_were_evaluated = all(task_name in results for task_name in all_tasks)",
      "voiceover": "Check if all tasks were evaluated",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "chatcore_metric_dict = {}",
      "voiceover": "Initialize ChatCORE metric dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "if all_tasks_were_evaluated:",
      "voiceover": "Calculate ChatCORE metric only if all tasks were evaluated",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "centered_mean = 0",
      "voiceover": "Initialize sum of centered accuracies",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "for task_name, acc in results.items():",
      "voiceover": "For each task result",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "baseline_acc = baseline_accuracies.get(task_name, 0.0)",
      "voiceover": "Get random baseline for this task",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "centered_acc = (acc - baseline_acc) / (1.0 - baseline_acc)",
      "voiceover": "Center accuracy relative to baseline (0 = baseline, 1 = perfect)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "centered_mean += centered_acc",
      "voiceover": "Accumulate centered accuracies",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "chatcore_metric = centered_mean / len(results)",
      "voiceover": "Calculate mean centered accuracy (ChatCORE metric)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "chatcore_metric_dict = {\"ChatCORE metric\": chatcore_metric}",
      "voiceover": "Store ChatCORE metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "get_report().log(section=\"Chat evaluation \" + args.source, data=[",
      "voiceover": "Log evaluation results to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "vars(args),",
      "voiceover": "Command-line arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "results,",
      "voiceover": "Task accuracy results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "chatcore_metric_dict,",
      "voiceover": "ChatCORE metric if available",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_eval.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed computing resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/chat_rl.py"
    },
    {
      "type": "writeText",
      "content": "import os\nimport itertools\nimport re\nimport wandb\nimport torch\nimport torch.distributed as dist\n\nfrom nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, DummyWandb\nfrom nanochat.checkpoint_manager import save_checkpoint, load_model\nfrom nanochat.engine import Engine\nfrom tasks.gsm8k import GSM8K\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import os",
      "voiceover": "Operating system interface for file paths and environment variables",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import itertools",
      "voiceover": "Iterator tools for cycling through training examples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import re",
      "voiceover": "Regular expressions (imported but not used in this file)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import wandb",
      "voiceover": "Weights & Biases for experiment tracking and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "import torch.distributed as dist",
      "voiceover": "PyTorch distributed training utilities for multi-GPU RL",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, print0, get_base_dir, DummyWandb",
      "voiceover": "Common utilities for distributed computing and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "from nanochat.checkpoint_manager import save_checkpoint, load_model",
      "voiceover": "Checkpoint saving and loading utilities",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for sampling rollouts",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "from tasks.gsm8k import GSM8K",
      "voiceover": "GSM8K grade school math benchmark task",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "run = \"dummy\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "run = \"dummy\"",
      "voiceover": "Wandb run name (\"dummy\" disables wandb logging)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "source = \"sft\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "source = \"sft\"",
      "voiceover": "Model source to load: mid (mid-training) or sft (supervised fine-tuned)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "dtype = \"bfloat16\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dtype = \"bfloat16\"",
      "voiceover": "Precision for model training and inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_batch_size = 8\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "device_batch_size = 8",
      "voiceover": "Maximum batch size per forward pass to avoid OOM",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "examples_per_step = 16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "examples_per_step = 16",
      "voiceover": "Total training examples per optimization step across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_samples = 16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_samples = 16",
      "voiceover": "Number of rollout samples to generate per example for variance reduction",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "max_new_tokens = 256\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "max_new_tokens = 256",
      "voiceover": "Maximum tokens to generate per completion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "temperature = 1.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "temperature = 1.0",
      "voiceover": "Sampling temperature for rollout generation (1.0 = standard sampling)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "top_k = 50\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "top_k = 50",
      "voiceover": "Top-k sampling parameter (limit sampling to top-k most likely tokens)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "unembedding_lr = 0.004\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "unembedding_lr = 0.004",
      "voiceover": "Learning rate for output projection layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "embedding_lr = 0.2\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "embedding_lr = 0.2",
      "voiceover": "Learning rate for embedding layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "matrix_lr = 0.02\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "matrix_lr = 0.02",
      "voiceover": "Learning rate for linear layer weight matrices (Muon optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "weight_decay = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "weight_decay = 0.0",
      "voiceover": "Weight decay for embedding/unembedding parameters (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "init_lr_frac = 0.05\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "init_lr_frac = 0.05",
      "voiceover": "Initial learning rate as fraction of base LR (warmup from lower LR)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_epochs = 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_epochs = 1",
      "voiceover": "Number of epochs to train on GSM8K dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "save_every = 60\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "save_every = 60",
      "voiceover": "Save checkpoint every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_every = 60\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "eval_every = 60",
      "voiceover": "Evaluate pass@k on validation set every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_examples = 400\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "eval_examples = 400",
      "voiceover": "Number of validation examples to use for pass@k evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]",
      "voiceover": "Extract all configuration variable names for CLI override",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "exec(open(os.path.join('nanochat', 'configurator.py')).read())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "exec(open(os.path.join('nanochat', 'configurator.py')).read())",
      "voiceover": "Execute configurator to allow command-line and config file overrides",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "user_config = {k: globals()[k] for k in config_keys}\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "user_config = {k: globals()[k] for k in config_keys}",
      "voiceover": "Capture final configuration values for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()",
      "voiceover": "Initialize distributed environment and get device info",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "master_process = ddp_rank == 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "master_process = ddp_rank == 0",
      "voiceover": "Master process (rank 0) handles logging and checkpointing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "dtype = torch.float32 if dtype == 'float32' else torch.bfloat16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dtype = torch.float32 if dtype == 'float32' else torch.bfloat16",
      "voiceover": "Convert dtype string to PyTorch dtype",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "autocast_ctx = torch.amp.autocast(device_type=\"cuda\", dtype=dtype)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=\"cuda\", dtype=dtype)",
      "voiceover": "Enable mixed precision training context",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "use_dummy_wandb = run == \"dummy\" or not master_process\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "use_dummy_wandb = run == \"dummy\" or not master_process",
      "voiceover": "Disable wandb if run is \"dummy\" or not master process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-rl\", name=run, config=user_config)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-rl\", name=run, config=user_config)",
      "voiceover": "Initialize wandb or dummy logger",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "model, tokenizer, meta = load_model(source, device, phase=\"eval\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model, tokenizer, meta = load_model(source, device, phase=\"eval\")",
      "voiceover": "Load model checkpoint and tokenizer (phase=\"eval\" to avoid unnecessary setup)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "engine = Engine(model, tokenizer)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create inference engine for sampling rollouts",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "train_task = GSM8K(subset=\"main\", split=\"train\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "train_task = GSM8K(subset=\"main\", split=\"train\")",
      "voiceover": "Load GSM8K training dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "val_task = GSM8K(subset=\"main\", split=\"test\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "val_task = GSM8K(subset=\"main\", split=\"test\")",
      "voiceover": "Load GSM8K validation dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_steps = (len(train_task) // examples_per_step) * num_epochs\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_steps = (len(train_task) // examples_per_step) * num_epochs",
      "voiceover": "Calculate total number of optimization steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Calculated number of steps: {num_steps}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Calculated number of steps: {num_steps}\")",
      "voiceover": "Print total steps (only on master process)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@torch.no_grad()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "@torch.no_grad()",
      "voiceover": "Disable gradient computation for rollout generation (not training yet)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "def get_batch():\n    rank_indices = range(ddp_rank, len(train_task), ddp_world_size)\n    for example_idx in itertools.cycle(rank_indices):\n\n        conversation = train_task[example_idx]\n\n        tokens = tokenizer.render_for_completion(conversation)\n        prefix_length = len(tokens)\n\n        model.eval()\n        generated_token_sequences = []\n        masks = []\n        num_sampling_steps = num_samples // device_batch_size\n        for sampling_step in range(num_sampling_steps):\n            seed = hash((step, example_idx, sampling_step)) & 0x7FFFFFFF\n            with autocast_ctx:\n                generated_token_sequences_batch, masks_batch = engine.generate_batch(\n                    tokens,\n                    num_samples=device_batch_size,\n                    max_tokens=max_new_tokens,\n                    temperature=temperature,\n                    top_k=top_k,\n                    seed=seed,\n                )\n            generated_token_sequences.extend(generated_token_sequences_batch)\n            masks.extend(masks_batch)\n\n        rewards = []\n        for sample_tokens in generated_token_sequences:\n            generated_tokens = sample_tokens[prefix_length:]\n            generated_text = tokenizer.decode(generated_tokens)\n            reward = train_task.reward(conversation, generated_text)\n            rewards.append(reward)\n\n        max_length = max(len(seq) for seq in generated_token_sequences)\n        padded_generated_token_sequences = [seq + [assistant_end] * (max_length - len(seq)) for seq in generated_token_sequences]\n        padded_masks = [mask + [0] * (max_length - len(mask)) for mask in masks]\n        ids = torch.tensor(padded_generated_token_sequences, dtype=torch.long, device=device)\n        mask_ids = torch.tensor(padded_masks, dtype=torch.long, device=device)\n        inputs = ids[:, :-1]\n        targets = ids[:, 1:].clone()\n        targets[mask_ids[:, 1:] == 0] = -1\n        rewards = torch.tensor(rewards, dtype=torch.float, device=device)\n        mu = rewards.mean()\n        advantages = rewards - mu\n        yield generated_token_sequences, inputs, targets, rewards, advantages\n",
      "highlight": true,
      "voiceover": "Generator function that yields batches of training data for reinforcement learning. This function implements the rollout generation phase where the model samples multiple completions for each training example and evaluates their rewards. It cycles through training examples in a distributed manner with each rank handling different examples, generates multiple samples per example using the inference engine with specified temperature and top-k sampling, evaluates the reward for each generated completion using the task's reward function, and prepares the data for policy gradient training. The function pads sequences to the same length for batching, creates autoregressive input-target pairs, masks out prompt tokens and invalid positions from the loss, and computes advantages by subtracting the mean reward across samples. This advantage estimation provides the signal for policy gradient updates, encouraging the model to generate completions with above-average rewards. The generator yields indefinitely, cycling through the training dataset.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "rank_indices = range(ddp_rank, len(train_task), ddp_world_size)",
      "voiceover": "each rank is responsible for different examples in the training data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model.eval()",
      "voiceover": "ensure the model is in eval mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_sampling_steps = num_samples // device_batch_size",
      "voiceover": "go sequentially to prevent OOMs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "seed = hash((step, example_idx, sampling_step)) & 0x7FFFFFFF",
      "voiceover": "positive half of int32",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "seed=seed,",
      "voiceover": "must make sure to change the seed for each sampling step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "targets = ids[:, 1:].clone()",
      "voiceover": "clone to avoid in-place modification:",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "targets[mask_ids[:, 1:] == 0] = -1",
      "voiceover": "<-- inplace modification right here. -1 is the ignore index",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def run_gsm8k_eval(task, tokenizer, engine,\n    max_examples=None,\n    num_samples=1,\n    max_completion_tokens=256,\n    temperature=0.0,\n    top_k=50\n):\n    max_examples = min(max_examples, len(task)) if max_examples is not None else len(task)\n    for idx in range(ddp_rank, max_examples, ddp_world_size):\n        conversation = task[idx]\n        tokens = tokenizer.render_for_completion(conversation)\n        prefix_length = len(tokens)\n        assert num_samples <= device_batch_size\n        generated_token_sequences, masks = engine.generate_batch(\n            tokens,\n            num_samples=num_samples,\n            max_tokens=max_completion_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n        outcomes = []\n        for sample_tokens in generated_token_sequences:\n            generated_tokens = sample_tokens[prefix_length:]\n            generated_text = tokenizer.decode(generated_tokens)\n            is_correct = task.evaluate(conversation, generated_text)\n            outcomes.append({\n                \"is_correct\": is_correct\n            })\n        record = {\n            \"idx\": idx,\n            \"outcomes\": outcomes,\n        }\n        yield record\n",
      "highlight": true,
      "voiceover": "Evaluates the model on the GSM8K task using pass-at-k evaluation and yields evaluation records. This function implements a distributed evaluation where each rank processes a subset of examples in a round-robin fashion. For each example, it generates multiple completions using the inference engine, evaluates each completion for correctness using the task's evaluation function, and yields a record containing the outcomes for all samples. The pass-at-k metric measures whether at least one of k samples is correct, providing a measure of the model's ability to solve problems when given multiple attempts. This function is designed as a generator to allow incremental processing and logging during long evaluations. The caller is responsible for aggregating results across ranks using distributed reduction operations. This evaluation approach is particularly useful for reinforcement learning where we want to track improvement in the model's ability to generate correct solutions.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "assert num_samples <= device_batch_size",
      "voiceover": "usually this is true. we can add a loop if not...",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "optimizers = model.setup_optimizers(\n    unembedding_lr=unembedding_lr,\n    embedding_lr=embedding_lr,\n    matrix_lr=matrix_lr,\n    weight_decay=weight_decay,\n)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "optimizers = model.setup_optimizers(",
      "voiceover": "Create optimizers (AdamW for embeddings, Muon for linear layers)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "for opt in optimizers:\n    for group in opt.param_groups:\n        group[\"lr\"] = group[\"lr\"] * init_lr_frac\n        group[\"initial_lr\"] = group[\"lr\"]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for opt in optimizers:",
      "voiceover": "Set initial learning rate as fraction of base LR",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "group[\"lr\"] = group[\"lr\"] * init_lr_frac",
      "voiceover": "Scale down learning rate by init_lr_frac",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "group[\"initial_lr\"] = group[\"lr\"]",
      "voiceover": "Save scaled LR as initial LR for scheduler",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def get_lr_multiplier(it):\n    lrm = 1.0 - it / num_steps\n    return lrm\n",
      "highlight": true,
      "voiceover": "Computes the learning rate multiplier for a given training iteration. This function implements a simple linear decay schedule that ramps down the learning rate from 1.0 to 0.0 over the course of training. The multiplier starts at 1.0 at the beginning of training and linearly decreases to 0.0 at the final step, providing a smooth annealing of the learning rate. This schedule is applied to the initial learning rates configured for each optimizer, allowing the model to make larger updates early in training and smaller, more refined updates as training progresses. Linear decay is a simple but effective schedule for reinforcement learning where we want to gradually reduce the step size as the policy improves.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Total sequences per step: {examples_per_step * num_samples}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Total sequences per step: {examples_per_step * num_samples}\")",
      "voiceover": "Print total batch size in sequences per step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assert examples_per_step % ddp_world_size == 0, \"Desired examples per step must be divisible by the number of ranks\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "assert examples_per_step % ddp_world_size == 0, \"Desired examples per step must be divisible by the number of ranks\"",
      "voiceover": "Ensure even distribution across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "examples_per_rank = examples_per_step // ddp_world_size\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "examples_per_rank = examples_per_step // ddp_world_size",
      "voiceover": "Calculate examples each rank processes per step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Calculated examples per rank: {examples_per_rank}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Calculated examples per rank: {examples_per_rank}\")",
      "voiceover": "Print examples per rank",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "batch_iterator = get_batch()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "batch_iterator = get_batch()",
      "voiceover": "Create batch generator for rollout sampling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for step in range(num_steps):\n\n    if step % eval_every == 0:\n        model.eval()\n        passk = torch.zeros(device_batch_size, device=device)\n        with autocast_ctx:\n            records_iter = run_gsm8k_eval(val_task, tokenizer, engine, num_samples=device_batch_size, max_examples=eval_examples, temperature=1.0)\n            records = list(records_iter)\n        for k in range(1, device_batch_size + 1):\n            passk[k - 1] = sum(any(o[\"is_correct\"] for o in r[\"outcomes\"][:k]) for r in records)\n        num_records = torch.tensor(len(records), dtype=torch.long, device=device)\n        if ddp:\n            dist.all_reduce(num_records, op=dist.ReduceOp.SUM)\n            dist.all_reduce(passk, op=dist.ReduceOp.SUM)\n        passk = passk / num_records.item()\n        print_passk = [f\"Pass@{k}: {passk[k - 1].item():.4f}\" for k in range(1, device_batch_size + 1)]\n        print0(f\"Step {step} | {', '.join(print_passk)}\")\n        log_passk = {f\"pass@{k}\": passk[k - 1].item() for k in range(1, device_batch_size + 1)}\n        wandb_run.log({\n            \"step\": step,\n            **log_passk,\n        })\n\n    rewards_list = []\n    sequence_lengths = []\n    for example_step in range(examples_per_rank):\n        sequences_all, inputs_all, targets_all, rewards_all, advantages_all = next(batch_iterator)\n        model.train()\n        assert inputs_all.size(0) % device_batch_size == 0\n        num_passes = inputs_all.size(0) // device_batch_size\n        for pass_idx in range(num_passes):\n            b0, b1 = pass_idx * device_batch_size, (pass_idx + 1) * device_batch_size\n            inputs = inputs_all[b0:b1]\n            targets = targets_all[b0:b1]\n            rewards = rewards_all[b0:b1]\n            advantages = advantages_all[b0:b1]\n            with autocast_ctx:\n                logp = -model(inputs, targets, loss_reduction='none').view_as(inputs)\n            pg_obj = (logp * advantages.unsqueeze(-1)).sum()\n            num_valid = (targets >= 0).sum().clamp(min=1)\n            pg_obj = pg_obj / (num_valid * num_passes * examples_per_rank)\n            loss = -pg_obj\n            loss.backward()\n            print0(f\"Step {step}/{num_steps} | Example step {example_step} | Pass {pass_idx} | loss: {loss.item():.6f} | Average reward: {rewards.mean().item()}\")\n        rewards_list.append(rewards_all.mean().item())\n        sequence_lengths.extend(len(seq) for seq in sequences_all)\n\n    mean_reward = sum(rewards_list) / len(rewards_list)\n    mean_sequence_length = sum(sequence_lengths) / len(sequence_lengths)\n    if ddp:\n        mean_reward_tensor = torch.tensor(mean_reward, dtype=torch.float, device=device)\n        mean_sequence_length_tensor = torch.tensor(mean_sequence_length, dtype=torch.float, device=device)\n        dist.all_reduce(mean_reward_tensor, op=dist.ReduceOp.AVG)\n        dist.all_reduce(mean_sequence_length_tensor, op=dist.ReduceOp.AVG)\n        mean_reward = mean_reward_tensor.item()\n        mean_sequence_length = mean_sequence_length_tensor.item()\n    print0(f\"Step {step}/{num_steps} | Average reward: {mean_reward} | Average sequence length: {mean_sequence_length:.2f}\")\n    wandb_run.log({\n        \"step\": step,\n        \"reward\": mean_reward,\n        \"sequence_length\": mean_sequence_length,\n    })\n\n    lrm = get_lr_multiplier(step)\n    for opt in optimizers:\n        for group in opt.param_groups:\n            group[\"lr\"] = group[\"initial_lr\"] * lrm\n    for opt in optimizers:\n        opt.step()\n    model.zero_grad(set_to_none=True)\n    wandb_run.log({\n        \"step\": step,\n        \"lrm\": lrm,\n    })\n\n    if master_process and ((step > 0 and step % save_every == 0) or step == num_steps - 1):\n        base_dir = get_base_dir()\n        depth = model.config.n_layer\n        model_tag = f\"d{depth}\"\n        checkpoint_dir = os.path.join(base_dir, \"chatrl_checkpoints\", model_tag)\n        model_config_kwargs = model.config.__dict__\n        save_checkpoint(\n            checkpoint_dir,\n            step,\n            model.state_dict(),\n            None,\n            {\n                \"model_config\": model_config_kwargs,\n            }\n        )\n        print(f\"✅ Saved model checkpoint to {checkpoint_dir}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for step in range(num_steps):",
      "voiceover": "Main training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "if step % eval_every == 0:",
      "voiceover": "Evaluate pass@k periodically",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "passk = torch.zeros(device_batch_size, device=device)",
      "voiceover": "Initialize pass@k counters for k=1..device_batch_size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision for evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "records_iter = run_gsm8k_eval(val_task, tokenizer, engine, num_samples=device_batch_size, max_examples=eval_examples, temperature=1.0)",
      "voiceover": "Generate evaluation records",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "records = list(records_iter)",
      "voiceover": "Collect all evaluation records",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for k in range(1, device_batch_size + 1):",
      "voiceover": "Calculate pass@k for each k",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "passk[k - 1] = sum(any(o[\"is_correct\"] for o in r[\"outcomes\"][:k]) for r in records)",
      "voiceover": "Count examples where at least one of first k samples is correct",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_records = torch.tensor(len(records), dtype=torch.long, device=device)",
      "voiceover": "Count records from this rank",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "if ddp:",
      "voiceover": "Aggregate results across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dist.all_reduce(num_records, op=dist.ReduceOp.SUM)",
      "voiceover": "Sum total records across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dist.all_reduce(passk, op=dist.ReduceOp.SUM)",
      "voiceover": "Sum pass@k counts across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "passk = passk / num_records.item()",
      "voiceover": "Normalize by total number of records to get pass@k rates",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print_passk = [f\"Pass@{k}: {passk[k - 1].item():.4f}\" for k in range(1, device_batch_size + 1)]",
      "voiceover": "Format pass@k strings for printing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Step {step} | {', '.join(print_passk)}\")",
      "voiceover": "Print pass@k metrics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "log_passk = {f\"pass@{k}\": passk[k - 1].item() for k in range(1, device_batch_size + 1)}",
      "voiceover": "Create pass@k dictionary for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "wandb_run.log({",
      "voiceover": "Log pass@k metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "rewards_list = []",
      "voiceover": "List to collect mean rewards from each example",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "sequence_lengths = []",
      "voiceover": "List to collect sequence lengths for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for example_step in range(examples_per_rank):",
      "voiceover": "Process multiple examples per optimization step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "sequences_all, inputs_all, targets_all, rewards_all, advantages_all = next(batch_iterator)",
      "voiceover": "Get rollout batch for one example",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model.train()",
      "voiceover": "Set model to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "assert inputs_all.size(0) % device_batch_size == 0",
      "voiceover": "Ensure batch size is divisible by device_batch_size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_passes = inputs_all.size(0) // device_batch_size",
      "voiceover": "Calculate number of forward passes needed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for pass_idx in range(num_passes):",
      "voiceover": "Split into smaller batches to avoid OOM",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "b0, b1 = pass_idx * device_batch_size, (pass_idx + 1) * device_batch_size",
      "voiceover": "Calculate batch slice indices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "inputs = inputs_all[b0:b1]",
      "voiceover": "Extract input tokens for this pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "targets = targets_all[b0:b1]",
      "voiceover": "Extract target tokens for this pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "rewards = rewards_all[b0:b1]",
      "voiceover": "Extract rewards for this pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "advantages = advantages_all[b0:b1]",
      "voiceover": "Extract advantages for this pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "logp = -model(inputs, targets, loss_reduction='none').view_as(inputs)",
      "voiceover": "Calculate log probabilities (negate NLL to get logp)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "pg_obj = (logp * advantages.unsqueeze(-1)).sum()",
      "voiceover": "Calculate policy gradient objective (sum of logp * advantage)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "num_valid = (targets >= 0).sum().clamp(min=1)",
      "voiceover": "Count valid tokens (targets >= 0, excluding ignore_index=-1)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "pg_obj = pg_obj / (num_valid * num_passes * examples_per_rank)",
      "voiceover": "Normalize by total valid tokens across all passes and examples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "loss = -pg_obj",
      "voiceover": "Formulate loss to minimize (negate objective we want to maximize)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "loss.backward()",
      "voiceover": "Accumulate gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Step {step}/{num_steps} | Example step {example_step} | Pass {pass_idx} | loss: {loss.item():.6f} | Average reward: {rewards.mean().item()}\")",
      "voiceover": "Print progress",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "rewards_list.append(rewards_all.mean().item())",
      "voiceover": "Store mean reward for this example",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "sequence_lengths.extend(len(seq) for seq in sequences_all)",
      "voiceover": "Store sequence lengths for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_reward = sum(rewards_list) / len(rewards_list)",
      "voiceover": "Calculate mean reward across examples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_sequence_length = sum(sequence_lengths) / len(sequence_lengths)",
      "voiceover": "Calculate mean sequence length",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "if ddp:",
      "voiceover": "Aggregate statistics across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_reward_tensor = torch.tensor(mean_reward, dtype=torch.float, device=device)",
      "voiceover": "Convert to tensor",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_sequence_length_tensor = torch.tensor(mean_sequence_length, dtype=torch.float, device=device)",
      "voiceover": "Convert to tensor",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dist.all_reduce(mean_reward_tensor, op=dist.ReduceOp.AVG)",
      "voiceover": "Average across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "dist.all_reduce(mean_sequence_length_tensor, op=dist.ReduceOp.AVG)",
      "voiceover": "Average across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_reward = mean_reward_tensor.item()",
      "voiceover": "Convert back to Python float",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "mean_sequence_length = mean_sequence_length_tensor.item()",
      "voiceover": "Convert back to Python float",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print0(f\"Step {step}/{num_steps} | Average reward: {mean_reward} | Average sequence length: {mean_sequence_length:.2f}\")",
      "voiceover": "Print step statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "wandb_run.log({",
      "voiceover": "Log rollout statistics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "lrm = get_lr_multiplier(step)",
      "voiceover": "Get learning rate multiplier for current step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for opt in optimizers:",
      "voiceover": "Update learning rate for all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "group[\"lr\"] = group[\"initial_lr\"] * lrm",
      "voiceover": "Apply learning rate multiplier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "for opt in optimizers:",
      "voiceover": "Step all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "opt.step()",
      "voiceover": "Update parameters using accumulated gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model.zero_grad(set_to_none=True)",
      "voiceover": "Clear gradients (set to None for memory efficiency)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "wandb_run.log({",
      "voiceover": "Log learning rate multiplier to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "if master_process and ((step > 0 and step % save_every == 0) or step == num_steps - 1):",
      "voiceover": "Save checkpoint periodically and at end (skip first step)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get base directory for checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "depth = model.config.n_layer",
      "voiceover": "Get model depth from config",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model_tag = f\"d{depth}\"",
      "voiceover": "Create model tag based on depth",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "checkpoint_dir = os.path.join(base_dir, \"chatrl_checkpoints\", model_tag)",
      "voiceover": "Construct checkpoint directory path",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model_config_kwargs = model.config.__dict__",
      "voiceover": "Extract model configuration as dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "save_checkpoint(",
      "voiceover": "Save model checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "model.state_dict(),",
      "voiceover": "Model parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "None,",
      "voiceover": "Don't save optimizer state (not needed for RL resumption)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "{",
      "voiceover": "Metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "print(f\"✅ Saved model checkpoint to {checkpoint_dir}\")",
      "voiceover": "Confirm checkpoint saved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Chat RL\", data=[\n    user_config,\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "get_report().log(section=\"Chat RL\", data=[",
      "voiceover": "Log training summary to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "user_config,",
      "voiceover": "User configuration (CLI arguments)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "wandb_run.finish()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "wandb_run.finish()",
      "voiceover": "Finalize wandb run and upload any remaining logs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "compute_cleanup()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_rl.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed computing resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/chat_sft.py"
    },
    {
      "type": "writeText",
      "content": "import os\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "import os",
      "voiceover": "Operating system interface for file paths and environment variables",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"",
      "voiceover": "Configure PyTorch CUDA memory allocator to use expandable segments for better memory management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import wandb\nimport torch\nimport torch.distributed as dist\nfrom contextlib import nullcontext\n\nfrom nanochat.common import compute_init, compute_cleanup, get_base_dir, print0, DummyWandb, autodetect_device_type\nfrom nanochat.checkpoint_manager import load_model\nfrom nanochat.checkpoint_manager import save_checkpoint\nfrom nanochat.engine import Engine\nfrom scripts.chat_eval import run_chat_eval\n\nfrom tasks.common import TaskMixture\nfrom tasks.arc import ARC\nfrom tasks.gsm8k import GSM8K\nfrom tasks.smoltalk import SmolTalk\nfrom tasks.customjson import CustomJSON\nfrom tasks.spellingbee import SimpleSpelling, SpellingBee\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "import wandb",
      "voiceover": "Weights & Biases for experiment tracking and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "import torch.distributed as dist",
      "voiceover": "PyTorch distributed training utilities for multi-GPU training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "Context manager that does nothing, used as placeholder when autocast is disabled",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, get_base_dir, print0, DummyWandb, autodetect_device_type",
      "voiceover": "Common utilities for distributed computing and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Utility to load trained model checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from nanochat.checkpoint_manager import save_checkpoint",
      "voiceover": "Utility to save model checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine for model evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from scripts.chat_eval import run_chat_eval",
      "voiceover": "Chat model evaluation function for benchmarks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.common import TaskMixture",
      "voiceover": "Utility to mix multiple datasets for training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.arc import ARC",
      "voiceover": "ARC (AI2 Reasoning Challenge) benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.gsm8k import GSM8K",
      "voiceover": "GSM8K grade school math benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.smoltalk import SmolTalk",
      "voiceover": "SmolTalk conversational dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.customjson import CustomJSON",
      "voiceover": "Custom JSONL dataset loader",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from tasks.spellingbee import SimpleSpelling, SpellingBee",
      "voiceover": "Spelling task datasets",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "run = \"dummy\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "run = \"dummy\"",
      "voiceover": "----------------------------------------------------------------------------- SFT Hyperparameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "run = \"dummy\"",
      "voiceover": "Wandb run name (\"dummy\" disables wandb logging)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "source = \"mid\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "source = \"mid\"",
      "voiceover": "Model source to load: base (base model) or mid (mid-trained model)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "model_tag = None\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model_tag = None",
      "voiceover": "Optional model tag identifier for loading specific checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "step = None\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "step = None",
      "voiceover": "Optional checkpoint step number to load",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_type = \"\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "device_type = \"\"",
      "voiceover": "Device type: cuda|cpu|mps (empty string auto-detects)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "dtype = \"bfloat16\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "dtype = \"bfloat16\"",
      "voiceover": "Precision for model training and inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "device_batch_size = 4\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "device_batch_size = 4",
      "voiceover": "Batch size per device (tune to avoid OOM)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_epochs = 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_epochs = 1",
      "voiceover": "Number of epochs to train on the dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "num_iterations = -1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_iterations = -1",
      "voiceover": "Explicit number of training steps (-1 = derive from num_epochs)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "target_examples_per_step = 32\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "target_examples_per_step = 32",
      "voiceover": "Target total examples per optimization step across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "unembedding_lr = 0.004\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "unembedding_lr = 0.004",
      "voiceover": "Learning rate for output projection layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "embedding_lr = 0.2\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "embedding_lr = 0.2",
      "voiceover": "Learning rate for embedding layer (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "matrix_lr = 0.02\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "matrix_lr = 0.02",
      "voiceover": "Learning rate for linear layer weight matrices (Muon optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "weight_decay = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "weight_decay = 0.0",
      "voiceover": "Weight decay for embedding/unembedding parameters (AdamW optimizer)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "init_lr_frac = 0.02\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "init_lr_frac = 0.02",
      "voiceover": "Initial learning rate as fraction of base LR (start from lower LR)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_every = 100\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "eval_every = 100",
      "voiceover": "Evaluate validation loss every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_steps = 100\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "eval_steps = 100",
      "voiceover": "Number of validation steps to average loss over",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_metrics_every = 200\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "eval_metrics_every = 200",
      "voiceover": "Evaluate benchmark metrics (MMLU, ARC) every N steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "eval_metrics_max_problems = 1024\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "eval_metrics_max_problems = 1024",
      "voiceover": "Maximum problems to evaluate for benchmark metrics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]",
      "voiceover": "Extract all configuration variable names for CLI override",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "exec(open(os.path.join('nanochat', 'configurator.py')).read())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "exec(open(os.path.join('nanochat', 'configurator.py')).read())",
      "voiceover": "Execute configurator to allow command-line and config file overrides",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "user_config = {k: globals()[k] for k in config_keys}\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "user_config = {k: globals()[k] for k in config_keys}",
      "voiceover": "Capture final configuration values for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "device_type = autodetect_device_type() if device_type == \"\" else device_type\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "device_type = autodetect_device_type() if device_type == \"\" else device_type",
      "voiceover": "Auto-detect device if not specified",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initialize distributed environment and get device info",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "master_process = ddp_rank == 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "master_process = ddp_rank == 0",
      "voiceover": "Master process (rank 0) handles logging and checkpointing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ptdtype = torch.float32 if dtype == 'float32' else torch.bfloat16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "ptdtype = torch.float32 if dtype == 'float32' else torch.bfloat16",
      "voiceover": "Convert dtype string to PyTorch dtype",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Enable mixed precision on CUDA, no-op otherwise",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "use_dummy_wandb = run == \"dummy\" or not master_process\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "use_dummy_wandb = run == \"dummy\" or not master_process",
      "voiceover": "Disable wandb if run is \"dummy\" or not master process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-sft\", name=run, config=user_config, save_code=True)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-sft\", name=run, config=user_config, save_code=True)",
      "voiceover": "Initialize wandb or dummy logger",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "model, tokenizer, meta = load_model(source, device, phase=\"train\", model_tag=model_tag, step=step)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model, tokenizer, meta = load_model(source, device, phase=\"train\", model_tag=model_tag, step=step)",
      "voiceover": "Load model checkpoint and tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "orig_model = model\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "orig_model = model",
      "voiceover": "Keep reference to original uncompiled model for saving",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "engine = Engine(model, tokenizer)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "engine = Engine(model, tokenizer)",
      "voiceover": "Create inference engine for evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "identity_conversations_filepath = os.path.join(get_base_dir(), \"identity_conversations.jsonl\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "identity_conversations_filepath = os.path.join(get_base_dir(), \"identity_conversations.jsonl\")",
      "voiceover": "----------------------------------------------------------------------------- Task data mixture we'll train on",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "identity_conversations_filepath = os.path.join(get_base_dir(), \"identity_conversations.jsonl\")",
      "voiceover": "Path to synthetic identity conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "train_ds = TaskMixture([\n    ARC(subset=\"ARC-Easy\", split=\"train\"),\n    ARC(subset=\"ARC-Challenge\", split=\"train\"),\n    GSM8K(subset=\"main\", split=\"train\"),\n    SmolTalk(split=\"train\", stop=10_000),\n    CustomJSON(filepath=identity_conversations_filepath),\n    SimpleSpelling(size=300, split=\"train\"),\n    SpellingBee(size=300, split=\"train\"),\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "train_ds = TaskMixture([",
      "voiceover": "Create training dataset mixture from multiple tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "ARC(subset=\"ARC-Easy\", split=\"train\"),",
      "voiceover": "2.3K rows of easy reasoning questions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "ARC(subset=\"ARC-Challenge\", split=\"train\"),",
      "voiceover": "1.1K rows of challenging reasoning questions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "GSM8K(subset=\"main\", split=\"train\"),",
      "voiceover": "8K rows of grade school math problems",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "SmolTalk(split=\"train\", stop=10_000),",
      "voiceover": "10K rows of conversational data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "CustomJSON(filepath=identity_conversations_filepath),",
      "voiceover": "1K rows of synthetic identity conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "SimpleSpelling(size=300, split=\"train\"),",
      "voiceover": "300 rows of simple spelling tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "SpellingBee(size=300, split=\"train\"),",
      "voiceover": "300 rows of spelling bee tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "])",
      "voiceover": "Total: 2.3K + 1.1K + 8K + 10K + 1K + 0.3K + 0.3K = 23K rows",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "val_ds = SmolTalk(split=\"test\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "val_ds = SmolTalk(split=\"test\")",
      "voiceover": "Validation dataset: general conversations (24K rows available)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def sft_data_generator(dataset, batch_size):\n    pad_token_id = tokenizer.encode_special(\"<|assistant_end|>\")\n    def collate_and_yield(batch):\n        nrows = len(batch)\n        ncols = max(len(ids) for ids, mask in batch) - 1\n        inputs = torch.full((nrows, ncols), pad_token_id, dtype=torch.long)\n        targets = torch.full((nrows, ncols), -1, dtype=torch.long)\n        for i, (ids, mask) in enumerate(batch):\n            n = len(ids)\n            ids_tensor = torch.tensor(ids, dtype=torch.long)\n            inputs[i, :n-1] = ids_tensor[:-1]\n            row_targets = ids_tensor[1:]\n            mask_tensor = torch.tensor(mask[1:], dtype=torch.long)\n            row_targets[mask_tensor == 0] = -1\n            targets[i, :n-1] = row_targets\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        return inputs, targets\n    batch = []\n    while True:\n        for i in range(ddp_rank, len(dataset), ddp_world_size):\n            doc = dataset[i]\n            ids, mask = tokenizer.render_conversation(doc)\n            batch.append((ids, mask))\n            if len(batch) == batch_size:\n                yield collate_and_yield(batch)\n                batch = []\n",
      "highlight": true,
      "voiceover": "Generator function that yields batches of tokenized conversation data for supervised fine-tuning. This function implements a custom data loader that handles variable-length conversations by tokenizing them, padding sequences to the same length within each batch, and creating autoregressive input-target pairs for language modeling. It uses a special assistant end token as the padding token and masks out padded positions in the targets using -1 as the ignore index. The function includes a nested collate_and_yield helper that prepares batches by finding the maximum sequence length, creating padded tensors, and properly masking targets based on the conversation structure so that the model only learns from assistant responses and not from user prompts or padding. The generator cycles through the dataset indefinitely in epochs, with each rank processing different examples in a distributed manner to ensure no data duplication across GPUs. This design allows for efficient training on mixed-task conversational data with varying lengths.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "pad_token_id = tokenizer.encode_special(\"<|assistant_end|>\")",
      "voiceover": "use <|assistant_end|> as the pad token is ok, these positions are masked in the loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "ncols = max(len(ids) for ids, mask in batch) - 1",
      "voiceover": "seq of n creates inputs/targets of n-1",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "targets = torch.full((nrows, ncols), -1, dtype=torch.long)",
      "voiceover": "-1 is ignore index",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "row_targets[mask_tensor == 0] = -1",
      "voiceover": "mask out targets where mask is 0",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "inputs = inputs.to(device)",
      "voiceover": "move to device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "examples_per_step = device_batch_size * ddp_world_size\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "examples_per_step = device_batch_size * ddp_world_size",
      "voiceover": "Total examples processed per step across all devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Target examples per step: {target_examples_per_step}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Target examples per step: {target_examples_per_step}\")",
      "voiceover": "Print target batch size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Device batch size: {device_batch_size}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Device batch size: {device_batch_size}\")",
      "voiceover": "Print per-device batch size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"Examples per step is device_batch_size * ddp_world_size: {examples_per_step}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Examples per step is device_batch_size * ddp_world_size: {examples_per_step}\")",
      "voiceover": "Print actual examples per step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assert target_examples_per_step % examples_per_step == 0, \"Target examples per step must be divisible by examples per step\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "assert target_examples_per_step % examples_per_step == 0, \"Target examples per step must be divisible by examples per step\"",
      "voiceover": "Ensure target is achievable with gradient accumulation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "grad_accum_steps = target_examples_per_step // examples_per_step\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "grad_accum_steps = target_examples_per_step // examples_per_step",
      "voiceover": "Calculate gradient accumulation steps needed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print0(f\"=> Setting grad accum steps: {grad_accum_steps}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"=> Setting grad accum steps: {grad_accum_steps}\")",
      "voiceover": "Print gradient accumulation steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if num_iterations == -1:\n    assert num_epochs > 0, \"num_epochs must be positive if num_iterations is -1\"\n    num_iterations = (len(train_ds) // target_examples_per_step) * num_epochs\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if num_iterations == -1:",
      "voiceover": "If num_iterations not explicitly set",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "assert num_epochs > 0, \"num_epochs must be positive if num_iterations is -1\"",
      "voiceover": "Ensure num_epochs is valid",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_iterations = (len(train_ds) // target_examples_per_step) * num_epochs",
      "voiceover": "Calculate iterations from epochs and dataset size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "train_loader = sft_data_generator(train_ds, batch_size=device_batch_size)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "train_loader = sft_data_generator(train_ds, batch_size=device_batch_size)",
      "voiceover": "Create training data generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "build_val_loader = lambda: sft_data_generator(val_ds, batch_size=device_batch_size)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "build_val_loader = lambda: sft_data_generator(val_ds, batch_size=device_batch_size)",
      "voiceover": "Lambda to create validation data generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "optimizers = model.setup_optimizers(\n    unembedding_lr=unembedding_lr,\n    embedding_lr=embedding_lr,\n    matrix_lr=matrix_lr,\n    weight_decay=weight_decay,\n)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "optimizers = model.setup_optimizers(",
      "voiceover": "Create optimizers (AdamW for embeddings, Muon for linear layers)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for opt in optimizers:\n    for group in opt.param_groups:\n        group[\"lr\"] = group[\"lr\"] * init_lr_frac\n        group[\"initial_lr\"] = group[\"lr\"]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for opt in optimizers:",
      "voiceover": "Set initial learning rate as fraction of base LR",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "group[\"lr\"] = group[\"lr\"] * init_lr_frac",
      "voiceover": "Scale down learning rate by init_lr_frac",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "group[\"initial_lr\"] = group[\"lr\"]",
      "voiceover": "Save scaled LR as initial LR for scheduler",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def get_lr_multiplier(it):\n    lrm = 1.0 - it / num_iterations\n    return lrm\n",
      "highlight": true,
      "voiceover": "Computes the learning rate multiplier for a given training iteration. This function implements a simple linear decay schedule that ramps down the learning rate from 1.0 to 0.0 over the course of training. The multiplier starts at 1.0 at the beginning of training and linearly decreases to 0.0 at the final step, providing a smooth annealing of the learning rate. This schedule is applied to the initial learning rates configured for each optimizer, allowing the model to make larger updates early in training and smaller, more refined updates as training progresses. Linear decay is a simple but effective schedule for supervised fine-tuning where we want to gradually reduce the step size as the model adapts to the chat format.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "step = 0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "step = 0",
      "voiceover": "Initialize step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for step in range(num_iterations):\n    last_step = step == num_iterations - 1\n\n    if last_step or step % eval_every == 0:\n        model.eval()\n        val_loader = build_val_loader()\n        losses = []\n        for _ in range(eval_steps):\n            val_inputs, val_targets = next(val_loader)\n            with torch.no_grad(), autocast_ctx:\n                loss = model(val_inputs, val_targets)\n            losses.append(loss)\n        val_loss = torch.stack(losses).mean()\n        if ddp:\n            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n        val_loss = val_loss.item()\n        print0(f\"Step {step:05d} | Validation loss: {val_loss:.6f}\")\n        wandb_run.log({\n            \"step\": step,\n            \"val_loss\": val_loss,\n        })\n        model.train()\n\n    if last_step or (step > 0 and step % eval_metrics_every == 0):\n        model.eval()\n        metrics = {}\n        with torch.no_grad(), autocast_ctx:\n            metrics[\"mmlu_acc\"] = run_chat_eval(\"MMLU\", model, tokenizer, engine, batch_size=device_batch_size*2, max_problems=eval_metrics_max_problems)\n            metrics[\"arc_easy_acc\"] = run_chat_eval(\"ARC-Easy\", model, tokenizer, engine, batch_size=device_batch_size*2, max_problems=eval_metrics_max_problems)\n        metrics_str = ', '.join(f'{k}: {v:.6f}' for k, v in metrics.items())\n        print0(f\"Step {step:05d} | {metrics_str}\")\n        wandb_run.log({\n            \"step\": step,\n            **metrics,\n        })\n        model.train()\n\n    if last_step:\n        break\n\n    num_tokens = torch.tensor(0, device=device)\n    for micro_step in range(grad_accum_steps):\n        train_inputs, train_targets = next(train_loader)\n        with autocast_ctx:\n            loss = model(train_inputs, train_targets)\n        train_loss = loss.detach()\n        loss = loss / grad_accum_steps\n        loss.backward()\n        num_tokens += (train_targets >= 0).sum()\n    if ddp:\n        dist.all_reduce(num_tokens, op=dist.ReduceOp.SUM)\n\n    lrm = get_lr_multiplier(step)\n    for opt in optimizers:\n        for group in opt.param_groups:\n            group[\"lr\"] = group[\"initial_lr\"] * lrm\n\n    for opt in optimizers:\n        opt.step()\n    model.zero_grad(set_to_none=True)\n\n    train_loss_item = train_loss.item()\n    num_tokens_item = num_tokens.item()\n    print0(f\"Step {step:05d}/{num_iterations:05d} | Training loss: {train_loss_item:.6f}| lrm: {lrm:.6f}| num_tokens: {num_tokens_item:,}\")\n    wandb_run.log({\n        \"step\": step,\n        \"lrm\": lrm,\n        \"train_loss\": train_loss_item,\n        \"num_tokens\": num_tokens_item,\n    })\n    step += 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for step in range(num_iterations):",
      "voiceover": "Main training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "last_step = step == num_iterations - 1",
      "voiceover": "Check if this is the final step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if last_step or step % eval_every == 0:",
      "voiceover": "Evaluate validation loss periodically and at end",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "val_loader = build_val_loader()",
      "voiceover": "Create validation data generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "losses = []",
      "voiceover": "List to collect validation losses",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for _ in range(eval_steps):",
      "voiceover": "Evaluate over multiple batches",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "val_inputs, val_targets = next(val_loader)",
      "voiceover": "Get validation batch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "with torch.no_grad(), autocast_ctx:",
      "voiceover": "Disable gradients and use mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "loss = model(val_inputs, val_targets)",
      "voiceover": "Compute validation loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "losses.append(loss)",
      "voiceover": "Collect loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "val_loss = torch.stack(losses).mean()",
      "voiceover": "Average losses over eval_steps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if ddp:",
      "voiceover": "Aggregate across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)",
      "voiceover": "Average validation loss across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "val_loss = val_loss.item()",
      "voiceover": "Convert to Python float",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Step {step:05d} | Validation loss: {val_loss:.6f}\")",
      "voiceover": "Print validation loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "wandb_run.log({",
      "voiceover": "Log validation loss to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if last_step or (step > 0 and step % eval_metrics_every == 0):",
      "voiceover": "Evaluate benchmark metrics periodically",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "metrics = {}",
      "voiceover": "Dictionary to store metric results",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "with torch.no_grad(), autocast_ctx:",
      "voiceover": "Disable gradients and use mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "metrics[\"mmlu_acc\"] = run_chat_eval(\"MMLU\", model, tokenizer, engine, batch_size=device_batch_size*2, max_problems=eval_metrics_max_problems)",
      "voiceover": "Evaluate MMLU accuracy (2x batch size since no gradients)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "metrics[\"arc_easy_acc\"] = run_chat_eval(\"ARC-Easy\", model, tokenizer, engine, batch_size=device_batch_size*2, max_problems=eval_metrics_max_problems)",
      "voiceover": "Evaluate ARC-Easy accuracy",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "metrics_str = ', '.join(f'{k}: {v:.6f}' for k, v in metrics.items())",
      "voiceover": "Format metrics for printing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Step {step:05d} | {metrics_str}\")",
      "voiceover": "Print metrics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "wandb_run.log({",
      "voiceover": "Log metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if last_step:",
      "voiceover": "Exit loop after final evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "break",
      "voiceover": "Terminate training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_tokens = torch.tensor(0, device=device)",
      "voiceover": "Counter for total number of supervised tokens seen in this step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for micro_step in range(grad_accum_steps):",
      "voiceover": "Gradient accumulation loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "train_inputs, train_targets = next(train_loader)",
      "voiceover": "Get training batch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "loss = model(train_inputs, train_targets)",
      "voiceover": "Forward pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "train_loss = loss.detach()",
      "voiceover": "Detach loss for logging (prevent gradient tracking)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "loss = loss / grad_accum_steps",
      "voiceover": "Normalize loss by accumulation steps (since .backward() sums gradients)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "loss.backward()",
      "voiceover": "Backward pass (accumulate gradients)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_tokens += (train_targets >= 0).sum()",
      "voiceover": "Count valid tokens (excluding ignore_index=-1)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if ddp:",
      "voiceover": "Aggregate token count across ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "dist.all_reduce(num_tokens, op=dist.ReduceOp.SUM)",
      "voiceover": "Sum token counts across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "lrm = get_lr_multiplier(step)",
      "voiceover": "Get learning rate multiplier for current step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for opt in optimizers:",
      "voiceover": "Update learning rate for all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "group[\"lr\"] = group[\"initial_lr\"] * lrm",
      "voiceover": "Apply learning rate multiplier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "for opt in optimizers:",
      "voiceover": "Step all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "opt.step()",
      "voiceover": "Update parameters using accumulated gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.zero_grad(set_to_none=True)",
      "voiceover": "Clear gradients (set to None for memory efficiency)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "train_loss_item = train_loss.item()",
      "voiceover": "Convert training loss to Python float",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "num_tokens_item = num_tokens.item()",
      "voiceover": "Convert token count to Python int",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print0(f\"Step {step:05d}/{num_iterations:05d} | Training loss: {train_loss_item:.6f}| lrm: {lrm:.6f}| num_tokens: {num_tokens_item:,}\")",
      "voiceover": "Print training progress",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "wandb_run.log({",
      "voiceover": "Log training metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "step += 1",
      "voiceover": "Increment step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if master_process:\n    base_dir = get_base_dir()\n    depth = model.config.n_layer\n    model_tag = f\"d{depth}\"\n    checkpoint_dir = os.path.join(base_dir, \"chatsft_checkpoints\", model_tag)\n    model_config_kwargs = model.config.__dict__\n    save_checkpoint(\n        checkpoint_dir,\n        step,\n        model.state_dict(),\n        None,\n        {\n            \"step\": step,\n            \"val_loss\": val_loss,\n            **metrics,\n            \"model_config\": model_config_kwargs,\n        }\n    )\n    print(f\"✅ Saved model checkpoint to {checkpoint_dir}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "if master_process:",
      "voiceover": "Save checkpoint only on master process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get base directory for checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "depth = model.config.n_layer",
      "voiceover": "Get model depth from config",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model_tag = f\"d{depth}\"",
      "voiceover": "Create model tag based on depth",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "checkpoint_dir = os.path.join(base_dir, \"chatsft_checkpoints\", model_tag)",
      "voiceover": "Construct checkpoint directory path",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model_config_kwargs = model.config.__dict__",
      "voiceover": "Extract model configuration as dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "save_checkpoint(",
      "voiceover": "Save model checkpoint",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "model.state_dict(),",
      "voiceover": "Model parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "None,",
      "voiceover": "Don't save optimizer state (not needed for SFT resumption)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "{",
      "voiceover": "Metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "print(f\"✅ Saved model checkpoint to {checkpoint_dir}\")",
      "voiceover": "Confirm checkpoint saved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Chat SFT\", data=[\n    user_config,\n    {\n        \"Training rows\": len(train_ds),\n        \"Number of iterations\": num_iterations,\n        \"Training loss\": train_loss_item,\n        \"Validation loss\": val_loss,\n    },\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "get_report().log(section=\"Chat SFT\", data=[",
      "voiceover": "Log training summary to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "user_config,",
      "voiceover": "User configuration (CLI arguments)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "{",
      "voiceover": "Training statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "wandb_run.finish()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "wandb_run.finish()",
      "voiceover": "Finalize wandb run and upload any remaining logs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "compute_cleanup()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_sft.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed computing resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/chat_web.py"
    },
    {
      "type": "writeText",
      "content": "import argparse\nimport json\nimport os\nimport torch\nimport asyncio\nimport logging\nimport random\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse, HTMLResponse, FileResponse\nfrom pydantic import BaseModel\nfrom typing import List, Optional, AsyncGenerator\nfrom dataclasses import dataclass\nfrom contextlib import nullcontext\nfrom nanochat.common import compute_init, autodetect_device_type\nfrom nanochat.checkpoint_manager import load_model\nfrom nanochat.engine import Engine\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import argparse",
      "voiceover": "Command-line argument parsing for server configuration options",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import json",
      "voiceover": "JSON encoding/decoding for API responses and server-sent events",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import os",
      "voiceover": "Operating system interface for file path operations when serving static files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import torch",
      "voiceover": "PyTorch framework for loading models and managing GPU devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import asyncio",
      "voiceover": "Asynchronous I/O for managing worker pool queue and concurrent request handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import logging",
      "voiceover": "Logging framework for recording conversation traffic and server events",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import random",
      "voiceover": "Random number generation for seeding generation to ensure different outputs per request",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from contextlib import asynccontextmanager",
      "voiceover": "Context manager decorator for FastAPI lifespan events that initialize models on startup",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from fastapi import FastAPI, HTTPException",
      "voiceover": "FastAPI web framework and HTTP exception handling for API endpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from fastapi.middleware.cors import CORSMiddleware",
      "voiceover": "CORS middleware to allow cross-origin requests from web browsers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from fastapi.responses import StreamingResponse, HTMLResponse, FileResponse",
      "voiceover": "Response types for streaming tokens, serving HTML UI, and serving static files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from pydantic import BaseModel",
      "voiceover": "Pydantic base class for request/response validation and serialization",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from typing import List, Optional, AsyncGenerator",
      "voiceover": "Type hints for function signatures and async generators",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from dataclasses import dataclass",
      "voiceover": "Dataclass decorator for Worker structure that holds model and device information",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "No-op context manager used when autocast is disabled on non-CUDA devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from nanochat.common import compute_init, autodetect_device_type",
      "voiceover": "Utilities for initializing compute environment and detecting available device type",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Checkpoint loading utility that loads trained model weights onto specified device",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "from nanochat.engine import Engine",
      "voiceover": "Inference engine that handles token generation with sampling parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "MAX_MESSAGES_PER_REQUEST = 500\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_MESSAGES_PER_REQUEST = 500",
      "voiceover": "Prevents excessively long conversation histories that could cause memory issues or slow processing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_MESSAGE_LENGTH = 8000\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_MESSAGE_LENGTH = 8000",
      "voiceover": "Limits individual message size to prevent abuse and ensure reasonable processing time per message",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_TOTAL_CONVERSATION_LENGTH = 32000\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_TOTAL_CONVERSATION_LENGTH = 32000",
      "voiceover": "Caps total conversation length to prevent context window overflow and excessive memory usage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MIN_TEMPERATURE = 0.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MIN_TEMPERATURE = 0.0",
      "voiceover": "Lower bound for temperature parameter ensuring deterministic generation is allowed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_TEMPERATURE = 2.0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_TEMPERATURE = 2.0",
      "voiceover": "Upper bound for temperature parameter preventing extremely random outputs that are usually nonsensical",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MIN_TOP_K = 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MIN_TOP_K = 1",
      "voiceover": "Minimum top-k value allowing greedy decoding when set to 1",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_TOP_K = 200\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_TOP_K = 200",
      "voiceover": "Maximum top-k value preventing excessive computation while still allowing diverse sampling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MIN_MAX_TOKENS = 1\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MIN_MAX_TOKENS = 1",
      "voiceover": "Minimum generation length ensuring at least one token is generated",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "MAX_MAX_TOKENS = 4096\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "MAX_MAX_TOKENS = 4096",
      "voiceover": "Maximum generation length preventing runaway generation and excessive computation time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "parser = argparse.ArgumentParser(description='NanoChat Web Server')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser = argparse.ArgumentParser(description='NanoChat Web Server')",
      "voiceover": "Creates argument parser for configuring server behavior via command-line flags",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-n', '--num-gpus', type=int, default=1, help='Number of GPUs to use (default: 1)')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-n', '--num-gpus', type=int, default=1, help='Number of GPUs to use (default: 1)')",
      "voiceover": "Controls worker pool size for parallel request handling across multiple GPUs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-i', '--source', type=str, default=\"sft\", help=\"Source of the model: sft|mid|rl\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-i', '--source', type=str, default=\"sft\", help=\"Source of the model: sft|mid|rl\")",
      "voiceover": "Determines which checkpoint directory to load model from based on training stage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-t', '--temperature', type=float, default=0.8, help='Default temperature for generation')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-t', '--temperature', type=float, default=0.8, help='Default temperature for generation')",
      "voiceover": "Sets default sampling temperature when client doesn't specify one",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-k', '--top-k', type=int, default=50, help='Default top-k sampling parameter')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-k', '--top-k', type=int, default=50, help='Default top-k sampling parameter')",
      "voiceover": "Sets default top-k value for nucleus sampling when client doesn't specify",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-m', '--max-tokens', type=int, default=512, help='Default max tokens for generation')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-m', '--max-tokens', type=int, default=512, help='Default max tokens for generation')",
      "voiceover": "Sets default generation length limit when client doesn't specify",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-g', '--model-tag', type=str, default=None, help='Model tag to load')",
      "voiceover": "Optional model tag for loading specific model variant from checkpoint directory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-s', '--step', type=int, default=None, help='Step to load')",
      "voiceover": "Optional checkpoint step number for loading model from specific training iteration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-p', '--port', type=int, default=8000, help='Port to run the server on')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-p', '--port', type=int, default=8000, help='Port to run the server on')",
      "voiceover": "Configures which port the HTTP server listens on for incoming requests",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('-d', '--dtype', type=str, default='bfloat16', choices=['float32', 'bfloat16'])",
      "voiceover": "Sets precision for model inference affecting memory usage and speed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('--device-type', type=str, default='', choices=['cuda', 'cpu', 'mps'], help='Device type for evaluation: cuda|cpu|mps. empty => autodetect')",
      "voiceover": "Explicitly sets device type or auto-detects if empty",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--host', type=str, default='0.0.0.0', help='Host to bind the server to')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "parser.add_argument('--host', type=str, default='0.0.0.0', help='Host to bind the server to')",
      "voiceover": "Configures network interface to bind to with 0.0.0.0 allowing external connections",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "args = parser.parse_args()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parses command-line arguments into args object used throughout server initialization and request handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "logging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "logging.basicConfig(",
      "voiceover": "Configures root logger for conversation traffic with timestamp and message format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "logger = logging.getLogger(__name__)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "logger = logging.getLogger(__name__)",
      "voiceover": "Creates module-level logger used for logging incoming conversations and assistant responses",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "device_type = autodetect_device_type() if args.device_type == \"\" else args.device_type",
      "voiceover": "Determines device type either from command-line argument or auto-detection for model loading",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initializes compute environment returning distributed training info though only device is used here",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "ptdtype = torch.float32 if args.dtype == 'float32' else torch.bfloat16",
      "voiceover": "Converts dtype string to PyTorch dtype object used for autocast context in inference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@dataclass\n"
    },
    {
      "type": "writeText",
      "content": "class Worker:\n",
      "highlight": true,
      "voiceover": "Represents a single worker process that holds a model replica loaded on a specific GPU device. This dataclass encapsulates all the resources needed for one worker to independently handle inference requests including the GPU identifier for tracking which device is being used, the PyTorch device object for tensor operations, the inference engine that wraps the model and handles token generation, the tokenizer for encoding and decoding text, and the autocast context manager for mixed precision inference. The worker design allows multiple replicas of the model to run in parallel across different GPUs enabling concurrent request handling where each worker can process one request at a time while other workers handle other requests simultaneously.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "    gpu_id: int\n"
    },
    {
      "type": "writeText",
      "content": "    device: torch.device\n"
    },
    {
      "type": "writeText",
      "content": "    engine: Engine\n"
    },
    {
      "type": "writeText",
      "content": "    tokenizer: object\n"
    },
    {
      "type": "writeText",
      "content": "    autocast_ctx: torch.amp.autocast\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class WorkerPool:\n",
      "highlight": true,
      "voiceover": "Manages a pool of worker processes each with a model replica on a different GPU for parallel request handling. This class is responsible for initializing multiple workers by loading the model onto each available GPU, maintaining an async queue of available workers that aren't currently processing requests, and providing acquire and release methods for request handlers to obtain and return workers. The pool design implements a simple load balancing strategy where incoming requests wait in the queue until a worker becomes available ensuring that no GPU is overloaded while others sit idle. The async queue automatically handles blocking when all workers are busy allowing the server to gracefully handle more concurrent requests than available GPUs by queuing them. This architecture enables horizontal scaling where adding more GPUs linearly increases request throughput without code changes.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, num_gpus: Optional[int] = None):\n",
      "highlight": true,
      "voiceover": "Initializes the worker pool by determining how many GPUs to use and setting up the async queue for worker management. If num_gpus is not specified the method auto-detects the number of available GPUs when using CUDA or defaults to 1 for CPU and MPS devices. The initialization creates an empty list to store worker objects and an async queue that will hold available workers enabling the acquire and release pattern. The queue-based design ensures thread-safe worker management in the async environment where multiple concurrent requests might try to acquire workers simultaneously. The auto-detection logic allows the server to automatically utilize all available GPUs without manual configuration while still supporting explicit GPU count specification for resource-constrained deployments.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    async def initialize(self, source: str, model_tag: Optional[str] = None, step: Optional[int] = None):\n        print(f\"Initializing worker pool with {self.num_gpus} GPUs...\")\n        if self.num_gpus > 1:\n            assert device_type == \"cuda\", \"Only CUDA supports multiple workers/GPUs. cpu|mps does not.\"\n\n        for gpu_id in range(self.num_gpus):\n\n            if device_type == \"cuda\":\n                device = torch.device(f\"cuda:{gpu_id}\")\n                print(f\"Loading model on GPU {gpu_id}...\")\n            else:\n                device = torch.device(device_type)\n                print(f\"Loading model on {device_type}...\")\n\n            model, tokenizer, _ = load_model(source, device, phase=\"eval\", model_tag=model_tag, step=step)\n            engine = Engine(model, tokenizer)\n            autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n\n            worker = Worker(\n                gpu_id=gpu_id,\n                device=device,\n                engine=engine,\n                tokenizer=tokenizer,\n                autocast_ctx=autocast_ctx\n            )\n            self.workers.append(worker)\n            await self.available_workers.put(worker)\n\n        print(f\"All {self.num_gpus} workers initialized!\")\n",
      "highlight": true,
      "voiceover": "Loads the model onto each GPU creating a worker for each device and populating the worker pool. This async method iterates through the specified number of GPUs loading a complete copy of the model onto each device along with its tokenizer and inference engine. For CUDA devices it creates separate device objects for each GPU index while for CPU and MPS it uses a single device since they don't support multiple workers. Each worker is configured with an autocast context for mixed precision inference on CUDA or a null context on other devices. After creating each worker it's immediately added to the available workers queue making it ready to handle requests. The method includes validation to ensure multiple workers are only used with CUDA since CPU and MPS don't support true parallelism across devices. This initialization happens during server startup in the FastAPI lifespan context ensuring all models are loaded before the server begins accepting requests.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "device = torch.device(device_type)",
      "voiceover": "e.g. cpu|mps",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    async def acquire_worker(self) -> Worker:\n",
      "highlight": true,
      "voiceover": "Retrieves an available worker from the pool blocking if all workers are currently busy processing requests. This async method calls get on the async queue which will wait until a worker becomes available if the queue is empty. The blocking behavior is intentional and desirable as it provides natural backpressure when the server is under heavy load preventing it from accepting more requests than it can handle. The method returns a Worker object that the caller is responsible for releasing back to the pool after completing the request. This acquire-release pattern ensures that each worker processes only one request at a time preventing race conditions and ensuring consistent inference behavior.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    async def release_worker(self, worker: Worker):\n",
      "highlight": true,
      "voiceover": "Returns a worker to the pool making it available for other requests after the current request completes. This async method puts the worker back into the async queue where it can be acquired by the next waiting request. The method is called in a finally block to ensure workers are always returned even if request processing encounters an error preventing worker leakage that would gradually reduce available capacity. The queue-based design ensures that workers are distributed fairly to waiting requests in FIFO order preventing request starvation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class ChatMessage(BaseModel):\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "class ChatMessage(BaseModel):",
      "voiceover": "Pydantic model for individual messages in conversation history with role and content fields",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    role: str\n"
    },
    {
      "type": "writeText",
      "content": "    content: str\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class ChatRequest(BaseModel):\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "class ChatRequest(BaseModel):",
      "voiceover": "Pydantic model for incoming chat API requests validating message structure and optional generation parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    messages: List[ChatMessage]\n"
    },
    {
      "type": "writeText",
      "content": "    temperature: Optional[float] = None\n"
    },
    {
      "type": "writeText",
      "content": "    max_tokens: Optional[int] = None\n"
    },
    {
      "type": "writeText",
      "content": "    top_k: Optional[int] = None\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def validate_chat_request(request: ChatRequest):\n    if len(request.messages) == 0:\n        raise HTTPException(status_code=400, detail=\"At least one message is required\")\n    if len(request.messages) > MAX_MESSAGES_PER_REQUEST:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Too many messages. Maximum {MAX_MESSAGES_PER_REQUEST} messages allowed per request\"\n        )\n\n    total_length = 0\n    for i, message in enumerate(request.messages):\n        if not message.content:\n            raise HTTPException(status_code=400, detail=f\"Message {i} has empty content\")\n\n        msg_length = len(message.content)\n        if msg_length > MAX_MESSAGE_LENGTH:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Message {i} is too long. Maximum {MAX_MESSAGE_LENGTH} characters allowed per message\"\n            )\n        total_length += msg_length\n\n    if total_length > MAX_TOTAL_CONVERSATION_LENGTH:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Total conversation is too long. Maximum {MAX_TOTAL_CONVERSATION_LENGTH} characters allowed\"\n        )\n\n    for i, message in enumerate(request.messages):\n        if message.role not in [\"user\", \"assistant\"]:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Message {i} has invalid role. Must be 'user', 'assistant', or 'system'\"\n            )\n\n    if request.temperature is not None:\n        if not (MIN_TEMPERATURE <= request.temperature <= MAX_TEMPERATURE):\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Temperature must be between {MIN_TEMPERATURE} and {MAX_TEMPERATURE}\"\n            )\n\n    if request.top_k is not None:\n        if not (MIN_TOP_K <= request.top_k <= MAX_TOP_K):\n            raise HTTPException(\n                status_code=400,\n                detail=f\"top_k must be between {MIN_TOP_K} and {MAX_TOP_K}\"\n            )\n\n    if request.max_tokens is not None:\n        if not (MIN_MAX_TOKENS <= request.max_tokens <= MAX_MAX_TOKENS):\n            raise HTTPException(\n                status_code=400,\n                detail=f\"max_tokens must be between {MIN_MAX_TOKENS} and {MAX_MAX_TOKENS}\"\n            )\n",
      "highlight": true,
      "voiceover": "Validates incoming chat requests against abuse prevention limits raising HTTP exceptions for invalid requests. This function implements multiple layers of validation to protect the server from malicious or accidental abuse. It first checks that at least one message exists and that the total message count doesn't exceed the maximum preventing excessively long conversation histories that could cause memory issues. Then it validates each individual message ensuring none are empty and none exceed the per-message character limit while also tracking total conversation length. The function validates that all message roles are either user or assistant rejecting invalid role values. Finally it validates optional generation parameters ensuring temperature, top_k, and max_tokens fall within acceptable ranges preventing extreme values that could cause poor generation quality or excessive computation. Each validation failure raises an HTTPException with a 400 status code and descriptive error message allowing clients to understand and fix their requests. This validation layer is crucial for production deployment where the server might face adversarial inputs or buggy clients.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n@asynccontextmanager\n"
    },
    {
      "type": "writeText",
      "content": "async def lifespan(app: FastAPI):\n",
      "highlight": true,
      "voiceover": "Manages the FastAPI application lifespan by initializing the worker pool on startup and cleaning up on shutdown. This async context manager is called by FastAPI when the server starts and stops providing hooks for resource initialization and cleanup. During startup it creates a WorkerPool with the specified number of GPUs and calls its initialize method to load models onto all devices. The worker pool is stored in app.state making it accessible to all request handlers. The yield statement marks the transition from startup to runtime where the server processes requests. After yield (during shutdown) any cleanup code would run though this implementation doesn't need explicit cleanup since PyTorch handles GPU memory deallocation automatically. This lifespan pattern ensures models are fully loaded before the server begins accepting requests preventing errors from requests arriving before initialization completes.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "app = FastAPI(lifespan=lifespan)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "app = FastAPI(lifespan=lifespan)",
      "voiceover": "Creates FastAPI application instance with lifespan context manager for model initialization on startup",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "app.add_middleware(",
      "voiceover": "Adds CORS middleware allowing cross-origin requests from any domain enabling browser-based clients to call the API",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@app.get(\"/\")\n"
    },
    {
      "type": "writeText",
      "content": "async def root():\n",
      "highlight": true,
      "voiceover": "Serves the chat UI HTML page as the root endpoint allowing users to interact with the model through a web interface. This handler reads the ui.html file from the nanochat directory and modifies it to use relative URLs instead of hardcoded localhost:8000 ensuring the UI works correctly when the server runs on different ports or hosts. The HTML content replacement changes the API_URL constant from an absolute URL to an empty string making the browser use the same origin for API calls. This approach allows the same HTML file to work in development and production without modification. The function returns an HTMLResponse with the modified content which the browser renders as the chat interface.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n@app.get(\"/logo.svg\")\n"
    },
    {
      "type": "writeText",
      "content": "async def logo():\n",
      "highlight": true,
      "voiceover": "Serves the NanoChat SVG logo file used as both the favicon and header image in the chat UI. This handler returns a FileResponse pointing to the logo.svg file in the nanochat directory with the correct MIME type for SVG images. Serving the logo through an API endpoint rather than embedding it in the HTML allows the browser to cache it separately and enables easy logo updates without modifying the HTML file. The SVG format ensures the logo scales cleanly at any size and loads quickly.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "async def generate_stream(\n    worker: Worker,\n    tokens,\n    temperature=None,\n    max_new_tokens=None,\n    top_k=None\n) -> AsyncGenerator[str, None]:\n    temperature = temperature if temperature is not None else args.temperature\n    max_new_tokens = max_new_tokens if max_new_tokens is not None else args.max_tokens\n    top_k = top_k if top_k is not None else args.top_k\n\n    assistant_end = worker.tokenizer.encode_special(\"<|assistant_end|>\")\n    bos = worker.tokenizer.get_bos_token_id()\n\n    accumulated_tokens = []\n    last_clean_text = \"\"\n\n    with worker.autocast_ctx:\n        for token_column, token_masks in worker.engine.generate(\n            tokens,\n            num_samples=1,\n            max_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            seed=random.randint(0, 2**31 - 1)\n        ):\n            token = token_column[0]\n\n            if token == assistant_end or token == bos:\n                break\n\n            accumulated_tokens.append(token)\n            current_text = worker.tokenizer.decode(accumulated_tokens)\n            if not current_text.endswith('�'):\n                new_text = current_text[len(last_clean_text):]\n                if new_text:\n                    yield f\"data: {json.dumps({'token': new_text, 'gpu': worker.gpu_id}, ensure_ascii=False)}\\n\\n\"\n                    last_clean_text = current_text\n\n    yield f\"data: {json.dumps({'done': True})}\\n\\n\"\n",
      "highlight": true,
      "voiceover": "Generates assistant response tokens one at a time yielding server-sent events for real-time streaming to the client. This async generator function wraps the model's token generation process handling the complexity of streaming partial UTF-8 sequences correctly. It uses the provided worker's engine to generate tokens with the specified sampling parameters falling back to server defaults when parameters aren't provided. The function implements sophisticated UTF-8 handling by accumulating tokens and only emitting text when it forms complete UTF-8 sequences without replacement characters ensuring multi-byte characters like emojis are transmitted correctly. Each generated token is decoded in the context of all previous tokens and only new text since the last clean decode is yielded preventing duplicate text in the stream. The function yields server-sent event formatted JSON containing the new text and the GPU ID for debugging. Generation stops when the assistant end token or BOS token is encountered or when max_new_tokens is reached. A random seed is used for each generation ensuring different outputs for the same input across requests. The final yield indicates completion allowing the client to know when to stop listening for events.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "if new_text:",
      "voiceover": "Only yield if there's new content",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n@app.post(\"/chat/completions\")\n"
    },
    {
      "type": "writeText",
      "content": "async def chat_completions(request: ChatRequest):\n\n    validate_chat_request(request)\n\n    logger.info(\"=\"*20)\n    for i, message in enumerate(request.messages):\n        logger.info(f\"[{message.role.upper()}]: {message.content}\")\n    logger.info(\"-\"*20)\n\n    worker_pool = app.state.worker_pool\n    worker = await worker_pool.acquire_worker()\n\n    try:\n        bos = worker.tokenizer.get_bos_token_id()\n        user_start = worker.tokenizer.encode_special(\"<|user_start|>\")\n        user_end = worker.tokenizer.encode_special(\"<|user_end|>\")\n        assistant_start = worker.tokenizer.encode_special(\"<|assistant_start|>\")\n        assistant_end = worker.tokenizer.encode_special(\"<|assistant_end|>\")\n\n        conversation_tokens = [bos]\n        for message in request.messages:\n            if message.role == \"user\":\n                conversation_tokens.append(user_start)\n                conversation_tokens.extend(worker.tokenizer.encode(message.content))\n                conversation_tokens.append(user_end)\n            elif message.role == \"assistant\":\n                conversation_tokens.append(assistant_start)\n                conversation_tokens.extend(worker.tokenizer.encode(message.content))\n                conversation_tokens.append(assistant_end)\n\n        conversation_tokens.append(assistant_start)\n\n        response_tokens = []\n        async def stream_and_release():\n            try:\n                async for chunk in generate_stream(\n                    worker,\n                    conversation_tokens,\n                    temperature=request.temperature,\n                    max_new_tokens=request.max_tokens,\n                    top_k=request.top_k\n                ):\n                    chunk_data = json.loads(chunk.replace(\"data: \", \"\").strip())\n                    if \"token\" in chunk_data:\n                        response_tokens.append(chunk_data[\"token\"])\n                    yield chunk\n            finally:\n                full_response = \"\".join(response_tokens)\n                logger.info(f\"[ASSISTANT] (GPU {worker.gpu_id}): {full_response}\")\n                logger.info(\"=\"*20)\n                await worker_pool.release_worker(worker)\n\n        return StreamingResponse(\n            stream_and_release(),\n            media_type=\"text/event-stream\"\n        )\n    except Exception as e:\n        await worker_pool.release_worker(worker)\n        raise e\n",
      "highlight": true,
      "voiceover": "Handles chat completion requests by acquiring a worker from the pool, building conversation context, and streaming the generated response. This endpoint is the main API for chat interactions implementing the full request-response cycle. It first validates the incoming request against abuse prevention limits then logs the conversation to the console for monitoring. The function acquires a worker from the pool which may block if all workers are busy providing natural backpressure. Once a worker is acquired it builds the conversation token sequence by encoding each message with appropriate role markers (user_start, user_end, assistant_start, assistant_end) and prepending a BOS token. The conversation tokens are then passed to generate_stream which yields tokens as they're generated. The function wraps the streaming in a nested async generator that accumulates response tokens for logging and ensures the worker is released back to the pool in a finally block even if an error occurs. The response is returned as a StreamingResponse with text/event-stream media type enabling server-sent events. The worker release happens after streaming completes ensuring the worker isn't returned to the pool while still generating preventing race conditions. The exception handling ensures workers are never leaked even when errors occur during generation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n@app.get(\"/health\")\n"
    },
    {
      "type": "writeText",
      "content": "async def health():\n    worker_pool = getattr(app.state, 'worker_pool', None)\n    return {\n        \"status\": \"ok\",\n        \"ready\": worker_pool is not None and len(worker_pool.workers) > 0,\n        \"num_gpus\": worker_pool.num_gpus if worker_pool else 0,\n        \"available_workers\": worker_pool.available_workers.qsize() if worker_pool else 0\n    }\n",
      "highlight": true,
      "voiceover": "Provides a health check endpoint returning server status and worker pool information for monitoring and load balancing. This endpoint returns a JSON object indicating whether the server is operational, whether the worker pool has been initialized and has workers available, the total number of GPUs configured, and the current number of available workers. The ready field is particularly useful for orchestration systems that need to know when the server has finished loading models and is ready to handle requests. The available_workers count helps monitoring systems detect when the server is under heavy load with all workers busy. This endpoint is designed to be lightweight and always return quickly even when all workers are busy making it suitable for frequent health checks from load balancers or monitoring systems.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n@app.get(\"/stats\")\n"
    },
    {
      "type": "writeText",
      "content": "async def stats():\n    worker_pool = app.state.worker_pool\n    return {\n        \"total_workers\": len(worker_pool.workers),\n        \"available_workers\": worker_pool.available_workers.qsize(),\n        \"busy_workers\": len(worker_pool.workers) - worker_pool.available_workers.qsize(),\n        \"workers\": [\n            {\n                \"gpu_id\": w.gpu_id,\n                \"device\": str(w.device)\n            } for w in worker_pool.workers\n        ]\n    }\n",
      "highlight": true,
      "voiceover": "Returns detailed worker pool statistics including total workers, availability, and per-worker information for debugging and monitoring. This endpoint provides more detailed information than the health check including the number of busy workers calculated by subtracting available workers from total workers, and a list of all workers with their GPU IDs and device strings. This information is useful for debugging worker pool behavior, understanding load distribution across GPUs, and diagnosing performance issues. The endpoint assumes the worker pool has been initialized and doesn't include the same safety checks as the health endpoint since it's intended for administrative use rather than automated health checks.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    import uvicorn\n    print(f\"Starting NanoChat Web Server\")\n    print(f\"Temperature: {args.temperature}, Top-k: {args.top_k}, Max tokens: {args.max_tokens}\")\n    uvicorn.run(app, host=args.host, port=args.port)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Entry point when script is run directly rather than imported as a module, starts the uvicorn server with configured host and port",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "import uvicorn",
      "voiceover": "ASGI server that runs the FastAPI application handling HTTP requests and websocket connections",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "print(f\"Starting NanoChat Web Server\")",
      "voiceover": "Logs server startup to console for user confirmation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "print(f\"Temperature: {args.temperature}, Top-k: {args.top_k}, Max tokens: {args.max_tokens}\")",
      "voiceover": "Displays default generation parameters so users know what settings are active",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/chat_web.py",
      "find": "uvicorn.run(app, host=args.host, port=args.port)",
      "voiceover": "Starts the uvicorn server with the FastAPI app instance binding to specified host and port for incoming HTTP requests",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/mid_train.py"
    },
    {
      "type": "writeText",
      "content": "\nfrom collections import deque\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport time\nimport wandb\nimport torch\nfrom contextlib import nullcontext\nfrom nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, get_base_dir, autodetect_device_type\nfrom nanochat.tokenizer import get_token_bytes\nfrom nanochat.checkpoint_manager import save_checkpoint\nfrom nanochat.loss_eval import evaluate_bpb\nfrom nanochat.checkpoint_manager import load_model\nimport torch.distributed as dist\n\nfrom tasks.common import TaskMixture\nfrom tasks.gsm8k import GSM8K\nfrom tasks.mmlu import MMLU\nfrom tasks.smoltalk import SmolTalk\nfrom tasks.customjson import CustomJSON\nfrom tasks.spellingbee import SimpleSpelling, SpellingBee\n\nrun = \"dummy\"\ndevice_type = \"\"\nmodel_tag = None\nstep = None\ndtype = \"bfloat16\"\nnum_iterations = -1\nmax_seq_len = 2048\ndevice_batch_size = 32\nunembedding_lr = 0.004\nembedding_lr = 0.2\nmatrix_lr = 0.02\ninit_lr_frac = 1.0\nweight_decay = 0.0\neval_every = 150\neval_tokens = 20*524288\ntotal_batch_size = 524288\ndry_run = 0\nconfig_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\nexec(open(os.path.join('nanochat', 'configurator.py')).read())\nuser_config = {k: globals()[k] for k in config_keys}\n\ndevice_type = autodetect_device_type() if device_type == \"\" else device_type\nddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\nmaster_process = ddp_rank == 0\nautocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\nsynchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\nget_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0\n\nuse_dummy_wandb = run == \"dummy\" or not master_process\nwandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-mid\", name=run, config=user_config)\n\nmodel, tokenizer, meta = load_model(\"base\", device, phase=\"train\", model_tag=model_tag, step=step)\npretrain_batch_size = meta.get(\"device_batch_size\", None)\nif pretrain_batch_size is not None and device_batch_size > pretrain_batch_size:\n    print0(f\"FOOTGUN WARNING: base model training used device_batch_size {pretrain_batch_size}, did you pass in a good --device_batch_size to this script?\")\norig_model = model\nmodel = torch.compile(model, dynamic=False)\ndepth = model.config.n_layer\nnum_flops_per_token = model.estimate_flops()\ntokens_per_fwdbwd = device_batch_size * max_seq_len\nworld_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size\nassert total_batch_size % world_tokens_per_fwdbwd == 0\ngrad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\nprint0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")\nprint0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\nprint0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")\ntoken_bytes = get_token_bytes(device=device)\n\noptimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\nadamw_optimizer, muon_optimizer = optimizers\nfor opt in optimizers:\n    for group in opt.param_groups:\n        group[\"lr\"] = group[\"lr\"] * init_lr_frac\n        group[\"initial_lr\"] = group[\"lr\"]\n\nbase_dir = get_base_dir()\nidentity_conversations_filepath = os.path.join(base_dir, \"identity_conversations.jsonl\")\ntrain_dataset = TaskMixture([\n    SmolTalk(split=\"train\"),\n    MMLU(subset=\"auxiliary_train\", split=\"train\"),\n    GSM8K(subset=\"main\", split=\"train\"),\n    CustomJSON(filepath=identity_conversations_filepath),\n    CustomJSON(filepath=identity_conversations_filepath),\n    SimpleSpelling(size=200000, split=\"train\"),\n    SpellingBee(size=80000, split=\"train\"),\n])\nval_dataset = TaskMixture([\n    SmolTalk(split=\"test\"),\n    MMLU(subset=\"all\", split=\"test\", stop=5200),\n    GSM8K(subset=\"main\", split=\"test\", stop=420),\n])\nlast_step = False\napprox_progress = 0.0\ndef mid_data_generator(split):\ntrain_loader = mid_data_generator(\"train\")\nbuild_val_loader = lambda: mid_data_generator(\"val\")\nprogress = 0\n    global last_step, approx_progress\n    assert split in {\"train\", \"val\"}, \"split must be 'train' or 'val'\"\n    dataset = train_dataset if split == \"train\" else val_dataset\n    dataset_size = len(dataset)\n    assert dataset_size > 0\n    needed_tokens = device_batch_size * max_seq_len + 1\n    token_buffer = deque()\n    scratch = torch.empty(needed_tokens, dtype=torch.int64, pin_memory=(device_type == \"cuda\"))\n    cursor = ddp_rank\n    it = 0\n    while True:\n        while len(token_buffer) < needed_tokens:\n            conversation = dataset[cursor]\n            ids, _ = tokenizer.render_conversation(conversation)\n            token_buffer.extend(ids)\n            cursor += ddp_world_size\n            if cursor >= dataset_size:\n                cursor -= dataset_size\n                if split == \"train\":\n                    last_step = True\n        it += 1\n        if 0 < num_iterations <= it and split == \"train\":\n            last_step = True\n        for i in range(needed_tokens):\n            scratch[i] = token_buffer.popleft()\n        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n        targets_cpu = scratch[1:]\n        inputs = inputs_cpu.view(device_batch_size, max_seq_len).to(device=device, dtype=torch.int32, non_blocking=True)\n        targets = targets_cpu.view(device_batch_size, max_seq_len).to(device=device, dtype=torch.int64, non_blocking=True)\n        if split == \"train\":\n            if num_iterations > 0:\n                approx_progress = it / num_iterations\n            else:\n                approx_progress = cursor / dataset_size\n        yield inputs, targets\n\ntrain_loader = mid_data_generator(\"train\")\nbuild_val_loader = lambda: mid_data_generator(\"val\")\nprogress = 0\n\ndef get_lr_multiplier(progress):\ndef get_muon_momentum(it):\nx, y = next(train_loader)\nmin_val_bpb = float(\"inf\")\nsmooth_train_loss = 0\nema_beta = 0.9\ntotal_training_time = 0\nstep = 0\nwhile True:\n    flops_so_far = num_flops_per_token * total_batch_size * step\n\n    if ddp:\n        last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)\n        dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)\n        last_step = bool(last_step_tensor.item())\n\n    if eval_every > 0 and (last_step or step % eval_every == 0):\n        model.eval()\n        val_loader = build_val_loader()\n        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n        with autocast_ctx:\n            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n        if val_bpb < min_val_bpb:\n            min_val_bpb = val_bpb\n        wandb_run.log({\n            \"step\": step,\n            \"total_training_flops\": flops_so_far,\n            \"total_training_time\": total_training_time,\n            \"val/bpb\": val_bpb,\n        })\n        model.train()\n\n    if master_process and last_step and not dry_run:\n        output_dirname = f\"d{depth}\"\n        checkpoint_dir = os.path.join(base_dir, \"mid_checkpoints\", output_dirname)\n        save_checkpoint(\n            checkpoint_dir,\n            step,\n            orig_model.state_dict(),\n            [opt.state_dict() for opt in optimizers],\n            {\n                \"step\": step,\n                \"val_bpb\": val_bpb,\n                \"model_config\": {\n                    \"sequence_len\": max_seq_len,\n                    \"vocab_size\": tokenizer.get_vocab_size(),\n                    \"n_layer\": depth,\n                    \"n_head\": model.config.n_head,\n                    \"n_kv_head\": model.config.n_kv_head,\n                    \"n_embd\": model.config.n_embd,\n                },\n                \"user_config\": user_config,\n            }\n        )\n\n    if last_step:\n        break\n\n    synchronize()\n    t0 = time.time()\n    for micro_step in range(grad_accum_steps):\n        with autocast_ctx:\n            loss = model(x, y)\n        train_loss = loss.detach()\n        loss = loss / grad_accum_steps\n        loss.backward()\n        x, y = next(train_loader)\n        progress = max(progress, approx_progress)\n    lrm = get_lr_multiplier(progress)\n    for opt in optimizers:\n        for group in opt.param_groups:\n            group[\"lr\"] = group[\"initial_lr\"] * lrm\n    muon_momentum = get_muon_momentum(step)\n    for group in muon_optimizer.param_groups:\n        group[\"momentum\"] = muon_momentum\n    for opt in optimizers:\n        opt.step()\n    model.zero_grad(set_to_none=True)\n    synchronize()\n    t1 = time.time()\n    dt = t1 - t0\n\n    step += 1\n\n    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item()\n    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))\n    pct_done = 100 * progress\n    tok_per_sec = int(total_batch_size / dt)\n    flops_per_sec = num_flops_per_token * total_batch_size / dt\n    promised_flops_per_sec_h100 = 989e12 * ddp_world_size\n    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100\n    if step > 10:\n        total_training_time += dt\n    print0(f\"step {step:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n    if step % 10 == 0:\n        wandb_run.log({\n            \"step\": step,\n            \"total_training_flops\": flops_so_far,\n            \"total_training_time\": total_training_time,\n            \"train/loss\": debiased_smooth_loss,\n            \"train/lrm\": lrm,\n            \"train/dt\": dt,\n            \"train/tok_per_sec\": tok_per_sec,\n            \"train/mfu\": mfu,\n        })\n\nprint0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")\nprint0(f\"Total training time: {total_training_time/60:.2f}m\")\nprint0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")\n\nif not dry_run:\n    from nanochat.report import get_report\n    get_report().log(section=\"Midtraining\", data=[\n        user_config,\n        {\n            \"Number of iterations\": step,\n            \"DDP world size\": ddp_world_size,\n        },\n        {\n            \"Minimum validation bpb\": min_val_bpb,\n        }\n    ])\n\nwandb_run.finish()\ncompute_cleanup()\n\n"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from collections import deque",
      "voiceover": "Double-ended queue for efficient token buffering allowing fast append and pop from both ends",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "import os",
      "voiceover": "Operating system interface for file paths and environment variable configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"",
      "voiceover": "Configure PyTorch CUDA memory allocator to use expandable segments for better memory management",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "import time",
      "voiceover": "Time measurement for calculating throughput and training duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "import wandb",
      "voiceover": "Weights & Biases for experiment tracking and logging training metrics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "import torch",
      "voiceover": "PyTorch deep learning framework for model training and tensor operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from contextlib import nullcontext",
      "voiceover": "No-op context manager used when autocast is disabled on non-CUDA devices",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.common import compute_init, compute_cleanup, print0, DummyWandb, get_base_dir, autodetect_device_type",
      "voiceover": "Common utilities for distributed computing, logging, and device detection",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.tokenizer import get_token_bytes",
      "voiceover": "Utility to get token byte mapping for bits-per-byte calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.checkpoint_manager import save_checkpoint",
      "voiceover": "Checkpoint saving utility for persisting model state",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.loss_eval import evaluate_bpb",
      "voiceover": "Evaluation function that computes bits-per-byte on validation data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.checkpoint_manager import load_model",
      "voiceover": "Checkpoint loading utility that loads base model weights",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "import torch.distributed as dist",
      "voiceover": "PyTorch distributed training utilities for multi-GPU synchronization",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.common import TaskMixture",
      "voiceover": "Utility to mix multiple datasets for training with automatic balancing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.gsm8k import GSM8K",
      "voiceover": "GSM8K grade school math benchmark dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.mmlu import MMLU",
      "voiceover": "MMLU (Massive Multitask Language Understanding) benchmark dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.smoltalk import SmolTalk",
      "voiceover": "SmolTalk conversational dataset for general chat ability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.customjson import CustomJSON",
      "voiceover": "Custom JSONL dataset loader for identity conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from tasks.spellingbee import SimpleSpelling, SpellingBee",
      "voiceover": "Spelling task datasets for character-level understanding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "run = \"dummy\"",
      "voiceover": "Wandb run name (\"dummy\" disables wandb logging to avoid cluttering workspace during development)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "device_type = \"\"",
      "voiceover": "Device type: cuda|cpu|mps (empty string triggers auto-detection based on available hardware)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model_tag = None",
      "voiceover": "Optional model tag identifier for loading specific base model variant from checkpoint directory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "step = None",
      "voiceover": "Optional checkpoint step number to resume training from specific iteration of base model",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "dtype = \"bfloat16\"",
      "voiceover": "Precision for model training and inference affecting memory usage and numerical stability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "num_iterations = -1",
      "voiceover": "Explicit number of optimization steps to run (-1 means train for one full epoch through dataset)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "max_seq_len = 2048",
      "voiceover": "Maximum sequence length for training affecting context window and memory requirements",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "device_batch_size = 32",
      "voiceover": "Batch size per device determining memory usage and gradient noise characteristics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "unembedding_lr = 0.004",
      "voiceover": "Learning rate for output projection layer (AdamW optimizer) controlling final layer adaptation speed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "embedding_lr = 0.2",
      "voiceover": "Learning rate for embedding layer (AdamW optimizer) controlling token representation learning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "matrix_lr = 0.02",
      "voiceover": "Learning rate for linear layer weight matrices (Muon optimizer) controlling transformer layer updates",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "init_lr_frac = 1.0",
      "voiceover": "Initial learning rate as fraction of base LR (1.0 means start at full learning rate without warmup)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "weight_decay = 0.0",
      "voiceover": "Weight decay for embedding/unembedding parameters (AdamW optimizer) controlling regularization strength",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "eval_every = 150",
      "voiceover": "Evaluate validation bits-per-byte every N steps (-1 disables periodic evaluation)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "eval_tokens = 20*524288",
      "voiceover": "Number of tokens to use for validation evaluation determining evaluation thoroughness",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "total_batch_size = 524288",
      "voiceover": "Total batch size across all devices and gradient accumulation steps controlling optimization stability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "dry_run = 0",
      "voiceover": "Dry run mode (1 = log to wandb but skip checkpoints and reports for testing configuration)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]",
      "voiceover": "Extract all configuration variable names for CLI override and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "exec(open(os.path.join('nanochat', 'configurator.py')).read())",
      "voiceover": "Execute configurator to allow command-line and config file overrides of hyperparameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "user_config = {k: globals()[k] for k in config_keys}",
      "voiceover": "Capture final configuration values after CLI overrides for logging to wandb and checkpoints",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "device_type = autodetect_device_type() if device_type == \"\" else device_type",
      "voiceover": "Determines device type either from command-line argument or auto-detection for model loading and training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)",
      "voiceover": "Initializes distributed environment returning rank info and device object for multi-GPU training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "master_process = ddp_rank == 0",
      "voiceover": "Master process (rank 0) handles checkpointing and logging while other ranks only train",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()",
      "voiceover": "Enable mixed precision training on CUDA for faster computation and lower memory usage",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None",
      "voiceover": "Function to synchronize CUDA operations for accurate timing measurements or no-op on CPU",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0",
      "voiceover": "Function to get peak GPU memory usage for profiling or returns 0 on CPU",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "use_dummy_wandb = run == \"dummy\" or not master_process",
      "voiceover": "Disable wandb if run is \"dummy\" or not master process to avoid duplicate logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat-mid\", name=run, config=user_config)",
      "voiceover": "Initialize wandb or dummy logger for experiment tracking",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model, tokenizer, meta = load_model(\"base\", device, phase=\"train\", model_tag=model_tag, step=step)",
      "voiceover": "Load base model checkpoint and tokenizer onto device for midtraining continuation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "pretrain_batch_size = meta.get(\"device_batch_size\", None)",
      "voiceover": "Extract device batch size used during pretraining from checkpoint metadata for validation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if pretrain_batch_size is not None and device_batch_size > pretrain_batch_size:",
      "voiceover": "Check if current batch size exceeds pretraining batch size which may cause instability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"FOOTGUN WARNING: base model training used device_batch_size {pretrain_batch_size}, did you pass in a good --device_batch_size to this script?\")",
      "voiceover": "Warn user about potential training instability from batch size mismatch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "orig_model = model",
      "voiceover": "Keep reference to original uncompiled model for checkpoint saving since compiled models can't be serialized",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model = torch.compile(model, dynamic=False)",
      "voiceover": "Compile model with static shapes for faster execution through graph optimization and kernel fusion",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "depth = model.config.n_layer",
      "voiceover": "Extract model depth from config for checkpoint directory naming and logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "num_flops_per_token = model.estimate_flops()",
      "voiceover": "Calculate FLOPs per token for MFU (model FLOPs utilization) calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "tokens_per_fwdbwd = device_batch_size * max_seq_len",
      "voiceover": "Tokens processed per forward-backward pass on single rank determining per-device throughput",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size",
      "voiceover": "Total tokens processed per iteration across all ranks for gradient accumulation calculation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "assert total_batch_size % world_tokens_per_fwdbwd == 0",
      "voiceover": "Ensure total batch size is divisible by world tokens for clean gradient accumulation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd",
      "voiceover": "Calculate gradient accumulation steps needed to achieve target batch size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")",
      "voiceover": "Log per-rank batch size for debugging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")",
      "voiceover": "Log total tokens per micro-batch across all ranks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")",
      "voiceover": "Log gradient accumulation configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "token_bytes = get_token_bytes(device=device)",
      "voiceover": "Get token byte mapping for bits-per-byte calculation during evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)",
      "voiceover": "Create optimizers (AdamW for embeddings, Muon for linear layers) with specified learning rates",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "adamw_optimizer, muon_optimizer = optimizers",
      "voiceover": "Unpack optimizers for separate momentum scheduling on Muon",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for opt in optimizers:",
      "voiceover": "Set initial learning rate as fraction of base LR for both optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group in optimizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "group[\"lr\"] = group[\"lr\"] * init_lr_frac",
      "voiceover": "Scale down learning rate by init_lr_frac",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "group[\"initial_lr\"] = group[\"lr\"]",
      "voiceover": "Save scaled LR as initial LR for scheduler to reference",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get base directory for checkpoint and data file paths",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "identity_conversations_filepath = os.path.join(base_dir, \"identity_conversations.jsonl\")",
      "voiceover": "Path to synthetic identity conversations teaching model about itself",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "train_dataset = TaskMixture([",
      "voiceover": "Create training dataset mixture from multiple tasks with automatic balancing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "SmolTalk(split=\"train\"),",
      "voiceover": "460K rows of general conversations for chat ability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "MMLU(subset=\"auxiliary_train\", split=\"train\"),",
      "voiceover": "100K rows of multiple choice problems from various benchmarks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "GSM8K(subset=\"main\", split=\"train\"),",
      "voiceover": "8K rows teaching math and calculator tool use",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "CustomJSON(filepath=identity_conversations_filepath),",
      "voiceover": "1000 rows of synthetic identity conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "CustomJSON(filepath=identity_conversations_filepath),",
      "voiceover": "Duplicate for 2 epochs of identity data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "SimpleSpelling(size=200000, split=\"train\"),",
      "voiceover": "200K rows of simple spelling tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "SpellingBee(size=80000, split=\"train\"),",
      "voiceover": "80K rows of spelling bee character counting tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "])",
      "voiceover": "Total: 460K + 100K + 8K + 200K + 80K = 848K rows",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "val_dataset = TaskMixture([",
      "voiceover": "Create validation dataset mixture matching train distribution",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "SmolTalk(split=\"test\"),",
      "voiceover": "24K rows in test set",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "MMLU(subset=\"all\", split=\"test\", stop=5200),",
      "voiceover": "14K rows in test set, use only 5.2K to match train ratios",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "GSM8K(subset=\"main\", split=\"test\", stop=420),",
      "voiceover": "1.32K rows in test set, use only 420 to match train ratios",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "])",
      "voiceover": "Total: 24K + 14K + 1.32K ~= 39K rows",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "last_step = False",
      "voiceover": "Global flag toggled to True when training should terminate either from epoch completion or iteration limit",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "approx_progress = 0.0",
      "voiceover": "Global progress tracker updated by data generator going from 0 to 1 over training for learning rate scheduling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "train_loader = mid_data_generator(\"train\")",
      "voiceover": "Create training data generator that yields batches indefinitely",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "build_val_loader = lambda: mid_data_generator(\"val\")",
      "voiceover": "Lambda to create validation data generator on demand for evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "progress = 0",
      "voiceover": "Training progress tracker going from 0 to 1 used for learning rate scheduling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "needed_tokens = device_batch_size * max_seq_len + 1",
      "voiceover": "to form one training batch of inputs,targets",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "cursor = ddp_rank",
      "voiceover": "increments by ddp_world_size each time, so each rank processes unique documents",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "it = 0",
      "voiceover": "iteration counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "cursor -= dataset_size",
      "voiceover": "wrap around for another epoch",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "last_step = True",
      "voiceover": "toggle last_step to True, which will terminate the training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "last_step = True",
      "voiceover": "toggle last_step to True, which will terminate the training loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "approx_progress = it / num_iterations",
      "voiceover": "calculate progress from the max number of iterations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "approx_progress = cursor / dataset_size",
      "voiceover": "approximate progress as a fraction of the dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "train_loader = mid_data_generator(\"train\")",
      "voiceover": "Create training data generator that yields batches indefinitely",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "build_val_loader = lambda: mid_data_generator(\"val\")",
      "voiceover": "Lambda to create validation data generator on demand for evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "progress = 0",
      "voiceover": "Training progress tracker going from 0 to 1 used for learning rate scheduling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "x, y = next(train_loader)",
      "voiceover": "Prefetch first batch of data before loop starts to overlap data loading with GPU computation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "min_val_bpb = float(\"inf\")",
      "voiceover": "Track minimum validation bits-per-byte achieved for logging best model performance",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "smooth_train_loss = 0",
      "voiceover": "Exponential moving average of training loss for smoother logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "ema_beta = 0.9",
      "voiceover": "EMA decay factor controlling smoothness (0.9 means 90% old value, 10% new value)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "total_training_time = 0",
      "voiceover": "Cumulative wall-clock training time excluding first 10 steps for accurate throughput measurement",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "step = 0",
      "voiceover": "Training step counter starting at 0 and incrementing after each optimization step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "while True:",
      "voiceover": "Infinite loop terminated by last_step flag when epoch completes or iteration limit reached",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "flops_so_far = num_flops_per_token * total_batch_size * step",
      "voiceover": "Total FLOPs computed so far for tracking computational cost",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if ddp:",
      "voiceover": "Synchronize last_step flag across all ranks to prevent hangs in distributed training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)",
      "voiceover": "Convert boolean to tensor for all_reduce",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)",
      "voiceover": "Use MAX to ensure if any rank sets last_step all ranks see it",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "last_step = bool(last_step_tensor.item())",
      "voiceover": "Convert back to boolean for conditional checks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if eval_every > 0 and (last_step or step % eval_every == 0):",
      "voiceover": "Evaluate validation bits-per-byte periodically and at end",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model.eval()",
      "voiceover": "Set model to evaluation mode disabling dropout and other training-specific behavior",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "val_loader = build_val_loader()",
      "voiceover": "Create fresh validation data generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)",
      "voiceover": "Calculate steps needed to process eval_tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision for evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)",
      "voiceover": "Compute validation bits-per-byte",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")",
      "voiceover": "Log validation metric",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if val_bpb < min_val_bpb:",
      "voiceover": "Update minimum validation bpb if current is better",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "min_val_bpb = val_bpb",
      "voiceover": "Track best validation performance",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "wandb_run.log({",
      "voiceover": "Log validation metrics to wandb",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model.train()",
      "voiceover": "Set model back to training mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if master_process and last_step and not dry_run:",
      "voiceover": "Save checkpoint at end of training only on master process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "output_dirname = f\"d{depth}\"",
      "voiceover": "Create directory name based on model depth",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "checkpoint_dir = os.path.join(base_dir, \"mid_checkpoints\", output_dirname)",
      "voiceover": "Construct checkpoint directory path",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "save_checkpoint(",
      "voiceover": "Save model checkpoint with metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "orig_model.state_dict(),",
      "voiceover": "Use original uncompiled model for serialization",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "[opt.state_dict() for opt in optimizers],",
      "voiceover": "Save optimizer states for potential resumption",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "{",
      "voiceover": "Metadata dictionary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "\"val_bpb\": val_bpb,",
      "voiceover": "Loss at last step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "\"model_config\": {",
      "voiceover": "Model architecture configuration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "\"user_config\": user_config,",
      "voiceover": "Training hyperparameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if last_step:",
      "voiceover": "Exit training loop when epoch completes or iteration limit reached",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "break",
      "voiceover": "Terminate infinite loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "synchronize()",
      "voiceover": "Synchronize CUDA operations before timing for accurate measurement",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "t0 = time.time()",
      "voiceover": "Start timing training step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for micro_step in range(grad_accum_steps):",
      "voiceover": "Gradient accumulation loop",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "with autocast_ctx:",
      "voiceover": "Use mixed precision",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "loss = model(x, y)",
      "voiceover": "Forward pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "train_loss = loss.detach()",
      "voiceover": "Detach loss for logging without gradient tracking",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "loss = loss / grad_accum_steps",
      "voiceover": "Normalize loss by accumulation steps since .backward() sums gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "loss.backward()",
      "voiceover": "Backward pass accumulating gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "x, y = next(train_loader)",
      "voiceover": "Prefetch next batch while GPU is busy with backward pass",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "progress = max(progress, approx_progress)",
      "voiceover": "Update progress monotonically from data generator",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "lrm = get_lr_multiplier(progress)",
      "voiceover": "Get learning rate multiplier based on progress",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for opt in optimizers:",
      "voiceover": "Update learning rate for all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for group in opt.param_groups:",
      "voiceover": "For each parameter group",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "group[\"lr\"] = group[\"initial_lr\"] * lrm",
      "voiceover": "Apply learning rate multiplier",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "muon_momentum = get_muon_momentum(step)",
      "voiceover": "Get momentum value for current step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for group in muon_optimizer.param_groups:",
      "voiceover": "Update momentum for Muon optimizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "group[\"momentum\"] = muon_momentum",
      "voiceover": "Apply momentum schedule",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "for opt in optimizers:",
      "voiceover": "Step all optimizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "opt.step()",
      "voiceover": "Update parameters using accumulated gradients",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "model.zero_grad(set_to_none=True)",
      "voiceover": "Clear gradients setting to None for memory efficiency",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "synchronize()",
      "voiceover": "Synchronize CUDA operations after step for accurate timing",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "t1 = time.time()",
      "voiceover": "End timing training step",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "dt = t1 - t0",
      "voiceover": "Calculate step duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "step += 1",
      "voiceover": "Increment step counter",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item()",
      "voiceover": "Update EMA of training loss",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))",
      "voiceover": "Debias EMA to account for initialization at zero",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "pct_done = 100 * progress",
      "voiceover": "Convert progress to percentage for logging",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "tok_per_sec = int(total_batch_size / dt)",
      "voiceover": "Calculate tokens per second throughput",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "flops_per_sec = num_flops_per_token * total_batch_size / dt",
      "voiceover": "Calculate FLOPs per second",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "promised_flops_per_sec_h100 = 989e12 * ddp_world_size",
      "voiceover": "Theoretical peak FLOPs for H100 SXM in bfloat16 without sparsity",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "mfu = 100 * flops_per_sec / promised_flops_per_sec_h100",
      "voiceover": "Model FLOPs utilization as percentage of theoretical peak",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if step > 10:",
      "voiceover": "Only count time after first 10 steps to exclude compilation and warmup overhead",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "total_training_time += dt",
      "voiceover": "Accumulate training time",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"step {step:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")",
      "voiceover": "Log training progress",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if step % 10 == 0:",
      "voiceover": "Log to wandb every 10 steps to reduce logging overhead",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "wandb_run.log({",
      "voiceover": "Log training metrics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB\")",
      "voiceover": "Log peak GPU memory usage for profiling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Total training time: {total_training_time/60:.2f}m\")",
      "voiceover": "Log total training duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "print0(f\"Minimum validation bpb: {min_val_bpb:.4f}\")",
      "voiceover": "Log best validation performance achieved",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "if not dry_run:",
      "voiceover": "Skip report logging in dry run mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "get_report().log(section=\"Midtraining\", data=[",
      "voiceover": "Log training summary to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "user_config,",
      "voiceover": "CLI arguments",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "{",
      "voiceover": "Training setup statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "{",
      "voiceover": "Training outcome statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "wandb_run.finish()",
      "voiceover": "Finalize wandb run and upload any remaining logs",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/mid_train.py",
      "find": "compute_cleanup()",
      "voiceover": "Clean up distributed computing resources",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "scripts/tok_eval.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.tokenizer import get_tokenizer, RustBPETokenizer\nfrom nanochat.dataset import parquets_iter_batched\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "from nanochat.tokenizer import get_tokenizer, RustBPETokenizer",
      "voiceover": "Tokenizer utilities for loading custom and pretrained tokenizers for compression comparison",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "from nanochat.dataset import parquets_iter_batched",
      "voiceover": "Dataset iterator for loading training and validation data that the tokenizer was trained on",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "news_text = r\"\"\"\n(Washington, D.C., July 9, 2025)- Yesterday, Mexico’s National Service of Agro-Alimentary Health, Safety, and Quality (SENASICA) reported a new case of New World Screwworm (NWS) in Ixhuatlan de Madero, Veracruz in Mexico, which is approximately 160 miles northward of the current sterile fly dispersal grid, on the eastern side of the country and 370 miles south of the U.S./Mexico border. This new northward detection comes approximately two months after northern detections were reported in Oaxaca and Veracruz, less than 700 miles away from the U.S. border, which triggered the closure of our ports to Mexican cattle, bison, and horses on May 11, 2025.\n\nWhile USDA announced a risk-based phased port re-opening strategy for cattle, bison, and equine from Mexico beginning as early as July 7, 2025, this newly reported NWS case raises significant concern about the previously reported information shared by Mexican officials and severely compromises the outlined port reopening schedule of five ports from July 7-September 15. Therefore, in order to protect American livestock and our nation’s food supply, Secretary Rollins has ordered the closure of livestock trade through southern ports of entry effective immediately.\n\n“The United States has promised to be vigilant — and after detecting this new NWS case, we are pausing the planned port reopening’s to further quarantine and target this deadly pest in Mexico. We must see additional progress combatting NWS in Veracruz and other nearby Mexican states in order to reopen livestock ports along the Southern border,” said U.S. Secretary of Agriculture Brooke L. Rollins. “Thanks to the aggressive monitoring by USDA staff in the U.S. and in Mexico, we have been able to take quick and decisive action to respond to the spread of this deadly pest.”"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "news_text = r\"\"\"",
      "voiceover": "Random text I got from a random website this morning",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "korean_text = r\"\"\"\n정직한 사실 위에, 공정한 시선을 더하다\nHerald Korea Times\n\n헤럴드코리아타임즈는 정치, 경제, 사회, 문화 등 한국 사회 전반의 주요 이슈를 심도 있게 다루는 종합 온라인 신문사입니다.\n\n우리는 단순히 뉴스를 전달하는 것이 아니라, 사실(Fact)에 기반한 양측의 시각을 균형 있게 조명하며, 독자 여러분이 스스로 판단할 수 있는 ‘정보의 균형’을 제공합니다.\n\n한국 언론의 오랜 문제로 지적되어 온 정치적 편향, 이념적 왜곡에서 벗어나\n오직 정직함과 공정함을 원칙으로 삼는 언론을 지향합니다.\n어느 한쪽의 주장만을 확대하거나 감추지 않고,\n**모든 쟁점에 대해 ‘무엇이 쟁점인지’, ‘누가 무엇을 주장하는지’, ‘사실은 무엇인지’**를 명확히 전달하는 데 집중합니다."
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "korean_text = r\"\"\"",
      "voiceover": "Random Korean text (to test non-English compression)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "code_text = r\"\"\"\nclass BasicTokenizer(Tokenizer):\n\n    def __init__(self):\n        super().__init__()\n\n    def train(self, text, vocab_size, verbose=False):\n        assert vocab_size >= 256\n        num_merges = vocab_size - 256\n\n        text_bytes = text.encode(\"utf-8\")\n        ids = list(text_bytes)\n\n        merges = {}\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for i in range(num_merges):\n            stats = get_stats(ids)\n            pair = max(stats, key=stats.get)\n            idx = 256 + i\n            ids = merge(ids, pair, idx)\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            if verbose:\n                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "code_text = r\"\"\"",
      "voiceover": "Random piece of code",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "math_text = r\"\"\"\n\\documentclass[12pt]{article}\n\\usepackage{amsmath,amsthm,amssymb}\n\\usepackage[margin=1in]{geometry}\n\n\\newtheorem{theorem}{Theorem}\n\\newtheorem*{remark}{Remark}\n\n\\begin{document}\n\n\\begin{center}\n{\\Large A Cute Identity: The Sum of Cubes is a Square}\n\\end{center}\n\n\\begin{theorem}\nFor every integer $n \\ge 1$,\n\\[\n\\sum_{k=1}^{n} k^{3} \\;=\\; \\left(\\frac{n(n+1)}{2}\\right)^{2}.\n\\]\n\\end{theorem}\n\n\\begin{proof}[Proof 1 (Induction)]\nLet $S(n) = \\sum_{k=1}^{n} k^3$. For $n=1$, $S(1)=1=(1\\cdot 2/2)^2$, so the base case holds.\n\nAssume $S(n)=\\big(\\tfrac{n(n+1)}{2}\\big)^2$ for some $n\\ge 1$.\nThen\n\\[\nS(n+1)\n= S(n) + (n+1)^3\n= \\left(\\frac{n(n+1)}{2}\\right)^2 + (n+1)^3.\n\\]\nFactor out $(n+1)^2$:\n\\[\nS(n+1)\n= (n+1)^2\\left( \\frac{n^2}{4} + (n+1) \\right)\n= (n+1)^2\\left( \\frac{n^2 + 4n + 4}{4} \\right)\n= (n+1)^2\\left( \\frac{(n+2)^2}{4} \\right).\n\\]\nThus\n\\[\nS(n+1)=\\left(\\frac{(n+1)(n+2)}{2}\\right)^2,\n\\]\nwhich matches the claimed formula with $n$ replaced by $n+1$. By induction, the identity holds for all $n\\ge 1$.\n\\end{proof}\n\n\\begin{proof}[Proof 2 (Algebraic telescoping)]\nRecall the binomial identity\n\\[\n(k+1)^4 - k^4 = 4k^3 + 6k^2 + 4k + 1.\n\\]\nSumming both sides from $k=0$ to $n$ telescopes:\n\\[\n(n+1)^4 - 0^4\n= \\sum_{k=0}^{n}\\big(4k^3 + 6k^2 + 4k + 1\\big)\n= 4\\sum_{k=1}^{n}k^3 + 6\\sum_{k=1}^{n}k^2 + 4\\sum_{k=1}^{n}k + (n+1).\n\\]\nUsing the standard sums\n\\[\n\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\n\\quad\\text{and}\\quad\n\\sum_{k=1}^{n}k^2 = \\frac{n(n+1)(2n+1)}{6},\n\\]\nsolve for $\\sum_{k=1}^{n}k^3$ to get\n\\[\n\\sum_{k=1}^{n}k^3 = \\left(\\frac{n(n+1)}{2}\\right)^2.\n\\]\n\\end{proof}\n\n\\begin{remark}\nGeometrically, the identity says: ``adding up $1^3,2^3,\\dots,n^3$ builds a perfect square’’—namely the square of the $n$th triangular number. This is why one sometimes calls it the \\emph{sum-of-cubes is a square} phenomenon.\n\\end{remark}\n\n\\end{document}"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "science_text = r\"\"\"\nPhotosynthesis is a photochemical energy transduction process in which light-harvesting pigment–protein complexes within the thylakoid membranes of oxygenic phototrophs absorb photons and initiate charge separation at the reaction center, driving the linear electron transport chain from water to NADP⁺ via photosystem II, the cytochrome b₆f complex, and photosystem I, concomitantly generating a trans-thylakoid proton motive force utilized by chloroplastic ATP synthase. The light-dependent reactions produce ATP and NADPH, which fuel the Calvin–Benson–Bassham cycle in the stroma, wherein ribulose-1,5-bisphosphate is carboxylated by ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) to form 3-phosphoglycerate, subsequently reduced and regenerated through a series of enzymatic steps, enabling net assimilation of CO₂ into triose phosphates and ultimately carbohydrates. This process is tightly regulated by photoprotective mechanisms, redox feedback, and metabolite flux, representing a central biochemical pathway coupling solar energy capture to the biosphere’s primary productivity."
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "train_docs = next(parquets_iter_batched(split=\"train\"))\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "train_docs = next(parquets_iter_batched(split=\"train\"))",
      "voiceover": "The tokenizer was trained on data from earlier shards, so it has seen this data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "train_text = \"\\n\".join(train_docs)\n"
    },
    {
      "type": "writeText",
      "content": "val_docs = next(parquets_iter_batched(split=\"val\"))\n"
    },
    {
      "type": "writeText",
      "content": "val_text = \"\\n\".join(val_docs)\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "all_text = [\n    (\"news\", news_text),\n    (\"korean\", korean_text),\n    (\"code\", code_text),\n    (\"math\", math_text),\n    (\"science\", science_text),\n    (\"fwe-train\", train_text),\n]\n"
    },
    {
      "type": "writeText",
      "content": "if val_text:\n    all_text.append((\"fwe-val\", val_text))\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "tokenizer_results = {}\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "tokenizer_results = {}",
      "voiceover": "Try out current default compared to GPT-2 and GPT-4 tokenizers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "vocab_sizes = {}\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "for tokenizer_name in [\"gpt2\", \"gpt4\", \"ours\"]:\n\n    if tokenizer_name == \"gpt2\":\n        tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\")\n    elif tokenizer_name == \"gpt4\":\n        tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\")\n    else:\n        tokenizer = get_tokenizer()\n\n    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n    tokenizer_results[tokenizer_name] = {}\n\n    for name, text in all_text:\n        encoded = tokenizer.encode(text)\n        decoded = tokenizer.decode(encoded)\n        assert decoded == text\n\n        encoded_bytes = text.encode('utf-8')\n        ratio = len(encoded_bytes) / len(encoded)\n        tokenizer_results[tokenizer_name][name] = {\n            'bytes': len(encoded_bytes),\n            'tokens': len(encoded),\n            'ratio': ratio\n        }\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\")",
      "voiceover": "gpt-2 base model tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\")",
      "voiceover": "gpt-4 base model tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "GREEN = '\\033[92m'\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "GREEN = '\\033[92m'",
      "voiceover": "ANSI color codes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "RED = '\\033[91m'\n"
    },
    {
      "type": "writeText",
      "content": "RESET = '\\033[0m'\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "print(f\"\\nVocab sizes:\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "print(f\"\\nVocab sizes:\")",
      "voiceover": "Print vocab sizes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"GPT-2: {vocab_sizes['gpt2']}\")\n"
    },
    {
      "type": "writeText",
      "content": "print(f\"GPT-4: {vocab_sizes['gpt4']}\")\n"
    },
    {
      "type": "writeText",
      "content": "print(f\"Ours: {vocab_sizes['ours']}\")\n"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def print_comparison(baseline_name, baseline_results, ours_results, all_text):\n    print(f\"\\nComparison with {baseline_name}:\")\n    print(\"=\" * 95)\n    print(f\"{'Text Type':<10} {'Bytes':<8} {baseline_name:<15} {'Ours':<15} {'Relative':<12} {'Better':<10}\")\n    print(f\"{'':10} {'':8} {'Tokens':<7} {'Ratio':<7} {'Tokens':<7} {'Ratio':<7} {'Diff %':<12}\")\n    print(\"-\" * 95)\n\n    for name, text in all_text:\n        baseline_data = baseline_results[name]\n        ours_data = ours_results[name]\n\n        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n\n        if baseline_data['ratio'] > ours_data['ratio']:\n            baseline_color, ours_color = GREEN, RED\n            better = baseline_name\n            diff_color = RED\n        elif ours_data['ratio'] > baseline_data['ratio']:\n            baseline_color, ours_color = RED, GREEN\n            better = \"Ours\"\n            diff_color = GREEN\n        else:\n            baseline_color, ours_color = \"\", \"\"\n            better = \"Tie\"\n            diff_color = \"\"\n\n        print(f\"{name:<10} {baseline_data['bytes']:<8} \"\n              f\"{baseline_color}{baseline_data['tokens']:<7}{RESET} \"\n              f\"{baseline_color}{baseline_data['ratio']:<7.2f}{RESET} \"\n              f\"{ours_color}{ours_data['tokens']:<7}{RESET} \"\n              f\"{ours_color}{ours_data['ratio']:<7.2f}{RESET} \"\n              f\"{diff_color}{relative_diff:+7.1f}%{RESET}     \"\n              f\"{better:<10}\")\n",
      "highlight": true,
      "voiceover": "Evaluate compression ratio of the tokenizer.\n\nPrint comparison table between baseline tokenizer and ours.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)",
      "voiceover": "Print comparisons",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print_comparison(\"GPT-4\", tokenizer_results['gpt4'], tokenizer_results['ours'], all_text)\n"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_eval.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Log to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "lines = []\n"
    },
    {
      "type": "writeText",
      "content": "for baseline_name in [\"GPT-2\", \"GPT-4\"]:\n    baseline_key = baseline_name.lower().replace('-', '')\n    baseline_results = tokenizer_results[baseline_key]\n    ours_results = tokenizer_results['ours']\n    lines.append(f\"\n    lines.append(\"\")\n    lines.append(\"| Text Type | Bytes | \" + baseline_name + \" Tokens | \" + baseline_name + \" Ratio | Ours Tokens | Ours Ratio | Relative Diff % |\")\n    lines.append(\"|-----------|-------|--------------|--------------|-------------|------------|-----------------|\")\n    for name, text in all_text:\n        baseline_data = baseline_results[name]\n        ours_data = ours_results[name]\n        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n        lines.append(f\"| {name} | {baseline_data['bytes']} | {baseline_data['tokens']} | {baseline_data['ratio']:.2f} | {ours_data['tokens']} | {ours_data['ratio']:.2f} | {relative_diff:+.1f}% |\")\n    lines.append(\"\")\n"
    },
    {
      "type": "writeText",
      "content": "report_markdown = \"\\n\".join(lines)\n"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Tokenizer evaluation\", data=[\n    report_markdown,\n])\n"
    },
    {
      "type": "openFile",
      "path": "scripts/tok_train.py"
    },
    {
      "type": "writeText",
      "content": "import os\nimport time\nimport argparse\nimport torch\nfrom nanochat.tokenizer import RustBPETokenizer\nfrom nanochat.common import get_base_dir\nfrom nanochat.dataset import parquets_iter_batched\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "import os",
      "voiceover": "Operating system interface for file path operations when saving tokenizer and metadata",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "import time",
      "voiceover": "Time measurement for tracking tokenizer training duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "import argparse",
      "voiceover": "Command-line argument parsing for configuring training parameters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "import torch",
      "voiceover": "PyTorch framework for creating and saving token byte mapping tensor",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "from nanochat.tokenizer import RustBPETokenizer",
      "voiceover": "BPE tokenizer implementation using Rust backend for efficient training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "from nanochat.common import get_base_dir",
      "voiceover": "Utility to get base directory for saving tokenizer files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "from nanochat.dataset import parquets_iter_batched",
      "voiceover": "Dataset iterator for loading training data in batches from parquet files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "parser = argparse.ArgumentParser(description='Train a BPE tokenizer')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "parser = argparse.ArgumentParser(description='Train a BPE tokenizer')",
      "voiceover": "Creates argument parser for configuring tokenizer training via command-line flags",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--max_chars', type=int, default=10_000_000_000, help='Maximum characters to train on (default: 10B)')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "parser.add_argument('--max_chars', type=int, default=10_000_000_000, help='Maximum characters to train on (default: 10B)')",
      "voiceover": "Limits total training data size to control training time and ensure representative vocabulary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--doc_cap', type=int, default=10_000, help='Maximum characters per document (default: 10,000)')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "parser.add_argument('--doc_cap', type=int, default=10_000, help='Maximum characters per document (default: 10,000)')",
      "voiceover": "Caps individual document length to prevent extremely long documents from dominating training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "parser.add_argument('--vocab_size', type=int, default=65536, help='Vocabulary size (default: 65536 = 2^16)')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "parser.add_argument('--vocab_size', type=int, default=65536, help='Vocabulary size (default: 65536 = 2^16)')",
      "voiceover": "Sets target vocabulary size determining tokenizer compression and model embedding table size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "args = parser.parse_args()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "args = parser.parse_args()",
      "voiceover": "Parses command-line arguments into args object used throughout training process",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"max_chars: {args.max_chars:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "print(f\"max_chars: {args.max_chars:,}\")",
      "voiceover": "Log maximum characters for user confirmation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"doc_cap: {args.doc_cap:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "print(f\"doc_cap: {args.doc_cap:,}\")",
      "voiceover": "Log document cap for user confirmation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"vocab_size: {args.vocab_size:,}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "print(f\"vocab_size: {args.vocab_size:,}\")",
      "voiceover": "Log vocabulary size for user confirmation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def text_iterator():\n    nchars = 0\n    for batch in parquets_iter_batched(split=\"train\"):\n        for doc in batch:\n            doc_text = doc\n            if len(doc_text) > args.doc_cap:\n                doc_text = doc_text[:args.doc_cap]\n            nchars += len(doc_text)\n            yield doc_text\n            if nchars > args.max_chars:\n                return\n",
      "highlight": true,
      "voiceover": "Train a tokenizer using the HuggingFace Tokenizers library.\nIn the style of GPT-4 tokenizer.\n\nGenerates a stream of text documents from the training dataset for tokenizer training with size controls. This generator function iterates through batches of documents from the parquet dataset applying two important constraints to manage training data quality and quantity. First it crops each document to a maximum length specified by doc_cap preventing extremely long documents from dominating the byte-pair encoding statistics which could lead to a vocabulary biased toward specific long-form content. Second it tracks the total number of characters yielded and stops when max_chars is reached ensuring training completes in reasonable time while still seeing enough diverse text to learn a representative vocabulary. The function flattens the batched structure yielding individual documents one at a time making it compatible with the HuggingFace tokenizer training interface. This design balances training efficiency with vocabulary quality by exposing the tokenizer to a large diverse corpus while preventing pathological cases from skewing the learned merges.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "text_iter = text_iterator()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "text_iter = text_iterator()",
      "voiceover": "Create text iterator instance that will be consumed by tokenizer training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "t0 = time.time()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "t0 = time.time()",
      "voiceover": "----------------------------------------------------------------------------- Train the tokenizer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "t0 = time.time()",
      "voiceover": "Record start time for measuring training duration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)",
      "voiceover": "Train BPE tokenizer on text iterator learning byte-pair merges up to target vocabulary size",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "t1 = time.time()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "t1 = time.time()",
      "voiceover": "Record end time after training completes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "train_time = t1 - t0\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "train_time = t1 - t0",
      "voiceover": "Calculate total training time in seconds",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"Training time: {train_time:.2f}s\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "print(f\"Training time: {train_time:.2f}s\")",
      "voiceover": "Log training duration for performance monitoring",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "base_dir = get_base_dir()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "----------------------------------------------------------------------------- Save the tokenizer to disk",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "base_dir = get_base_dir()",
      "voiceover": "Get base directory for saving tokenizer files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")",
      "voiceover": "Construct path to tokenizer directory",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "tokenizer.save(tokenizer_dir)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "tokenizer.save(tokenizer_dir)",
      "voiceover": "Save trained tokenizer to disk including vocabulary and merge rules",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "test_text = \"\"\"Hello world! This is a test.\nNumbers: 123, 4567, 89\nContractions: I'm, you're, it's\nSpecial chars: @\nUnicode: 你好世界 🌍\"\"\"\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "test_text = \"\"\"Hello world! This is a test. # Sample text covering various character types for testing tokenizer correctness",
      "voiceover": "----------------------------------------------------------------------------- Quick inline sanity check",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "encoded = tokenizer.encode(test_text)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "encoded = tokenizer.encode(test_text)",
      "voiceover": "Encode test text to token IDs to verify tokenizer can handle diverse characters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "decoded = tokenizer.decode(encoded)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "decoded = tokenizer.decode(encoded)",
      "voiceover": "Decode back to text to verify lossless round-trip encoding",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "assert decoded == test_text\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "assert decoded == test_text",
      "voiceover": "Ensure tokenizer preserves all characters including Unicode and special symbols",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "vocab_size = tokenizer.get_vocab_size()\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "vocab_size = tokenizer.get_vocab_size()",
      "voiceover": "----------------------------------------------------------------------------- One more thing: we wish to cache a mapping from token id to number of bytes of that token for efficient evaluation of bits per byte. Unlike the typical mean loss, this allows us to report a loss that is invariant to the vocab size of the tokenizer. The bits per byte on the validation set is then one of the primary metrics we care about.",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "vocab_size = tokenizer.get_vocab_size()",
      "voiceover": "Get total vocabulary size including special tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "special_set = set(tokenizer.get_special_tokens())\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "special_set = set(tokenizer.get_special_tokens())",
      "voiceover": "Get set of special tokens that don't represent actual text bytes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]",
      "voiceover": "Decode each token ID to its string representation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes = []\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes = []",
      "voiceover": "List to store byte count for each token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "for token_id in range(vocab_size):\n    token_str = token_strings[token_id]\n    if token_str in special_set:\n        token_bytes.append(0)\n    else:\n        id_bytes = len(token_str.encode(\"utf-8\"))\n        token_bytes.append(id_bytes)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "for token_id in range(vocab_size):",
      "voiceover": "Iterate through all tokens in vocabulary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_str = token_strings[token_id]",
      "voiceover": "Get Python string representation of this token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "if token_str in special_set:",
      "voiceover": "Check if token is special (BOS, EOS, PAD, etc.)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes.append(0)",
      "voiceover": "Special tokens don't represent actual text so assign 0 bytes",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "else:",
      "voiceover": "Regular token representing actual text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "id_bytes = len(token_str.encode(\"utf-8\"))",
      "voiceover": "Calculate UTF-8 byte length of token string",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes.append(id_bytes)",
      "voiceover": "Store byte count for this token",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')",
      "voiceover": "Convert to PyTorch tensor for efficient loading during evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")",
      "voiceover": "Construct path for saving token bytes mapping",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "with open(token_bytes_path, \"wb\") as f:\n    torch.save(token_bytes, f)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "with open(token_bytes_path, \"wb\") as f:",
      "voiceover": "Open file in binary write mode",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "torch.save(token_bytes, f)",
      "voiceover": "Save tensor to disk for use during model evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "print(f\"Saved token_bytes to {token_bytes_path}\")\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "print(f\"Saved token_bytes to {token_bytes_path}\")",
      "voiceover": "Log save location for user confirmation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "from nanochat.report import get_report\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "from nanochat.report import get_report # Import report logging utility for saving training statistics",
      "voiceover": "Log to report",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "from nanochat.report import get_report",
      "voiceover": "Import report logging utility for saving training statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)",
      "voiceover": "Filter out special tokens (0 bytes) and convert to float for statistics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "get_report().log(section=\"Tokenizer training\", data=[\n    vars(args),\n    {\"train_time\": train_time},\n    {\"num_special_tokens\": len(special_set)},\n    {\n        \"token_bytes_min\": int(token_bytes_nonzero.min().item()),\n        \"token_bytes_max\": int(token_bytes_nonzero.max().item()),\n        \"token_bytes_mean\": token_bytes_nonzero.mean().item(),\n        \"token_bytes_std\": token_bytes_nonzero.std().item(),\n    }\n])\n"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "get_report().log(section=\"Tokenizer training\", data=[",
      "voiceover": "Log training summary to report file",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "vars(args),",
      "voiceover": "Command-line arguments used for training",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "{\"train_time\": train_time},",
      "voiceover": "Total training duration in seconds",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "{\"num_special_tokens\": len(special_set)},",
      "voiceover": "Count of special tokens in vocabulary",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "{",
      "voiceover": "Statistics about token byte lengths for understanding compression characteristics",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "\"token_bytes_min\": int(token_bytes_nonzero.min().item()),",
      "voiceover": "Minimum bytes per token (typically 1 for single ASCII characters)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "\"token_bytes_max\": int(token_bytes_nonzero.max().item()),",
      "voiceover": "Maximum bytes per token (longer merged sequences)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "\"token_bytes_mean\": token_bytes_nonzero.mean().item(),",
      "voiceover": "Average bytes per token indicating overall compression ratio",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "scripts/tok_train.py",
      "find": "\"token_bytes_std\": token_bytes_nonzero.std().item(),",
      "voiceover": "Standard deviation showing variability in token lengths",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tasks/arc.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from datasets import load_dataset\nfrom tasks.common import Task, render_mc\n"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading the ARC benchmark from the hub",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "from tasks.common import Task, render_mc",
      "voiceover": "Base task class and utility for rendering multiple-choice questions in chat format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class ARC(Task):\n",
      "highlight": true,
      "voiceover": "The ARC dataset from Allen AI.\nhttps://huggingface.co/datasets/allenai/ai2_arc",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "class ARC(Task):",
      "voiceover": "ARC (AI2 Reasoning Challenge) task class for science question answering with multiple-choice answers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, subset, split, **kwargs):\n        super().__init__(**kwargs)\n        assert subset in [\"ARC-Easy\", \"ARC-Challenge\"], \"ARC subset must be ARC-Easy or ARC-Challenge\"\n        assert split in [\"train\", \"validation\", \"test\"], \"ARC split must be train|validation|test\"\n        self.ds = load_dataset(\"allenai/ai2_arc\", subset, split=split).shuffle(seed=42)\n",
      "highlight": true,
      "voiceover": "Initializes the ARC task by loading the specified subset and split from HuggingFace datasets. This constructor validates that the subset is either ARC-Easy or ARC-Challenge which represent different difficulty levels of science questions, and that the split is one of train, validation, or test for different evaluation purposes. The dataset is loaded from the allenai/ai2_arc repository and shuffled with a fixed seed of 42 to ensure reproducible ordering across runs while still providing variety in the data distribution. The shuffle is important for training to prevent the model from learning spurious patterns based on dataset ordering. This initialization pattern allows the task to be used flexibly for both training and evaluation on different difficulty levels and data splits.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'categorical'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is categorical indicating that answers are selected from a fixed set of choices. This property is used by the evaluation framework to determine how to process model outputs. For categorical tasks the model's output logits are examined at the answer position to find which choice has the highest probability, rather than generating free-form text. This approach is more reliable for multiple-choice questions because it avoids issues with parsing generated text and ensures the answer is always one of the valid choices. The categorical evaluation type enables efficient batched evaluation by processing multiple questions simultaneously without needing to generate and parse text.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return len(self.ds)\n",
      "highlight": true,
      "voiceover": "Returns the total number of examples in the loaded dataset split. This method is called by the evaluation and training frameworks to determine how many examples to process and to calculate metrics like accuracy over the full dataset. The length is determined by the HuggingFace dataset object which knows the size of the loaded split. This information is used for progress tracking, determining when to stop iteration, and calculating dataset statistics.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        row = self.ds[index]\n        question = row[\"question\"]\n        choices = row[\"choices\"][\"text\"]\n        answer_string = row[\"answerKey\"]\n        letters = row[\"choices\"][\"label\"]\n        assert answer_string in letters, f\"ARC answer {answer_string} must be one of {letters}\"\n        user_message = render_mc(question, letters, choices)\n        messages = [\n            {\"role\": \"user\", \"content\": user_message},\n            {\"role\": \"assistant\", \"content\": answer_string}\n        ]\n        conversation = {\n            \"messages\": messages,\n            \"letters\": letters,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves and formats a single example from the dataset as a conversation object suitable for chat model training and evaluation. This method takes an integer index and returns a structured conversation containing the question formatted as a multiple-choice prompt and the correct answer. The function extracts the question text, choice texts, answer key letter, and choice labels from the dataset row. It uses the render_mc utility to format the question and choices into a natural language prompt following the chat format. The returned conversation includes both the formatted messages for the model and the letters list which is used during evaluation to constrain the model's output to valid choices. This design separates data loading from formatting and provides a consistent interface for the training and evaluation pipelines to consume examples regardless of the underlying dataset structure.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "question = row[\"question\"]",
      "voiceover": "the question text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "choices = row[\"choices\"][\"text\"]",
      "voiceover": "the text of each choice",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "answer_string = row[\"answerKey\"]",
      "voiceover": "e.g. \"A\", \"B\", \"C\", \"D\"",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "letters = row[\"choices\"][\"label\"]",
      "voiceover": "e.g. [\"A\", \"B\", \"C\", \"D\"]",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "assert answer_string in letters, f\"ARC answer {answer_string} must be one of {letters}\"",
      "voiceover": "sanity check",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "\"letters\": letters,",
      "voiceover": "useful during evaluation, so we can narrow and clamp the assistant prediction to one of the letters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, conversation, assistant_response):\n        assert assistant_response in conversation['letters'], f\"ARC answer {assistant_response} is expected to be one of {conversation['letters']}\"\n        assistant_message = conversation['messages'][-1]['content']\n        return assistant_response == assistant_message\n",
      "highlight": true,
      "voiceover": "Evaluates whether the model's predicted answer matches the correct answer for a given question. This method takes the conversation object containing the ground truth answer and the assistant_response which is the model's predicted letter choice. It validates that the predicted response is one of the valid choice letters to catch potential bugs in the evaluation pipeline where the response might not have been properly constrained. The function then compares the predicted answer against the correct answer extracted from the conversation's last assistant message. The assertion serves as a safety check to ensure the evaluation framework is working correctly and the model's output has been properly processed. This evaluation approach is simple and deterministic returning a boolean indicating correctness which can be aggregated across examples to compute accuracy.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/arc.py",
      "find": "assistant_message = conversation['messages'][-1]['content']",
      "voiceover": "e.g. \"A\"",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tasks/common.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import random\n"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "import random",
      "voiceover": "Random number generation for deterministic shuffling of task mixtures with fixed seed",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class Task:\n",
      "highlight": true,
      "voiceover": "Base class for all Tasks.\nA Task is basically a dataset of conversations, together with some\nmetadata and often also evaluation criteria.\nExample tasks: MMLU, ARC-Easy, ARC-Challenge, GSM8K, HumanEval, SmolTalk.\n\nBase class of a Task. Allows for lightweight slicing of the underlying dataset.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "class Task:",
      "voiceover": "Base class for all task datasets providing common interface for training and evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, start=0, stop=None, step=1):\n        assert start >= 0, f\"Start must be non-negative, got {start}\"\n        assert stop is None or stop >= start, f\"Stop should be greater than or equal to start, got {stop} and {start}\"\n        assert step >= 1, f\"Step must be strictly positive, got {step}\"\n        self.start = start\n        self.stop = stop\n        self.step = step\n",
      "highlight": true,
      "voiceover": "Initializes a task with optional slicing parameters to create a lightweight logical view over the dataset. This constructor accepts start, stop, and step parameters similar to Python's slice notation allowing users to work with subsets of the dataset without copying data. The start parameter specifies the first example index to include, stop specifies the exclusive upper bound (or None for all remaining examples), and step allows skipping examples for sampling. The validation ensures start is non-negative, stop is greater than or equal to start if specified, and step is at least 1 to prevent infinite loops or backward iteration. This slicing design enables flexible dataset manipulation for creating train/validation splits, debugging with small subsets, or implementing custom sampling strategies without modifying the underlying dataset or creating expensive copies.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "self.stop = stop",
      "voiceover": "could be None here",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        raise NotImplementedError\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which must be either 'generative' or 'categorical'. This property is used by the evaluation framework to determine how to process model outputs. Generative tasks require the model to generate free-form text which is then compared against reference answers, while categorical tasks examine the model's output logits at specific positions to select from a fixed set of choices. Subclasses must override this method to specify their evaluation type. This design allows the evaluation framework to handle different task types uniformly while applying the appropriate evaluation strategy for each.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        raise NotImplementedError\n",
      "highlight": true,
      "voiceover": "Returns the total number of examples in the underlying dataset before any slicing is applied. This method must be implemented by subclasses to report the size of their dataset. The value is used by the __len__ method to calculate the effective length after applying start, stop, and step parameters. This separation allows the base class to handle slicing logic while subclasses only need to report their raw dataset size.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        raise NotImplementedError\n",
      "highlight": true,
      "voiceover": "Retrieves a single example from the dataset at the specified physical index. This method must be implemented by subclasses to return a conversation object containing the messages and any metadata needed for training or evaluation. The index refers to the actual position in the underlying dataset without considering slicing parameters. Subclasses are responsible for loading, formatting, and returning the example in the standard conversation format. This design separates data loading from slicing logic allowing the base class to handle index translation in __getitem__.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __len__(self):\n        start = self.start\n        stop = self.num_examples() if self.stop is None else self.stop\n        step = self.step\n        span = stop - start\n        num = (span + step - 1) // step\n        assert num >= 0, f\"Negative number of examples???: {num}\"\n        return num\n",
      "highlight": true,
      "voiceover": "Calculates and returns the effective number of examples after applying slicing parameters. This method computes the length by determining the span between start and stop (using the full dataset size if stop is None), then dividing by step using ceiling division to include partial steps. The calculation ensures that all examples within the slice are counted even if the span is not evenly divisible by step. The assertion prevents returning negative lengths which would indicate a logic error. This implementation allows users to query the size of sliced datasets without iterating through all examples.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "num = (span + step - 1) // step",
      "voiceover": "ceil_div(span, step)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "assert num >= 0, f\"Negative number of examples???: {num}\"",
      "voiceover": "prevent footguns",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __getitem__(self, index: int):\n        assert isinstance(index, int), f\"Index must be an integer, got {type(index)}\"\n        physical_index = self.start + index * self.step\n        conversation = self.get_example(physical_index)\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves an example using logical indexing that accounts for slicing parameters. This method translates a logical index (0 to len-1) into a physical index in the underlying dataset by applying the start offset and step multiplier. The physical index is then passed to get_example to retrieve the actual conversation. This design allows sliced datasets to be accessed with simple 0-based indexing while the base class handles the translation to physical indices. The assertion ensures only integer indices are used preventing confusion with slice objects or other index types.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, problem, completion):\n        raise NotImplementedError\n",
      "highlight": true,
      "voiceover": "Evaluates a model's completion against the correct answer for a given problem. This method must be implemented by subclasses to define task-specific evaluation logic. The problem parameter contains the conversation object with the ground truth answer, and completion contains the model's predicted response. Subclasses should return a boolean or numeric score indicating correctness or quality. This abstract interface allows different tasks to implement custom evaluation strategies while the evaluation framework can call them uniformly.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class TaskMixture(Task):\n",
      "highlight": true,
      "voiceover": "For SFT Training it becomes useful to train on a mixture of datasets.\nFun trick: if you wish to oversample any task, just pass it in multiple times in the list.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "class TaskMixture(Task):",
      "voiceover": "Task mixture class for training on multiple datasets simultaneously with deterministic shuffling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, tasks, **kwargs):\n        super().__init__(**kwargs)\n        self.tasks = tasks\n        self.lengths = [len(task) for task in self.tasks]\n        self.num_conversations = sum(self.lengths)\n        self.index_map = []\n        for task_idx, task_length in enumerate(self.lengths):\n            for local_idx in range(task_length):\n                self.index_map.append((task_idx, local_idx))\n        rng = random.Random(42)\n        rng.shuffle(self.index_map)\n",
      "highlight": true,
      "voiceover": "Initializes a task mixture by combining multiple task datasets into a single unified dataset with deterministic shuffling. This constructor takes a list of Task objects and creates an index mapping that interleaves examples from all tasks ensuring they are mixed throughout training rather than processed sequentially. The method first computes the length of each task and the total number of conversations across all tasks. It then builds an index map containing tuples of (task_idx, local_idx) for every example in every task. This index map is shuffled using a fixed random seed of 42 to ensure reproducible ordering across training runs while still providing good mixing of different task types. The deterministic shuffle is crucial for reproducibility and fair comparison across experiments. The design allows oversampling specific tasks by including them multiple times in the input list which is useful for balancing datasets of different sizes or emphasizing certain capabilities. This mixture approach enables multi-task learning where the model sees diverse examples in each batch rather than learning one task at a time.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.num_conversations\n",
      "highlight": true,
      "voiceover": "Returns the total number of conversations across all tasks in the mixture. This method simply returns the precomputed sum of all task lengths calculated during initialization. The value represents the combined size of all datasets and is used by the base class __len__ method to support slicing operations. This approach avoids recomputing the sum on every call making it efficient for repeated queries.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        assert 0 <= index < self.num_conversations, f\"Index {index} out of range for mixture with {self.num_conversations} conversations\"\n        task_idx, local_idx = self.index_map[index]\n        return self.tasks[task_idx][local_idx]\n",
      "highlight": true,
      "voiceover": "Access conversations according to a deterministic shuffle of all examples.\nThis ensures tasks are mixed throughout training, regardless of dataset size.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class TaskSequence(Task):\n",
      "highlight": true,
      "voiceover": "For SFT Training sometimes we want to sequentially train on a list of tasks.\nThis is useful for cases that require a training curriculum.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "class TaskSequence(Task):",
      "voiceover": "Task sequence class for sequential training on multiple datasets in curriculum learning scenarios",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, tasks, **kwargs):\n        super().__init__(**kwargs)\n        self.tasks = tasks\n        self.lengths = [len(task) for task in self.tasks]\n        self.num_conversations = sum(self.lengths)\n",
      "highlight": true,
      "voiceover": "Initializes a task sequence by concatenating multiple task datasets in order without shuffling. This constructor takes a list of Task objects and creates a sequential dataset where examples from the first task are accessed before examples from the second task and so on. The method computes the length of each task and the total number of conversations across all tasks. Unlike TaskMixture this class does not shuffle the examples maintaining the original task ordering. This design is useful for curriculum learning where the model should learn simpler tasks before more complex ones, or for staged training where different capabilities are introduced sequentially. The sequential access pattern ensures that when iterating through the dataset all examples from one task are processed before moving to the next task. This approach can be beneficial when tasks have dependencies or when a specific learning progression is desired.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.num_conversations\n",
      "highlight": true,
      "voiceover": "Returns the total number of conversations across all tasks in the sequence. This method returns the precomputed sum of all task lengths calculated during initialization. The value represents the combined size of all datasets in sequential order and is used by the base class __len__ method to support slicing operations. This approach avoids recomputing the sum on every call providing efficient access to the total dataset size.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        assert 0 <= index < self.num_conversations, f\"Index {index} out of range for sequence with {self.num_conversations} conversations\"\n        for task_idx, task_length in enumerate(self.lengths):\n            if index < task_length:\n                return self.tasks[task_idx][index]\n            index -= task_length\n",
      "highlight": true,
      "voiceover": "Retrieves an example by sequentially searching through tasks until the index is found. This method takes a global index and determines which task it belongs to by iterating through the task lengths and subtracting each length from the index until the index falls within a task's range. Once the correct task is identified the remaining index value is used to retrieve the example from that task. This sequential lookup ensures examples are accessed in task order without any shuffling. The assertion validates that the index is within the valid range preventing out-of-bounds access. This design maintains the curriculum learning property where tasks are processed in the specified order.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "def render_mc(question, letters, choices):\n    query = f\"Multiple Choice question: {question}\\n\"\n    query += \"\".join([f\"- {choice}={letter}\\n\" for letter, choice in zip(letters, choices)])\n    query += \"\\nRespond only with the letter of the correct answer.\"\n    return query\n",
      "highlight": true,
      "voiceover": "The common multiple choice rendering format we will use.\n\nNote two important design decisions:\n1)\nBigger models don't care as much, but smaller models prefer to have\nthe letter *after* the choice, which results in better binding.\n2)\nThere is no whitespace between the delimiter (=) and the letter.\nThis is actually critical because the tokenizer has different token ids\nfor \" A\" vs. \"A\". The assistant responses will be just the letter itself,\ni.e. \"A\", so it is important that here in the prompt it is the exact same\ntoken, i.e. \"A\" with no whitespace before it. Again, bigger models don't care\nabout this too much, but smaller models do care about some of these details.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n    from tasks.mmlu import MMLU\n\n    ds = MMLU(subset=\"auxiliary_train\", split=\"train\")\n    print(\"Length of MMLU: \", len(ds))\n    ex = ds[5]\n    print(\"5th example: \", ex)\n\n    ds = MMLU(subset=\"auxiliary_train\", split=\"train\", start=5, stop=10)\n    print(\"Length of sliced MMLU[5:10]: \", len(ds))\n    print(\"0th example of sliced MMLU: \", ds[0])\n\n    print(\"They match: \", ex == ds[0])\n"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Test block for demonstrating task slicing functionality with MMLU dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "from tasks.mmlu import MMLU",
      "voiceover": "Import MMLU task for testing slicing behavior",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "ds = MMLU(subset=\"auxiliary_train\", split=\"train\")",
      "voiceover": "Load full MMLU auxiliary training set",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "print(\"Length of MMLU: \", len(ds))",
      "voiceover": "Display total number of examples in dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "ex = ds[5]",
      "voiceover": "Retrieve 5th example from full dataset for comparison",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "print(\"5th example: \", ex)",
      "voiceover": "Display the example content",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "ds = MMLU(subset=\"auxiliary_train\", split=\"train\", start=5, stop=10)",
      "voiceover": "Create sliced view of dataset from index 5 to 10",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "print(\"Length of sliced MMLU[5:10]: \", len(ds))",
      "voiceover": "Display length of sliced dataset (should be 5)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "print(\"0th example of sliced MMLU: \", ds[0])",
      "voiceover": "Retrieve first example from sliced dataset (should be same as index 5 from full dataset)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/common.py",
      "find": "print(\"They match: \", ex == ds[0])",
      "voiceover": "Verify that slicing correctly maps logical index 0 to physical index 5",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tasks/customjson.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import os\nimport json\nfrom tasks.common import Task\n"
    },
    {
      "type": "highlight",
      "path": "tasks/customjson.py",
      "find": "import os",
      "voiceover": "Operating system interface for checking file existence before loading conversations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/customjson.py",
      "find": "import json",
      "voiceover": "JSON parsing for loading conversation messages from JSONL format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/customjson.py",
      "find": "from tasks.common import Task",
      "voiceover": "Base task class providing common interface for dataset handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class CustomJSON(Task):\n",
      "highlight": true,
      "voiceover": "CustomJSON task for loading conversations from JSONL files.\nEach line in the JSONL file should be a JSON array of messages.\n\nLoad conversations from a JSONL file.\nEach line should be a JSON array of message objects with 'role' and 'content' fields.\nExample line: [{\"role\":\"user\",\"content\":\"Hi\"},{\"role\":\"assistant\",\"content\":\"Hello\"}]",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/customjson.py",
      "find": "class CustomJSON(Task):",
      "voiceover": "Custom task class for loading arbitrary conversation datasets from JSONL files",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, filepath, **kwargs):\n        super().__init__(**kwargs)\n        self.filepath = filepath\n        self.conversations = []\n\n        if not os.path.exists(filepath):\n            print(\"-\" * 80)\n            print(f\"Warning: File {filepath} does not exist\")\n            print(\"HINT (Oct 21 2025)\")\n            print(\"If you recently did a git pull and suddely see this, it might be due to the new addition of identity conversations\")\n            print(\"See this discussion for more details: https://github.com/karpathy/nanochat/discussions/139\")\n            print(\"Quick fix: simply run the following command to download the file and you're done:\")\n            print(f\"curl -L -o {filepath} https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl\")\n            print(\"-\" * 80)\n\n        else:\n            with open(filepath, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    messages = json.loads(line)\n                    assert isinstance(messages, list), f\"Expected list of messages, got {type(messages)}\"\n                    assert len(messages) >= 2, f\"Conversation must have at least 2 messages, got {len(messages)}\"\n                    for i, message in enumerate(messages):\n                        assert \"role\" in message, f\"Message {i} missing 'role' field\"\n                        assert \"content\" in message, f\"Message {i} missing 'content' field\"\n                        expected_role = \"user\" if i % 2 == 0 else \"assistant\"\n                        assert message[\"role\"] == expected_role, f\"Message {i} has role {message['role']} but should be {expected_role}\"\n                        assert isinstance(message[\"content\"], str), f\"Message {i} content must be a string\"\n\n                    self.conversations.append(messages)\n\n        self.length = len(self.conversations)\n",
      "highlight": true,
      "voiceover": "Initializes the CustomJSON task by loading and validating all conversations from a JSONL file. This constructor takes a filepath to a JSONL file where each line contains a JSON array of message objects representing one conversation. The method first checks if the file exists and provides a helpful error message with download instructions if it's missing, which is particularly useful for the identity_conversations.jsonl file that may need to be downloaded separately. If the file exists, it reads each line, parses the JSON, and validates the conversation structure ensuring each conversation is a list with at least two messages, that each message has 'role' and 'content' fields, and that roles alternate between 'user' and 'assistant' starting with user. This strict validation catches malformed data early preventing errors during training. The conversations are stored in memory as a list making access fast during training. This design allows users to easily add custom training data by creating JSONL files without modifying code, enabling flexible dataset composition for supervised fine-tuning.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/customjson.py",
      "find": "if not line:",
      "voiceover": "skip empty lines",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.length\n",
      "highlight": true,
      "voiceover": "Returns the total number of conversations loaded from the JSONL file. This method returns the precomputed length stored during initialization representing the count of valid conversations that passed validation. The value is used by the base class __len__ method to support slicing operations and by training frameworks to determine dataset size. This approach avoids recomputing the length on every call providing efficient access to the dataset size.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        messages = self.conversations[index]\n        conversation = {\n            \"messages\": messages,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves a single conversation from the loaded dataset at the specified index. This method takes an integer index and returns a conversation object containing the messages array. The messages are already loaded and validated during initialization so this method simply packages them in the standard conversation format expected by the training framework. The design keeps the interface consistent with other task types while allowing arbitrary custom conversations to be loaded from JSONL files. This flexibility enables users to create specialized training datasets for specific use cases like teaching the model about its identity, adding domain-specific knowledge, or fine-tuning conversational style.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "tasks/gsm8k.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import re\nfrom datasets import load_dataset\nfrom tasks.common import Task\n"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "import re",
      "voiceover": "Regular expressions for parsing numerical answers and tool call markers in GSM8K format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading GSM8K grade school math benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "from tasks.common import Task",
      "voiceover": "Base task class providing common interface for dataset handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "GSM_RE = re.compile(r\"\n"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "GSM_RE = re.compile(r\"",
      "voiceover": "Compiled regex pattern for extracting numerical answer after #### marker in GSM8K format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "def extract_answer(completion):\n    match = GSM_RE.search(completion)\n    if match:\n        match_str = match.group(1).strip()\n        match_str = match_str.replace(\",\", \"\")\n        return match_str\n    return None\n",
      "highlight": true,
      "voiceover": "GSM8K evaluation.\nhttps://huggingface.co/datasets/openai/gsm8k\n\nExample problem instance:\n\nQuestion:\nWeng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\nAnswer:\nWeng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n#### 10\n\nNotice that GSM8K uses tool calls inside << >> tags.\n\nExtracts the numerical answer from a GSM8K completion string by finding the value after the #### marker. This function uses a regular expression to search for the distinctive #### marker that GSM8K uses to denote the final answer in its solution format. The regex pattern matches optional negative signs and numbers with decimals or commas. Once a match is found the function extracts the matched string, strips whitespace, and removes commas to normalize the number format for comparison. This normalization follows the official GSM8K evaluation code ensuring compatibility with the benchmark's evaluation methodology. The function returns None if no answer marker is found which can happen with incomplete or malformed completions. This extraction is critical for both evaluation during testing and reward calculation during reinforcement learning where the model's generated answer must be compared against the ground truth.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class GSM8K(Task):\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "class GSM8K(Task):",
      "voiceover": "GSM8K task class for grade school math word problems with calculator tool use",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, subset, split, **kwargs):\n        super().__init__(**kwargs)\n        assert subset in [\"main\", \"socratic\"], \"GSM8K subset must be main|socratic\"\n        assert split in [\"train\", \"test\"], \"GSM8K split must be train|test\"\n        self.ds = load_dataset(\"openai/gsm8k\", subset, split=split).shuffle(seed=42)\n",
      "highlight": true,
      "voiceover": "Initializes the GSM8K task by loading the specified subset and split from HuggingFace datasets. This constructor validates that the subset is either 'main' for standard problems or 'socratic' for problems with more detailed step-by-step reasoning, and that the split is either 'train' or 'test' for different evaluation purposes. The dataset is loaded from the openai/gsm8k repository and shuffled with a fixed seed of 42 to ensure reproducible ordering across runs while providing variety in the data distribution. The shuffle is important for training to prevent the model from learning spurious patterns based on dataset ordering. This initialization pattern allows the task to be used flexibly for both training the model to use calculator tools and evaluating its mathematical reasoning abilities.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'generative'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is generative indicating that the model must generate free-form text containing the solution. This property is used by the evaluation framework to determine how to process model outputs. For generative tasks the model generates a complete response which is then parsed to extract the answer rather than selecting from a fixed set of choices. This approach is necessary for GSM8K because the model needs to show its work including calculator tool calls and reasoning steps before arriving at the final numerical answer. The generative evaluation type enables the model to demonstrate its problem-solving process which is valuable for both training and debugging.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return len(self.ds)\n",
      "highlight": true,
      "voiceover": "Returns the total number of examples in the loaded dataset split. This method is called by the evaluation and training frameworks to determine how many examples to process and to calculate metrics like accuracy over the full dataset. The length is determined by the HuggingFace dataset object which knows the size of the loaded split. This information is used for progress tracking, determining when to stop iteration, and calculating dataset statistics.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        row = self.ds[index]\n        question = row['question']\n        answer = row['answer']\n        assistant_message_parts = []\n        parts = re.split(r'(<<[^>]+>>)', answer)\n        for part in parts:\n            if part.startswith('<<') and part.endswith('>>'):\n                inner = part[2:-2]\n                if '=' in inner:\n                    expr, result = inner.rsplit('=', 1)\n                else:\n                    expr, result = inner, \"\"\n                assistant_message_parts.append({\"type\": \"python\", \"text\": expr})\n                assistant_message_parts.append({\"type\": \"python_output\", \"text\": result})\n            else:\n                assistant_message_parts.append({\"type\": \"text\", \"text\": part})\n        messages = [\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": assistant_message_parts},\n        ]\n        conversation = {\n            \"messages\": messages,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves and formats a single math problem from the dataset as a conversation object with parsed tool calls. This method takes an integer index and returns a structured conversation containing the question and the solution with calculator tool calls properly formatted. The function extracts the question text and answer string from the dataset row. The answer string contains both the reasoning steps and calculator tool calls enclosed in << >> markers following the GSM8K format. The method parses this format by splitting on the tool call markers and creating a list of message parts where each part is either regular text, a python tool call containing the expression to evaluate, or a python_output part containing the result. This parsing transforms the GSM8K format into the chat model's tool use format enabling the model to learn when and how to use the calculator tool. The returned conversation includes the formatted messages suitable for training the model on mathematical reasoning with tool use. This design separates the GSM8K-specific parsing from the general training pipeline allowing the model to learn tool use patterns that generalize beyond this specific dataset format.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "question = row['question']",
      "voiceover": "string of the question prompt",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "answer = row['answer']",
      "voiceover": "string of the full solution and the answer after #### marker",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "inner = part[2:-2]",
      "voiceover": "Remove << >>",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "{\"role\": \"user\", \"content\": question},",
      "voiceover": "note: simple string",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "{\"role\": \"assistant\", \"content\": assistant_message_parts},",
      "voiceover": "note: list of parts (as dicts)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, conversation, assistant_response):\n        assert isinstance(assistant_response, str), \"Assuming simple string response for now\"\n        assistant_message = conversation['messages'][-1]\n        assert assistant_message['role'] == \"assistant\", \"Last message must be from the Assistant\"\n        assert isinstance(assistant_message['content'], list), \"This is expected to be a list of parts\"\n        last_text_part = assistant_message['content'][-1]['text']\n        ref_num = extract_answer(last_text_part)\n        pred_num = extract_answer(assistant_response)\n        is_correct = int(pred_num == ref_num)\n        return is_correct\n",
      "highlight": true,
      "voiceover": "Evaluates whether the model's generated solution contains the correct numerical answer. This method takes the conversation object containing the ground truth answer and the assistant_response which is the model's generated solution string. It extracts the final answer from both the ground truth and the prediction by searching for the #### marker and parsing the number that follows. The ground truth answer is found in the last text part of the assistant message's content list which contains the complete solution with tool calls. The predicted answer is extracted from the assistant_response string using the same extraction logic. The function compares the normalized numerical strings for exact equality and returns 1 for correct or 0 for incorrect. This binary evaluation is simple and deterministic making it suitable for calculating accuracy metrics. The method currently assumes the assistant_response is a simple string rather than a structured message with parts which is noted as a potential future enhancement. This evaluation approach focuses solely on the final numerical answer rather than the reasoning process which aligns with the GSM8K benchmark's evaluation methodology.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/gsm8k.py",
      "find": "last_text_part = assistant_message['content'][-1]['text']",
      "voiceover": "this contains the final answer in GSM8K",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def reward(self, conversation, assistant_response):\n        is_correct = self.evaluate(conversation, assistant_response)\n        is_correct_float = float(is_correct)\n        return is_correct_float\n",
      "highlight": true,
      "voiceover": "Computes the reward signal for reinforcement learning by reusing the evaluation logic. This method takes the same inputs as evaluate and returns a float reward value where 1.0 indicates a correct answer and 0.0 indicates an incorrect answer. The function simply calls the evaluate method and converts the integer result to a float to match the expected reward type for RL algorithms. This simple reward structure provides a clear learning signal where the model is rewarded for producing the correct numerical answer. The design keeps the reward calculation simple and aligned with the evaluation metric ensuring that what the model optimizes during RL training matches what it will be evaluated on. Future enhancements could add partial credit for correct reasoning steps or format matching but the current binary reward is effective for learning basic mathematical problem-solving with tool use.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "tasks/humaneval.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import re\nfrom datasets import load_dataset\nfrom nanochat.execution import execute_code\nfrom tasks.common import Task\n"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "import re",
      "voiceover": "Regular expressions for extracting Python code from markdown code blocks in LLM completions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading HumanEval coding benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "from nanochat.execution import execute_code",
      "voiceover": "Code execution utility for safely running generated Python programs in sandboxed environment",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "from tasks.common import Task",
      "voiceover": "Base task class providing common interface for dataset handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def extract_imports(prompt):\n    imports = []\n    for line in prompt.split('\\n'):\n        stripped = line.strip()\n        if stripped.startswith('import ') or stripped.startswith('from '):\n            imports.append(stripped)\n        elif stripped and not stripped.startswith('\n            break\n    return '\\n'.join(imports)\n",
      "highlight": true,
      "voiceover": "Evaluate the Chat model on HumanEval dataset.\nBtw this dataset is a misnomer and has nothing to do with humans.\nIt is a coding benchmark.\n\nExtracts import statements from the beginning of a code block for prepending to generated code. This function parses the prompt line by line collecting all import and from-import statements that appear at the start of the code before any other executable code. It stops collecting when it encounters the first non-import, non-comment line ensuring only the necessary imports are extracted. This extraction is necessary because LLM completions often omit import statements when generating function implementations, but these imports are required for the code to execute successfully. The function returns a newline-joined string of all extracted import statements which can be prepended to the generated code. This design separates the import extraction logic making it reusable and testable while ensuring generated code has all necessary dependencies for execution.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def extract_program(completion):\n    pattern = r'```(?:python)?\\s*\\n(.*?)\\n```'\n    matches = re.findall(pattern, completion, re.DOTALL)\n\n    if matches:\n        return matches[0].strip()\n\n    return completion.strip()\n",
      "highlight": true,
      "voiceover": "Extracts Python code from LLM completion handling various output formats including markdown code blocks and plain code. This function uses a regular expression to search for code enclosed in markdown code blocks with either ```python or just ``` delimiters. The regex pattern uses DOTALL flag to match across multiple lines and captures the content between the delimiters. If markdown code blocks are found the function returns the first one stripped of whitespace, which handles cases where the LLM wraps its code in markdown formatting or includes explanatory text before or after the code. If no code blocks are found the function returns the entire completion stripped, assuming the LLM generated plain code without markdown formatting. This flexible extraction is crucial for robust evaluation because different models and prompting strategies produce different output formats. The design prioritizes finding properly formatted code blocks but gracefully falls back to using the raw completion ensuring the evaluation can proceed even with unexpected output formats.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class HumanEval(Task):\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "class HumanEval(Task):",
      "voiceover": "HumanEval task class for evaluating code generation capabilities on Python programming problems",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.ds = load_dataset(\"openai/openai_humaneval\", split=\"test\").shuffle(seed=42)\n",
      "highlight": true,
      "voiceover": "Initializes the HumanEval task by loading the test split from HuggingFace datasets. This constructor loads the openai_humaneval dataset which contains Python programming problems with function signatures, docstrings, canonical solutions, and test cases. The dataset is shuffled with a fixed seed of 42 to ensure reproducible ordering across evaluation runs while providing variety in the problem distribution. The shuffle helps prevent any ordering effects during evaluation. This initialization pattern allows the task to be used for both training the model on code generation and evaluating its programming capabilities. The HumanEval benchmark is widely used for measuring code generation quality making it a standard evaluation for chat models with coding abilities.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'generative'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is generative indicating that the model must generate free-form code. This property is used by the evaluation framework to determine how to process model outputs. For generative tasks the model generates a complete response which is then executed and tested rather than selecting from predefined choices. This approach is necessary for HumanEval because the model needs to generate working Python code that passes test cases. The generative evaluation type enables the model to demonstrate its programming abilities through actual code execution which is the most direct measure of coding capability.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return len(self.ds)\n",
      "highlight": true,
      "voiceover": "Returns the total number of programming problems in the HumanEval test set. This method is called by the evaluation framework to determine how many examples to process and to calculate metrics like pass rate over the full dataset. The length is determined by the HuggingFace dataset object which knows the size of the test split. This information is used for progress tracking, determining when to stop iteration, and calculating dataset statistics.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        row = self.ds[index]\n        prompt = row['prompt']\n        solution = row['canonical_solution']\n        entry_point = row['entry_point']\n        test = row['test']\n        complete_solution = f\"{prompt}\\n{solution}\"\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": complete_solution},\n        ]\n        conversation = {\n            \"messages\": messages,\n            \"entry_point\": entry_point,\n            \"test\": test,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves and formats a single programming problem from the dataset as a conversation object. This method takes an integer index and returns a structured conversation containing the function prompt and the complete canonical solution. The function extracts the prompt which contains the function signature and docstring, the canonical solution which is the correct implementation, the entry point which is the function name to test, and the test cases which verify correctness. It combines the prompt and solution to create the complete reference implementation. The returned conversation includes the formatted messages with user providing the prompt and assistant providing the complete solution, plus the entry point and test metadata needed during evaluation. This design separates the HumanEval-specific data format from the general training and evaluation pipeline allowing the model to learn code generation patterns in a conversational format while preserving the information needed for execution-based evaluation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "prompt = row['prompt']",
      "voiceover": "prompts in HumanEval are the beginning of the program",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "solution = row['canonical_solution']",
      "voiceover": "the correct continuation of the program",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "entry_point = row['entry_point']",
      "voiceover": "the function to check",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "test = row['test']",
      "voiceover": "the test cases",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "\"entry_point\": entry_point,",
      "voiceover": "needed during evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/humaneval.py",
      "find": "\"test\": test,",
      "voiceover": "needed during evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, conversation, completion):\n        imports = extract_imports(conversation['messages'][0]['content'])\n        completion_code = extract_program(completion)\n        program = (\n            imports\n            + \"\\n\\n\"\n            + completion_code\n            + \"\\n\\n\"\n            + conversation['test']\n            + \"\\n\"\n            + f\"check({conversation['entry_point']})\"\n        )\n        result = execute_code(program)\n        success = result.success\n        return success\n",
      "highlight": true,
      "voiceover": "Evaluates whether the model's generated code passes all test cases by executing it in a sandboxed environment. This method takes the conversation object containing the original prompt and test cases, and the completion which is the model's generated code. It first extracts import statements from the prompt to ensure the generated code has all necessary dependencies. Then it extracts the actual Python code from the completion handling various output formats including markdown code blocks. The function assembles a complete program by combining the imports, the generated code, the test cases, and a call to the check function with the entry point. This assembled program is executed in a sandboxed environment using execute_code which safely runs the code and captures any errors. The evaluation returns a boolean indicating whether the code executed successfully and passed all tests. This execution-based evaluation is the gold standard for code generation because it directly measures whether the generated code works correctly rather than relying on text similarity or other proxy metrics. The design ensures safe execution while providing accurate assessment of the model's coding capabilities.",
      "voiceoverTiming": "during"
    },
    {
      "type": "openFile",
      "path": "tasks/mmlu.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from datasets import load_dataset\nfrom tasks.common import Task, render_mc\n"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading MMLU massive multitask language understanding benchmark",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "from tasks.common import Task, render_mc",
      "voiceover": "Base task class and utility for rendering multiple-choice questions in chat format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class MMLU(Task):\n",
      "highlight": true,
      "voiceover": "The MMLU dataset.\nhttps://huggingface.co/datasets/cais/mmlu",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "class MMLU(Task):",
      "voiceover": "MMLU task class for evaluating knowledge across 57 academic subjects with multiple-choice questions",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    letters = ('A', 'B', 'C', 'D')\n"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "letters = ('A', 'B', 'C', 'D')",
      "voiceover": "Fixed choice letters for all MMLU questions enabling consistent answer format across subjects",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "    groups = ('abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions')\n"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "groups = ('abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions')",
      "voiceover": "Complete list of 57 academic subjects covered by MMLU for comprehensive knowledge evaluation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, subset, split, **kwargs):\n        super().__init__(**kwargs)\n        assert subset in [\"all\", \"auxiliary_train\"], f\"subset {subset} must be all|auxiliary_train\"\n        assert split in [\"train\", \"validation\", \"dev\", \"test\"], f\"split {split} must be train|validation|dev|test\"\n        if subset == \"auxiliary_train\":\n            assert split == \"train\", \"auxiliary_train must be split into train\"\n        self.subset = subset\n        self.split = split\n        self.ds = load_dataset(\"cais/mmlu\", subset, split=split).shuffle(seed=42)\n        if subset == \"auxiliary_train\":\n            self.ds = self.ds.map(lambda row: row['train'], remove_columns=['train'])\n",
      "highlight": true,
      "voiceover": "Initializes the MMLU task by loading the specified subset and split from HuggingFace datasets. This constructor validates that the subset is either 'all' for the complete MMLU benchmark across all subjects or 'auxiliary_train' for additional training data, and that the split is one of train, validation, dev, or test for different evaluation purposes. The auxiliary_train subset can only be used with the train split as enforced by an assertion. The dataset is loaded from the cais/mmlu repository and shuffled with a fixed seed of 42 to ensure reproducible ordering across runs while providing variety in the question distribution. For the auxiliary_train subset the constructor applies a mapping to unwrap a quirky 'train' wrapper that exists in the dataset structure, flattening the data to the expected format. This initialization pattern allows the task to be used flexibly for both training on diverse academic knowledge and evaluating the model's understanding across multiple domains. The MMLU benchmark is widely used for measuring broad knowledge making it a standard evaluation for general-purpose language models.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'categorical'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is categorical indicating that answers are selected from a fixed set of choices. This property is used by the evaluation framework to determine how to process model outputs. For categorical tasks the model's output logits are examined at the answer position to find which choice has the highest probability, rather than generating free-form text. This approach is more reliable for multiple-choice questions because it avoids issues with parsing generated text and ensures the answer is always one of the valid choices A, B, C, or D. The categorical evaluation type enables efficient batched evaluation by processing multiple questions simultaneously without needing to generate and parse text.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return len(self.ds)\n",
      "highlight": true,
      "voiceover": "Returns the total number of questions in the loaded dataset split. This method is called by the evaluation and training frameworks to determine how many examples to process and to calculate metrics like accuracy over the full dataset. The length is determined by the HuggingFace dataset object which knows the size of the loaded split. This information is used for progress tracking, determining when to stop iteration, and calculating dataset statistics.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        row = self.ds[index]\n        question = row[\"question\"]\n        choices = row[\"choices\"]\n        answer = row[\"answer\"]\n        subject = row[\"subject\"]\n        assert len(choices) == 4, \"MMLU should have 4 choices\"\n        user_message = render_mc(question, self.letters, choices)\n        assistant_message = self.letters[answer]\n        messages = [\n            {\"role\": \"user\", \"content\": user_message},\n            {\"role\": \"assistant\", \"content\": assistant_message}\n        ]\n        conversation = {\n            \"messages\": messages,\n            \"subject\": subject,\n            \"letters\": self.letters,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves and formats a single question from the dataset as a conversation object suitable for chat model training and evaluation. This method takes an integer index and returns a structured conversation containing the question formatted as a multiple-choice prompt and the correct answer letter. The function extracts the question text, the four choice texts, the answer index (0-3), and the subject category from the dataset row. It validates that exactly four choices are present as required by the MMLU format. The question and choices are formatted using the render_mc utility which creates a natural language prompt following the chat format. The answer index is converted to the corresponding letter (A, B, C, or D) for the assistant's response. The returned conversation includes both the formatted messages for the model and metadata including the subject for potential grouping of metrics by academic domain and the letters list which is used during evaluation to constrain the model's output to valid choices. This design separates data loading from formatting and provides a consistent interface for the training and evaluation pipelines to consume examples regardless of the underlying dataset structure.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "question = row[\"question\"]",
      "voiceover": "the question text",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "choices = row[\"choices\"]",
      "voiceover": "the text of each choice",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "answer = row[\"answer\"]",
      "voiceover": "index of the answer, e.g. 0,1,2,3 (for A,B,C,D)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "subject = row[\"subject\"]",
      "voiceover": "e.g. \"college_biology\", \"college_chemistry\", etc.",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "\"subject\": subject,",
      "voiceover": "might be useful later for grouping metrics by subject",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "\"letters\": self.letters,",
      "voiceover": "useful during evaluation, so we can narrow and clamp the assistant prediction to one of the letters",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, conversation, assistant_response):\n        assert assistant_response in self.letters, f\"MMLU answer {assistant_response} is expected to be one of {self.letters}\"\n        assistant_message = conversation['messages'][-1]['content']\n        return assistant_response == assistant_message\n",
      "highlight": true,
      "voiceover": "Evaluates whether the model's predicted answer matches the correct answer for a given question. This method takes the conversation object containing the ground truth answer and the assistant_response which is the model's predicted letter choice. It validates that the predicted response is one of the valid choice letters (A, B, C, D) to catch potential bugs in the evaluation pipeline where the response might not have been properly constrained. The function then compares the predicted answer against the correct answer extracted from the conversation's last assistant message. The assertion serves as a safety check to ensure the evaluation framework is working correctly and the model's output has been properly processed. This evaluation approach is simple and deterministic returning a boolean indicating correctness which can be aggregated across examples to compute accuracy overall or broken down by subject to identify strengths and weaknesses in different academic domains.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/mmlu.py",
      "find": "assistant_message = conversation['messages'][-1]['content']",
      "voiceover": "e.g. \"A\"",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tasks/smoltalk.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "from datasets import load_dataset\nfrom tasks.common import Task\n"
    },
    {
      "type": "highlight",
      "path": "tasks/smoltalk.py",
      "find": "from datasets import load_dataset",
      "voiceover": "HuggingFace datasets library for loading SmolTalk conversational dataset",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/smoltalk.py",
      "find": "from tasks.common import Task",
      "voiceover": "Base task class providing common interface for dataset handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class SmolTalk(Task):\n",
      "highlight": true,
      "voiceover": "SmolTalk by HuggingFace. Good \"general\" conversational dataset.\nhttps://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk\nWe use the \"smol\" version, which is more appropriate for smaller models.\n\nsmol-smoltalk dataset. train is 460K rows, test is 24K rows. ",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/smoltalk.py",
      "find": "class SmolTalk(Task):",
      "voiceover": "SmolTalk task class for training on general conversational data with natural dialogue patterns",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, split, **kwargs):\n        super().__init__(**kwargs)\n        assert split in [\"train\", \"test\"], \"SmolTalk split must be train|test\"\n        self.ds = load_dataset(\"HuggingFaceTB/smol-smoltalk\", split=split).shuffle(seed=42)\n        self.length = len(self.ds)\n",
      "highlight": true,
      "voiceover": "Initializes the SmolTalk task by loading the specified split from HuggingFace datasets. This constructor validates that the split is either 'train' for the 460K training conversations or 'test' for the 24K test conversations. The dataset is loaded from the HuggingFaceTB/smol-smoltalk repository and shuffled with a fixed seed of 42 to ensure reproducible ordering across runs while providing variety in the conversation distribution. The shuffle is important for training to prevent the model from learning spurious patterns based on dataset ordering. The length is precomputed and stored for efficient access during training and evaluation. This initialization pattern allows the task to be used for training the model on natural conversational patterns and evaluating its ability to engage in general dialogue. SmolTalk is particularly valuable for teaching models how to maintain coherent multi-turn conversations with appropriate tone and context awareness.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.length\n",
      "highlight": true,
      "voiceover": "Returns the total number of conversations in the loaded dataset split. This method returns the precomputed length stored during initialization representing the count of conversations in either the train (460K) or test (24K) split. The value is used by the base class __len__ method to support slicing operations and by training frameworks to determine dataset size. This approach avoids recomputing the length on every call providing efficient access to the dataset size.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        row = self.ds[index]\n        messages = row[\"messages\"]\n        assert len(messages) >= 1\n        first_message = messages[0]\n        if first_message[\"role\"] == \"system\":\n            rest_messages = messages[1:]\n        else:\n            rest_messages = messages\n        assert len(rest_messages) >= 2, \"SmolTalk messages must have at least 2 messages\"\n        for i, message in enumerate(rest_messages):\n            expected_role = \"user\" if i % 2 == 0 else \"assistant\"\n            assert message[\"role\"] == expected_role, f\"Message {i} has role {message['role']} but should be {expected_role}\"\n            assert isinstance(message[\"content\"], str), \"Content must be a string\"\n        conversation = {\n            \"messages\": messages,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Retrieves and validates a single conversation from the dataset ensuring proper message structure. This method takes an integer index and returns a conversation object containing the messages array. The function extracts the messages from the dataset row and performs comprehensive validation to catch malformed data early. It checks that there is at least one message and handles the optional system message that may appear at the beginning of some conversations. After accounting for the optional system message it validates that at least two messages remain ensuring a meaningful conversation. The function then verifies that messages alternate between user and assistant roles starting with user, and that all message content is a string rather than a more complex structure. These strict validation checks prevent errors during training by ensuring the data matches the expected conversational format. The returned conversation includes the complete messages array including any system message which provides context or instructions for the conversation. This design allows the model to learn from diverse conversational patterns while ensuring data quality through validation.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/smoltalk.py",
      "find": "rest_messages = messages[1:]",
      "voiceover": "optional system message is OK",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tasks/spellingbee.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import re\nimport random\nfrom tasks.common import Task\nfrom nanochat.common import download_file_with_lock\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "import re",
      "voiceover": "Regular expressions for extracting numerical answers from completion strings",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "import random",
      "voiceover": "Random number generation for deterministic data generation with per-example seeds",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "from tasks.common import Task",
      "voiceover": "Base task class providing common interface for dataset handling",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "from nanochat.common import download_file_with_lock",
      "voiceover": "Utility for downloading and caching word list file with thread-safe locking",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "LETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "LETTERS = \"abcdefghijklmnopqrstuvwxyz\"",
      "voiceover": "Letters of the alphabet",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "LETTERS = \"abcdefghijklmnopqrstuvwxyz\"",
      "voiceover": "Complete lowercase alphabet for random letter selection in letter counting tasks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "WORD_LIST_URL = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt\"\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "WORD_LIST_URL = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt\"",
      "voiceover": "A list of 370K English words of large variety",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "WORD_LIST_URL = \"https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt\"",
      "voiceover": "URL to comprehensive English word list for generating diverse spelling and counting examples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "TEST_RANDOM_SEED_OFFSET = 10_000_000\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "TEST_RANDOM_SEED_OFFSET = 10_000_000",
      "voiceover": "A number bigger than 370K to separate train and test random seeds",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "TEST_RANDOM_SEED_OFFSET = 10_000_000",
      "voiceover": "Large offset added to test indices to ensure train and test examples use different random seeds preventing overlap",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "ANSWER_RE = re.compile(r\"\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "ANSWER_RE = re.compile(r\"",
      "voiceover": "Identical to gsm8k's answer extraction",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "ANSWER_RE = re.compile(r\"",
      "voiceover": "Compiled regex pattern for extracting numerical answer after #### marker following GSM8K format",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "def extract_answer(completion):\n    match = ANSWER_RE.search(completion)\n    if match:\n        match_str = match.group(1).strip()\n        match_str = match_str.replace(\",\", \"\")\n        return match_str\n    return None\n",
      "highlight": true,
      "voiceover": "Task intended to make nanochat better in spelling and counting, for example:\n\n\"How many r are in strawberry?\" -> 3\n\nAn interesting part of this task is that we will get the assistant to\nsolve the problem using a combination of manual counting and Python.\nThis is a good problem solving \"instinct\" to mix into the model and RL\nmay further refine it to trust one over the other. If we were extra fancy\n(which we could/should be) we'd add small errors here and there to allow\nthe model also learn recoveries. We can do this in future versions.\n\nThere are two tasks in this file:\n1. SpellingBee: Counting the number of occurrences of a letter in a word\n2. SimpleSpelling: Simply spelling words\n\n(1) is the goal, but (2) exists as a highly condensed version of the part\nthat makes (1) difficult, which is word spelling. This is non-trivial for an\nLLM because it has to learn how every token (a little semantic chunk/atom)\nmaps to the sequence of individual characters that make it up. Larger models\nlearn this eventually on their own, but if we want this capability to exist\nin smaller models, we have to actively encourage it by over-representing it\nin the training data. Midtraining is a good place to do this.\n\nTo preview a few example conversations, run:\npython -m tasks.spellingbee\n\nExtracts the numerical answer from a completion string by finding the value after the #### marker. This function uses a regular expression to search for the distinctive #### marker that indicates the final answer in the completion format. The regex pattern matches optional negative signs and numbers with decimals or commas. Once a match is found the function extracts the matched string, strips whitespace, and removes commas to normalize the number format for comparison. This normalization ensures consistent evaluation across different number formatting styles. The function returns None if no answer marker is found which can happen with incomplete or malformed completions. This extraction is critical for both evaluation during testing and reward calculation during reinforcement learning where the model's generated answer must be compared against the ground truth count.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "USER_MSG_TEMPLATES = [\n    \"How many {letter} are in the word {word}\",\n    \"How many {letter} are in {word}\",\n    \"Count the number of {letter} in {word}\",\n    \"How many times does {letter} appear in {word}\",\n    \"What's the count of {letter} in {word}\",\n    \"In the word {word}, how many {letter} are there\",\n    \"How many letter {letter} are in the word {word}\",\n    \"Count how many {letter} appear in {word}\",\n    \"Tell me the number of {letter} in {word}\",\n    \"How many occurrences of {letter} are in {word}\",\n    \"Find the count of {letter} in {word}\",\n    \"Can you count the {letter} letters in {word}\",\n    \"What is the frequency of {letter} in {word}\",\n    \"How many {letter}s are in {word}\",\n    \"How many {letter}'s are in {word}\",\n    \"Count all the {letter} in {word}\",\n    \"How many times is {letter} in {word}\",\n    \"Number of {letter} in {word}\",\n    \"Total count of {letter} in {word}\",\n    \"How many {letter} does {word} have\",\n    \"How many {letter} does {word} contain\",\n    \"What's the number of {letter} in {word}\",\n    \"{word} has how many {letter}\",\n    \"In {word}, count the {letter}\",\n    \"How many {letter} appear in {word}\",\n    \"Count the {letter} in {word}\",\n    \"Give me the count of {letter} in {word}\",\n    \"How many instances of {letter} in {word}\",\n    \"Show me how many {letter} are in {word}\",\n    \"Calculate the number of {letter} in {word}\",\n    \"¿Cuántas {letter} hay en {word}?\",\n    \"¿Cuántas veces aparece {letter} en {word}?\",\n    \"Cuenta las {letter} en {word}\",\n    \"¿Cuántas letras {letter} tiene {word}?\",\n    \"{word}中有多少个{letter}\",\n    \"{word}里有几个{letter}\",\n    \"数一下{word}中的{letter}\",\n    \"{word}这个词里有多少{letter}\",\n    \"{word}에 {letter}가 몇 개 있나요\",\n    \"{word}에서 {letter}의 개수는\",\n    \"{word}에 {letter}가 몇 번 나오나요\",\n    \"{word}라는 단어에 {letter}가 몇 개\",\n    \"Combien de {letter} dans {word}\",\n    \"Combien de fois {letter} apparaît dans {word}\",\n    \"Compte les {letter} dans {word}\",\n    \"Wie viele {letter} sind in {word}\",\n    \"Wie oft kommt {letter} in {word} vor\",\n    \"Zähle die {letter} in {word}\",\n    \"{word}に{letter}は何個ありますか\",\n    \"{word}の中に{letter}がいくつ\",\n    \"{word}に{letter}が何回出てくる\",\n]\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "USER_MSG_TEMPLATES = [",
      "voiceover": "User message templates for data augmentation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "USER_MSG_TEMPLATES = [",
      "voiceover": "Diverse question templates in multiple languages for asking about letter counts providing data augmentation and multilingual capability",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "class SpellingBee(Task):\n",
      "highlight": true
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "class SpellingBee(Task):",
      "voiceover": "SpellingBee task class for training models to count letter occurrences using manual counting and Python verification",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, size=1000, split=\"train\", **kwargs):\n        super().__init__(**kwargs)\n        assert split in [\"train\", \"test\"], \"SpellingBee split must be train|test\"\n        self.size = size\n        self.split = split\n        filename = WORD_LIST_URL.split(\"/\")[-1]\n        word_list_path = download_file_with_lock(WORD_LIST_URL, filename)\n        with open(word_list_path, 'r', encoding='utf-8') as f:\n            words = [line.strip() for line in f]\n        self.words = words\n",
      "highlight": true,
      "voiceover": "Initializes the SpellingBee task by downloading the word list and preparing for synthetic data generation. This constructor validates that the split is either 'train' or 'test' for different evaluation purposes. It downloads a comprehensive English word list containing 370K words from GitHub using thread-safe locking to prevent race conditions when multiple processes access the file simultaneously. The word list is loaded into memory for efficient random sampling during example generation. The size parameter determines how many examples this task will generate allowing flexible dataset sizing. This initialization pattern enables on-the-fly generation of diverse letter counting examples rather than storing a fixed dataset. The synthetic generation approach ensures unlimited variety in training data while maintaining reproducibility through deterministic random seeds.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'generative'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is generative indicating that the model must generate free-form text containing the solution. This property is used by the evaluation framework to determine how to process model outputs. For generative tasks the model generates a complete response which is then parsed to extract the answer rather than selecting from a fixed set of choices. This approach is necessary for SpellingBee because the model needs to show its work including manual counting steps and Python verification before arriving at the final numerical answer. The generative evaluation type enables the model to demonstrate its problem-solving process combining manual reasoning with tool use which is valuable for both training and debugging.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.size\n",
      "highlight": true,
      "voiceover": "Returns the number of examples this task will generate determined by the size parameter. This method returns the precomputed size specified during initialization rather than the length of an underlying dataset since examples are generated synthetically on-the-fly. The value is used by the base class __len__ method to support slicing operations and by training frameworks to determine how many examples to process. This synthetic generation approach allows flexible dataset sizing without storing large amounts of data.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        seed = index if self.split == 'train' else TEST_RANDOM_SEED_OFFSET + index\n        rng = random.Random(seed)\n\n        word = rng.choice(self.words)\n        letter = rng.choice(word) if rng.random() < 0.9 else rng.choice(LETTERS)\n\n        count = word.count(letter)\n\n        template = rng.choice(USER_MSG_TEMPLATES)\n        if rng.random() < 0.3:\n            template = template.lower()\n        quote_options = ['', \"'\", '\"']\n        letter_quote = rng.choice(quote_options)\n        word_quote = rng.choice(quote_options)\n        letter_wrapped = f\"{letter_quote}{letter}{letter_quote}\"\n        word_wrapped = f\"{word_quote}{word}{word_quote}\"\n        user_msg = template.format(letter=letter_wrapped, word=word_wrapped)\n        if rng.random() < 0.5:\n            user_msg += \"?\"\n\n        assistant_parts = []\n        word_letters = \",\".join(list(word))\n        manual_text = f\"\"\"We are asked to find the number '{letter}' in the word '{word}'. Let me try a manual approach first.\n\nFirst spell the word out:\n{word}:{word_letters}\n\nThen count the occurrences of '{letter}':",
      "highlight": true,
      "voiceover": "Generates a single letter counting example with manual counting and Python verification using deterministic randomization. This method takes an integer index and creates a complete conversation demonstrating how to count letter occurrences in a word using both manual character-by-character counting and Python string methods. The function uses a deterministic random seed based on the index (with an offset for test split) ensuring reproducible examples across runs. It randomly selects a word from the word list and a letter to count, with 90% probability choosing a letter that exists in the word and 10% choosing a random letter to include zero-count cases. The user message is generated from diverse multilingual templates with random variations in capitalization, quoting, and punctuation to increase robustness. The assistant response is constructed as a list of message parts demonstrating the complete solution process. First it manually spells out the word and counts occurrences character by character showing the running count. Then it verifies the answer using Python's count method as a tool call. Finally it provides the answer in the standard #### format. This multi-step approach teaches the model to combine manual reasoning with programmatic verification, a valuable problem-solving pattern. The design deliberately avoids whitespace before characters in the manual counting to ensure consistent tokenization. This synthetic generation creates unlimited diverse training examples while maintaining the pedagogical structure of showing work and verification.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "letter_quote = rng.choice(quote_options)",
      "voiceover": "is the letter quoted?",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "word_quote = rng.choice(quote_options)",
      "voiceover": "is the word quoted?",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "if rng.random() < 0.5:",
      "voiceover": "50% of people don't even use question marks",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def evaluate(self, conversation, assistant_response):\n        assert isinstance(assistant_response, str), \"Assuming simple string response for now\"\n        assistant_message = conversation['messages'][-1]\n        assert assistant_message['role'] == \"assistant\", \"Last message must be from the Assistant\"\n        assert isinstance(assistant_message['content'], list), \"This is expected to be a list of parts\"\n        last_text_part = assistant_message['content'][-1]['text']\n        ref_num = extract_answer(last_text_part)\n        pred_num = extract_answer(assistant_response)\n        is_correct = int(pred_num == ref_num)\n        return is_correct\n",
      "highlight": true,
      "voiceover": "Evaluates whether the model's generated solution contains the correct numerical answer. This method takes the conversation object containing the ground truth answer and the assistant_response which is the model's generated solution string. It extracts the final answer from both the ground truth and the prediction by searching for the #### marker and parsing the number that follows. The ground truth answer is found in the last text part of the assistant message's content list which contains the complete solution with manual counting and Python verification. The predicted answer is extracted from the assistant_response string using the same extraction logic. The function compares the normalized numerical strings for exact equality and returns 1 for correct or 0 for incorrect. This binary evaluation is simple and deterministic making it suitable for calculating accuracy metrics. The method currently assumes the assistant_response is a simple string rather than a structured message with parts. This evaluation approach focuses solely on the final numerical answer rather than the reasoning process which aligns with the task's goal of teaching correct counting.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def reward(self, conversation, assistant_response):\n        is_correct = self.evaluate(conversation, assistant_response)\n        is_correct_float = float(is_correct)\n        return is_correct_float\n",
      "highlight": true,
      "voiceover": "Computes the reward signal for reinforcement learning by reusing the evaluation logic. This method takes the same inputs as evaluate and returns a float reward value where 1.0 indicates a correct answer and 0.0 indicates an incorrect answer. The function simply calls the evaluate method and converts the integer result to a float to match the expected reward type for RL algorithms. This simple reward structure provides a clear learning signal where the model is rewarded for producing the correct numerical count. The design keeps the reward calculation simple and aligned with the evaluation metric ensuring that what the model optimizes during RL training matches what it will be evaluated on. Future enhancements could add partial credit for correct manual counting or proper Python usage but the current binary reward is effective for learning basic letter counting with verification.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "class SimpleSpelling(Task):\n",
      "highlight": true,
      "voiceover": "Much simpler task designed to get the model to just practice spelling words.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "class SimpleSpelling(Task):",
      "voiceover": "SimpleSpelling task class for training models to spell words character by character",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def __init__(self, size=1000, split=\"train\", **kwargs):\n        super().__init__(**kwargs)\n        assert split in [\"train\", \"test\"], \"SpellingBee split must be train|test\"\n        self.size = size\n        self.split = split\n        filename = WORD_LIST_URL.split(\"/\")[-1]\n        word_list_path = download_file_with_lock(WORD_LIST_URL, filename)\n        with open(word_list_path, 'r', encoding='utf-8') as f:\n            words = [line.strip() for line in f]\n        rng = random.Random(42)\n        rng.shuffle(words)\n        self.words = words\n",
      "highlight": true,
      "voiceover": "Initializes the SimpleSpelling task by downloading the word list and preparing for synthetic spelling data generation. This constructor validates that the split is either 'train' or 'test' for different evaluation purposes. It downloads the same comprehensive English word list used by SpellingBee containing 370K words from GitHub using thread-safe locking. The word list is loaded into memory and shuffled with a fixed seed of 42 to create a different word order than the SpellingBee task, ensuring variety when both tasks are used together. The size parameter determines how many examples this task will generate allowing flexible dataset sizing. This initialization pattern enables on-the-fly generation of diverse spelling examples focusing on the fundamental skill of mapping words to their character sequences. This simpler task complements SpellingBee by providing concentrated practice on the spelling component which is particularly challenging for smaller language models that need to learn how semantic tokens decompose into individual characters.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "rng.shuffle(words)",
      "voiceover": "use a different word order than the SpellingBee task",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n    @property\n"
    },
    {
      "type": "writeText",
      "content": "    def eval_type(self):\n        return 'generative'\n",
      "highlight": true,
      "voiceover": "Returns the evaluation type for this task which is generative indicating that the model must generate the spelled-out word. This property is used by the evaluation framework to determine how to process model outputs. For generative tasks the model generates a complete response which is the comma-separated character sequence. This approach allows the model to practice the fundamental skill of decomposing words into their constituent characters which is essential for tasks like letter counting. The generative evaluation type enables straightforward training on this basic but important capability.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def num_examples(self):\n        return self.size\n",
      "highlight": true,
      "voiceover": "Returns the number of examples this task will generate determined by the size parameter. This method returns the precomputed size specified during initialization rather than the length of an underlying dataset since examples are generated synthetically on-the-fly. The value is used by the base class __len__ method to support slicing operations and by training frameworks to determine how many examples to process. This synthetic generation approach allows flexible dataset sizing without storing large amounts of data.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "    def get_example(self, index):\n        seed = index if self.split == 'train' else TEST_RANDOM_SEED_OFFSET + index\n        rng = random.Random(seed)\n        word = rng.choice(self.words)\n        word_letters = \",\".join(list(word))\n        messages = [\n            {\"role\": \"user\", \"content\": f\"Spell the word: {word}\"},\n            {\"role\": \"assistant\", \"content\": f\"{word}:{word_letters}\"}\n        ]\n        conversation = {\n            \"messages\": messages,\n        }\n        return conversation\n",
      "highlight": true,
      "voiceover": "Generates a single spelling example asking the model to spell a word character by character. This method takes an integer index and creates a simple conversation where the user asks to spell a word and the assistant provides the comma-separated character sequence. The function uses a deterministic random seed based on the index (with an offset for test split) ensuring reproducible examples across runs. It randomly selects a word from the shuffled word list and formats it as word:c,h,a,r,s where each character is separated by commas. This simple format teaches the model the fundamental mapping between words and their character sequences which is non-trivial for language models because they operate on semantic tokens rather than individual characters. This concentrated practice on spelling helps smaller models develop the character-level understanding needed for more complex tasks like letter counting. The synthetic generation creates unlimited diverse training examples with minimal complexity focusing purely on the spelling skill.",
      "voiceoverTiming": "during"
    },
    {
      "type": "writeText",
      "content": "\n\n"
    },
    {
      "type": "writeText",
      "content": "if __name__ == \"__main__\":\n\n    task = SpellingBee()\n    for i in range(10):\n        ex = task.get_example(i)\n        print(\"=\" * 100)\n        print(ex['messages'][0]['content'])\n        print(\"-\" * 100)\n        assistant_parts = ex['messages'][1]['content']\n        for part in assistant_parts:\n            if part['type'] == 'text':\n                print(part['text'], end='')\n            elif part['type'] == 'python':\n                print(f\"<<{part['text']}=\", end='')\n            elif part['type'] == 'python_output':\n                print(f\"{part['text']}>>\", end='')\n        print()\n        print(\"-\" * 100)\n"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "if __name__ == \"__main__\":",
      "voiceover": "Test block for previewing SpellingBee task examples when running module directly",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "task = SpellingBee()",
      "voiceover": "Create SpellingBee task instance with default parameters for demonstration",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "for i in range(10):",
      "voiceover": "Generate and display first 10 examples to show task format and variety",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "ex = task.get_example(i)",
      "voiceover": "Get example with deterministic seed for reproducibility",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "print(\"=\" * 100)",
      "voiceover": "Visual separator between examples",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "print(ex['messages'][0]['content'])",
      "voiceover": "Display user question asking about letter count",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "print(\"-\" * 100)",
      "voiceover": "Separator between question and answer",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "assistant_parts = ex['messages'][1]['content']",
      "voiceover": "Extract assistant response parts (text, python, python_output)",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "for part in assistant_parts:",
      "voiceover": "Iterate through parts to display complete solution",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "if part['type'] == 'text':",
      "voiceover": "Regular text parts printed directly",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "elif part['type'] == 'python':",
      "voiceover": "Python tool calls formatted with << >> markers",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "elif part['type'] == 'python_output':",
      "voiceover": "Python output formatted with >> marker",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "print()",
      "voiceover": "Newline after complete assistant response",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tasks/spellingbee.py",
      "find": "print(\"-\" * 100)",
      "voiceover": "Final separator after each example",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tests/test_engine.py"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "import torch\nfrom nanochat.engine import KVCache\n"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "import torch",
      "voiceover": "PyTorch tensor library for creating test tensors and verifying KV cache contents",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "from nanochat.engine import KVCache",
      "voiceover": "Key-value cache implementation for transformer attention mechanism being tested",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "writeText",
      "content": "\n"
    },
    {
      "type": "writeText",
      "content": "def test_kv_cache_resize():\n\n    batch_size = 2\n    num_heads = 3\n    seq_len = 4\n    head_dim = 5\n    num_layers = 6\n\n    kv_cache = KVCache(\n        batch_size=batch_size,\n        num_heads=num_heads,\n        seq_len=seq_len,\n        head_dim=head_dim,\n        num_layers=num_layers\n    )\n\n    def insert_token(token_idx):\n        for layer_idx in range(num_layers):\n            k = torch.full((batch_size, num_heads, 1, head_dim), fill_value=float(token_idx), dtype=torch.float32)\n            v = torch.full((batch_size, num_heads, 1, head_dim), fill_value=float(token_idx * 100), dtype=torch.float32)\n            kv_cache.insert_kv(layer_idx, k, v)\n\n    for i in range(4):\n        insert_token(i)\n\n    original_cache = kv_cache.kv_cache.clone()\n    original_seq_len = original_cache.shape[4]\n\n    insert_token(4)\n    new_seq_len = kv_cache.kv_cache.shape[4]\n    assert new_seq_len > original_seq_len, f\"Cache did not resize: original seq_len={original_seq_len}, new seq_len={new_seq_len}\"\n\n    for layer_idx in range(num_layers):\n        for token_idx in range(4):\n            expected_k = float(token_idx)\n            expected_v = float(token_idx * 100)\n            actual_k = kv_cache.kv_cache[layer_idx, 0, :, :, token_idx, :]\n            actual_v = kv_cache.kv_cache[layer_idx, 1, :, :, token_idx, :]\n            assert (actual_k == expected_k).all(), f\"Layer {layer_idx}, token {token_idx}: key corrupted, expected {expected_k}\"\n            assert (actual_v == expected_v).all(), f\"Layer {layer_idx}, token {token_idx}: value corrupted, expected {expected_v}\"\n            original_k = original_cache[layer_idx, 0, :, :, token_idx, :]\n            original_v = original_cache[layer_idx, 1, :, :, token_idx, :]\n            assert (actual_k == original_k).all(), f\"Layer {layer_idx}, token {token_idx}: key doesn't match original\"\n            assert (actual_v == original_v).all(), f\"Layer {layer_idx}, token {token_idx}: value doesn't match original\"\n",
      "highlight": true,
      "voiceover": "Test Engine class. Example run:\n\npython -m pytest tests/test_engine.py -v\n\nTests that the KV cache correctly resizes when exceeding initial capacity while preserving existing data. This test function reproduces and validates the fix for a bug where the KV cache was not resized correctly when the number of tokens exceeded the initial sequence length. The issue was identified in GitHub pull request #186 where the cache would either fail to resize or corrupt existing key-value pairs during the resize operation. The test creates a KV cache with a small initial sequence length of 4, fills it completely with distinct values for each token position, then inserts a 5th token to trigger the resize operation. It verifies two critical properties: first that the cache actually increased in size, and second that all original token data remains intact and uncorrupted after the resize. The test uses distinct fill values for each token (token_idx for keys, token_idx * 100 for values) making it easy to detect any data corruption or misalignment. This comprehensive validation ensures the KV cache can dynamically grow during inference while maintaining data integrity, which is essential for handling variable-length sequences in production. The test design isolates the resize functionality by using predictable synthetic data rather than real model outputs, making failures easy to diagnose and debug.",
      "voiceoverTiming": "during"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "batch_size = 2",
      "voiceover": "Number of sequences processed in parallel for testing batched cache operations",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "num_heads = 3",
      "voiceover": "Number of attention heads in the transformer model being cached",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "seq_len = 4",
      "voiceover": "Initial sequence length capacity of the cache which will be exceeded to trigger resize",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "head_dim = 5",
      "voiceover": "Dimensionality of each attention head's key and value vectors",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "num_layers = 6",
      "voiceover": "Number of transformer layers each maintaining separate key-value caches",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "kv_cache = KVCache(",
      "voiceover": "Create KV cache instance with small initial capacity to test resize behavior",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "for i in range(4):",
      "voiceover": "Fill cache to initial capacity before triggering resize",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "original_cache = kv_cache.kv_cache.clone()",
      "voiceover": "Clone cache state before resize to verify data preservation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "original_seq_len = original_cache.shape[4]",
      "voiceover": "Record original sequence dimension to confirm resize occurred",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "insert_token(4)",
      "voiceover": "Exceed initial capacity forcing cache to resize and testing resize correctness",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "new_seq_len = kv_cache.kv_cache.shape[4]",
      "voiceover": "Get new sequence dimension after resize operation",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "assert new_seq_len > original_seq_len, f\"Cache did not resize: original seq_len={original_seq_len}, new seq_len={new_seq_len}\"",
      "voiceover": "Ensure resize actually occurred",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "for layer_idx in range(num_layers):",
      "voiceover": "Check all layers for data integrity",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "for token_idx in range(4):",
      "voiceover": "Verify each of the original 4 tokens",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "expected_k = float(token_idx)",
      "voiceover": "Expected key value based on token index",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "expected_v = float(token_idx * 100)",
      "voiceover": "Expected value based on token index times 100",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "actual_k = kv_cache.kv_cache[layer_idx, 0, :, :, token_idx, :]",
      "voiceover": "Extract actual key from resized cache",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "actual_v = kv_cache.kv_cache[layer_idx, 1, :, :, token_idx, :]",
      "voiceover": "Extract actual value from resized cache",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "assert (actual_k == expected_k).all(), f\"Layer {layer_idx}, token {token_idx}: key corrupted, expected {expected_k}\"",
      "voiceover": "Verify key data integrity",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "assert (actual_v == expected_v).all(), f\"Layer {layer_idx}, token {token_idx}: value corrupted, expected {expected_v}\"",
      "voiceover": "Verify value data integrity",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "original_k = original_cache[layer_idx, 0, :, :, token_idx, :]",
      "voiceover": "Extract key from pre-resize cache clone",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "original_v = original_cache[layer_idx, 1, :, :, token_idx, :]",
      "voiceover": "Extract value from pre-resize cache clone",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "assert (actual_k == original_k).all(), f\"Layer {layer_idx}, token {token_idx}: key doesn't match original\"",
      "voiceover": "Verify resize preserved original key data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "highlight",
      "path": "tests/test_engine.py",
      "find": "assert (actual_v == original_v).all(), f\"Layer {layer_idx}, token {token_idx}: value doesn't match original\"",
      "voiceover": "Verify resize preserved original value data",
      "voiceoverTiming": "during",
      "moveCursor": "endOfFile"
    },
    {
      "type": "openFile",
      "path": "tests/test_rustbpe.py"
    },
    {
      "type": "writeText",
      "content": "import regex as re\nfrom collections import Counter, defaultdict\nimport time\nimport rustbpe\nimport tiktoken\nimport pytest\n\nGPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n\ndef get_stats(ids, counts=None):\n    counts = {} if counts is None else counts\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\ndef merge(ids, pair, idx):\n    newids = []\n    i = 0\n    while i < len(ids):\n        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids\n\nclass RegexTokenizer:\n\n    def __init__(self, pattern=None):\n        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n        self.merges = {}\n        self.compiled_pattern = re.compile(self.pattern)\n        self.special_tokens = {}\n        self.inverse_special_tokens = {}\n        self.vocab = self._build_vocab()\n\n    def _build_vocab(self):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0, p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n        for special, idx in self.special_tokens.items():\n            vocab[idx] = special.encode(\"utf-8\")\n        return vocab\n\n    def train(self, text, vocab_size, verbose=False):\n        assert vocab_size >= 256\n        num_merges = vocab_size - 256\n\n        ambiguous = False\n\n        text_chunks = re.findall(self.compiled_pattern, text)\n\n        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n\n        merges = {}\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for i in range(num_merges):\n            stats = {}\n            for chunk_ids in ids:\n                get_stats(chunk_ids, stats)\n            pair = max(stats, key=stats.get)\n            pair_count = stats[pair]\n            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]\n            if len(pairs_with_max_count) > 1:\n                ambiguous = True\n            idx = 256 + i\n            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            if verbose:\n                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n\n        self.merges = merges\n        self.vocab = vocab\n        return ambiguous\n\n    def _encode_chunk(self, text_bytes):\n        ids = list(text_bytes)\n        while len(ids) >= 2:\n            stats = get_stats(ids)\n            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n            if pair not in self.merges:\n                break\n            idx = self.merges[pair]\n            ids = merge(ids, pair, idx)\n        return ids\n\n    def encode_ordinary(self, text):\n        text_chunks = re.findall(self.compiled_pattern, text)\n        ids = []\n        for chunk in text_chunks:\n            chunk_bytes = chunk.encode(\"utf-8\")\n            chunk_ids = self._encode_chunk(chunk_bytes)\n            ids.extend(chunk_ids)\n        return ids<|endoftext|>': 100257}\n        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n        self.merges = {}\n        self.compiled_pattern = re.compile(self.pattern)\n        self.special_tokens = {}\n        self.inverse_special_tokens = {}\n        self.vocab = self._build_vocab()\n\n    def _build_vocab(self):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0, p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n        for special, idx in self.special_tokens.items():\n            vocab[idx] = special.encode(\"utf-8\")\n        return vocab\n\n    def train(self, text, vocab_size, verbose=False):\n        assert vocab_size >= 256\n        num_merges = vocab_size - 256\n\n        ambiguous = False\n\n        text_chunks = re.findall(self.compiled_pattern, text)\n\n        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n\n        merges = {}\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for i in range(num_merges):\n            stats = {}\n            for chunk_ids in ids:\n                get_stats(chunk_ids, stats)\n            pair = max(stats, key=stats.get)\n            pair_count = stats[pair]\n            pairs_with_max_count = [pair for pair, count in stats.items() if count == pair_count]\n            if len(pairs_with_max_count) > 1:\n                ambiguous = True\n            idx = 256 + i\n            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            if verbose:\n                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n\n        self.merges = merges\n        self.vocab = vocab\n        return ambiguous\n\n    def _encode_chunk(self, text_bytes):\n        ids = list(text_bytes)\n        while len(ids) >= 2:\n            stats = get_stats(ids)\n            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n            if pair not in self.merges:\n                break\n            idx = self.merges[pair]\n            ids = merge(ids, pair, idx)\n        return ids\n\n    def encode_ordinary(self, text):\n        text_chunks = re.findall(self.compiled_pattern, text)\n        ids = []\n        for chunk in text_chunks:\n            chunk_bytes = chunk.encode(\"utf-8\")\n            chunk_ids = self._encode_chunk(chunk_bytes)\n            ids.extend(chunk_ids)\n        return ids\n\ndef fast_merge_inplace(ids, pair, idx):\n    i = 0\n    while i < len(ids) - 1:\n        if ids[i] == pair[0] and ids[i+1] == pair[1]:\n            ids[i] = idx\n            ids.pop(i+1)\n        else:\n            i += 1\n    return ids\n\nclass FastRegexTokenizer:\n\n    def __init__(self, pattern=None):\n        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n        self.compiled_pattern = re.compile(self.pattern)\n        self.special_tokens = {}\n        self.inverse_special_tokens = {}\n        self.merges = {}\n        self.vocab = self._build_vocab()\n\n    def _build_vocab(self):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0, p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n        for special, idx in self.special_tokens.items():\n            vocab[idx] = special.encode(\"utf-8\")\n        return vocab\n\n    def train(self, text, vocab_size, verbose=False):\n \n        assert vocab_size >= 256\n        num_merges = vocab_size - 256\n\n        text_chunks = re.findall(self.compiled_pattern, text)\n\n        counts = Counter(text_chunks)\n        unique_chunks = [ch for ch, count in counts.items()]\n        chunk_counts = [count for ch, count in counts.items()]\n\n        ids = [list(ch.encode(\"utf-8\")) for ch in unique_chunks]\n        merges = {}\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n\n        stats = defaultdict(int)\n        positions = defaultdict(set)\n\n        for chunk_idx, (chunk_ids, count) in enumerate(zip(ids, chunk_counts)):\n            for pair in zip(chunk_ids, chunk_ids[1:]):\n                stats[pair] += count\n                positions[pair].add(chunk_idx)\n\n        for i in range(num_merges):\n            if not stats:\n                break\n\n            pair = max(stats, key=stats.get)\n            idx = 256 + i\n\n            affected_chunks = positions[pair]\n\n            count_changes = defaultdict(int)\n\n            for chunk_idx in affected_chunks:\n                chunk_ids = ids[chunk_idx]\n                chunk_count = chunk_counts[chunk_idx]\n                ix = 0\n                while ix < len(chunk_ids) - 1:\n                    if chunk_ids[ix] == pair[0] and chunk_ids[ix+1] == pair[1]:\n                        if ix > 0:\n                            old_left = (chunk_ids[ix-1], chunk_ids[ix])\n                            count_changes[old_left] -= chunk_count\n\n                        count_changes[pair] -= chunk_count\n\n                        if ix + 2 < len(chunk_ids):\n                            old_right = (chunk_ids[ix+1], chunk_ids[ix+2])\n                            count_changes[old_right] -= chunk_count\n\n                        chunk_ids[ix] = idx\n                        chunk_ids.pop(ix+1)\n\n                        if ix > 0:\n                            new_left = (chunk_ids[ix-1], chunk_ids[ix])\n                            count_changes[new_left] += chunk_count\n\n                        if ix + 1 < len(chunk_ids):\n                            new_right = (chunk_ids[ix], chunk_ids[ix+1])\n                            count_changes[new_right] += chunk_count\n                    else:\n                        ix += 1\n\n            for changed_pair, delta in count_changes.items():\n                if changed_pair == pair:\n                    continue\n\n                stats[changed_pair] += delta\n\n                for chunk_idx in affected_chunks:\n                    chunk_ids = ids[chunk_idx]\n                    contains_pair = any((chunk_ids[j], chunk_ids[j+1]) == changed_pair\n                                      for j in range(len(chunk_ids) - 1))\n                    if contains_pair:\n                        positions[changed_pair].add(chunk_idx)\n                    else:\n                        positions[changed_pair].discard(chunk_idx)\n\n            del stats[pair]\n            del positions[pair]\n\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n\n        self.merges = merges\n        self.vocab = vocab\n\n    def register_special_tokens(self, special_tokens):\n        self.special_tokens = special_tokens\n        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n\n    def decode(self, ids):\n        part_bytes = []\n        for idx in ids:\n            if idx in self.vocab:\n                part_bytes.append(self.vocab[idx])\n            elif idx in self.inverse_special_tokens:\n                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n            else:\n                raise ValueError(f\"invalid token id: {idx}\")\n        text_bytes = b\"\".join(part_bytes)\n        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n        return text\n\n    def _encode_chunk(self, text_bytes):\n        ids = list(text_bytes)\n        while len(ids) >= 2:\n            stats = get_stats(ids)\n            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n            if pair not in self.merges:\n                break\n            idx = self.merges[pair]\n            ids = fast_merge_inplace(ids, pair, idx)\n        return ids\n\n    def encode_ordinary(self, text):\n        text_chunks = re.findall(self.compiled_pattern, text)\n        ids = []\n        for chunk in text_chunks:\n            chunk_bytes = chunk.encode(\"utf-8\")\n            chunk_ids = self._encode_chunk(chunk_bytes)\n            ids.extend(chunk_ids)\n        return ids\n\nfrom tokenizers import Tokenizer as HFTokenizer\nfrom tokenizers import pre_tokenizers, decoders, Regex\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\n\nclass HuggingFaceTokenizer:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    @classmethod\n    def train_from_iterator(cls, text_iterator, vocab_size):\n        tokenizer = HFTokenizer(BPE(\n            byte_fallback=True,\n            unk_token=None,\n            fuse_unk=False,\n        ))\n        tokenizer.normalizer = None\n        gpt4_split_regex = Regex(GPT4_SPLIT_PATTERN)\n        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n            pre_tokenizers.Split(pattern=gpt4_split_regex, behavior=\"isolated\", invert=False),\n            pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)\n        ])\n        tokenizer.decoder = decoders.ByteLevel()\n        tokenizer.post_processor = None\n        trainer = BpeTrainer(\n            vocab_size=vocab_size,\n            show_progress=True,\n            min_frequency=0,\n            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n            special_tokens=[],\n        )\n        tokenizer.train_from_iterator(text_iterator, trainer)\n        return cls(tokenizer)\n\n    def encode_ordinary(self, text):\n        ids = self.tokenizer.encode(text, add_special_tokens=False).ids\n        return ids\n\n@pytest.fixture(scope=\"module\")\ndef enwik8_path():\n    import os\n    import zipfile\n    from nanochat.common import get_base_dir\n    base_dir = get_base_dir()\n    enwik8_url = \"https://mattmahoney.net/dc/enwik8.zip\"\n    enwik8_local_path = os.path.join(base_dir, \"enwik8\")\n    enwik8_local_path_zip = os.path.join(base_dir, \"enwik8.zip\")\n    if not os.path.exists(enwik8_local_path):\n        print(f\"Downloading enwik8 to {enwik8_local_path_zip}\")\n        import requests\n        response = requests.get(enwik8_url)\n        with open(enwik8_local_path_zip, \"wb\") as f:\n            f.write(response.content)\n        with zipfile.ZipFile(enwik8_local_path_zip, \"r\") as zip_ref:\n            zip_ref.extractall(base_dir)\n        print(f\"Unzipped enwik8 to {enwik8_local_path}\")\n        os.remove(enwik8_local_path_zip)\n        print(f\"Removed {enwik8_local_path_zip}\")\n    else:\n        print(f\"Using existing enwik8 at {enwik8_local_path}\")\n    return enwik8_local_path\n\n@pytest.fixture(scope=\"module\")\ndef enwik8_small(enwik8_path):\n    with open(enwik8_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read(100_000)\n\n@pytest.fixture(scope=\"module\")\ndef enwik8_large(enwik8_path):\n    with open(enwik8_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read(10**7)\n\ndef time_function(func, *args, **kwargs):\n    start_time = time.time()\n    result = func(*args, **kwargs)\n    end_time = time.time()\n    elapsed = end_time - start_time\n    return result, elapsed\n\ndef test_correctness(enwik8_small):\n    text = enwik8_small\n    encode_text = text\n    vocab_size = 256 + 20\n\n    print(\"\\nTraining slow reference...\")\n    slow_reference_tokenizer = RegexTokenizer()\n    ambiguous_flag, slow_reference_train_time = time_function(slow_reference_tokenizer.train, text, vocab_size)\n    slow_reference_ids, slow_reference_encode_time = time_function(slow_reference_tokenizer.encode_ordinary, encode_text)\n    print(f\"Slow reference train time: {slow_reference_train_time:.4f}s\")\n    print(f\"Slow reference encode time: {slow_reference_encode_time:.4f}s\")\n    print(slow_reference_ids[:20])\n\n    if ambiguous_flag:\n        print(\"‼️ WARNING: merge order was detected to be ambiguous given current text and vocab size\")\n        print(\"The implementation could be correct but we might see different results below\")\n    else:\n        print(\"✅ Merge order is NOT ambiguous\")\n\n    print(\"\\nTraining fast reference...\")\n    fast_reference_tokenizer = FastRegexTokenizer()\n    _, fast_reference_train_time = time_function(fast_reference_tokenizer.train, text, vocab_size)\n    fast_reference_ids, fast_reference_encode_time = time_function(fast_reference_tokenizer.encode_ordinary, encode_text)\n    print(f\"Fast reference train time: {fast_reference_train_time:.4f}s\")\n    print(f\"Fast reference encode time: {fast_reference_encode_time:.4f}s\")\n    print(fast_reference_ids[:20])\n\n    assert fast_reference_ids == slow_reference_ids, \"Fast reference should match slow reference\"\n    print(\"✅ Fast == Slow\")\n\n    print(\"\\nTraining HuggingFace...\")\n    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)\n    hf_ids, hf_encode_time = time_function(hf_tokenizer.encode_ordinary, encode_text)\n    print(f\"HuggingFace train time: {hf_train_time:.4f}s\")\n    print(f\"HuggingFace encode time: {hf_encode_time:.4f}s\")\n    print(hf_ids[:20])\n\n    def custom_match(ids1, ids2):\n        perm = {}\n        for x, y in zip(ids1, ids2):\n            if x < 256:\n                if x in perm:\n                    if perm[x] != y:\n                        return False\n                perm[x] = y\n            if x >= 256 and x != y:\n                return False\n        return True\n\n    assert custom_match(hf_ids, fast_reference_ids), \"HuggingFace should match fast reference\"\n    print(\"✅ HuggingFace == Fast\")\n\n    print(\"\\nTraining rustbpe...\")\n    rustbpe_tokenizer = rustbpe.Tokenizer()\n    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)\n    rustbpe_ids, rustbpe_encode_time = time_function(rustbpe_tokenizer.encode, encode_text)\n    print(f\"RustBPE train time: {rustbpe_train_time:.4f}s\")\n    print(f\"RustBPE encode time: {rustbpe_encode_time:.4f}s\")\n    print(rustbpe_ids[:20])\n\n    assert rustbpe_ids == fast_reference_ids, \"RustBPE should match fast reference\"\n    print(\"✅ RustBPE == Fast\")\n\n    print(\"\\nTesting tiktoken export...\")\n    pattern = rustbpe_tokenizer.get_pattern()\n    mergeable_ranks_list = rustbpe_tokenizer.get_mergeable_ranks()\n    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n    enc = tiktoken.Encoding(\n        name=\"rustbpe\",\n        pat_str=pattern,\n        mergeable_ranks=mergeable_ranks,\n        special_tokens={},\n    )\n    tiktoken_ids, tiktoken_encode_time = time_function(enc.encode, encode_text)\n    print(f\"Tiktoken encode time: {tiktoken_encode_time:.4f}s\")\n    print(tiktoken_ids[:20])\n\n    assert tiktoken_ids == rustbpe_ids, \"Tiktoken should match RustBPE\"\n    print(\"✅ Tiktoken == RustBPE\")\n\n@pytest.mark.slow\ndef test_training_performance(enwik8_large):\n    text = enwik8_large\n    vocab_size = 2048\n    print(f\"\\nText length: {len(text)}\")\n    print(\"\\nTraining rustbpe...\")\n    rustbpe_tokenizer = rustbpe.Tokenizer()\n    _, rustbpe_train_time = time_function(rustbpe_tokenizer.train_from_iterator, [text], vocab_size)\n    print(f\"RustBPE train time: {rustbpe_train_time:.4f}s\")\n    assert rustbpe_train_time > 0, \"Training should take some time\"\n\n    print(\"\\nTraining HuggingFace...\")\n    hf_tokenizer, hf_train_time = time_function(HuggingFaceTokenizer.train_from_iterator, [text], vocab_size)\n    print(f\"HuggingFace train time: {hf_train_time:.4f}s\")\n    assert hf_train_time > 0, \"Training should take some time\"\n\n    print(f\"\\n📊 Performance comparison:\")\n    print(f\"   RustBPE: {rustbpe_train_time:.4f}s\")\n    print(f\"   HuggingFace: {hf_train_time:.4f}s\")\n    print(f\"   Speedup: {hf_train_time/rustbpe_train_time:.2f}x\")\n\ndef test_interface(enwik8_small):\n    import tempfile\n    from nanochat.tokenizer import RustBPETokenizer\n\n    vocab_size = 300\n    tok = RustBPETokenizer.train_from_iterator([enwik8_small], vocab_size)\n    assert tok.get_vocab_size() == vocab_size, f\"Expected vocab size {vocab_size}, got {tok.get_vocab_size()}\"\n    print(f\"✅ Trained tokenizer with vocab size {vocab_size}\")\n\n    encode_text = \"Hello world! How are you? 🙃\"\n    ids = tok.encode(encode_text)\n    print(f\"\\nInput text: {encode_text}\")\n    print(f\"IDs: {ids}\")\n    decoded = tok.decode(ids)\n    print(f\"Decoded: {decoded}\")\n    assert decoded == encode_text, f\"Decoded text doesn't match: {decoded} != {encode_text}\"\n    print(\"✅ Encode/decode test passed\")\n\n    ids_new = tok.encode([encode_text, encode_text])\n    assert all(x == ids for x in ids_new), \"Batch encoding should produce identical results\"\n    print(\"✅ Encode batch OK\")\n\n    ids_special = tok.encode(encode_text, prepend=\"<|bos|>\", append=\"<|bos|>\")\n    bos_token_id = tok.encode_special(\"<|bos|>\")\n    assert ids_special == [bos_token_id] + ids + [bos_token_id], \"Special tokens not correctly added\"\n    print(\"✅ append/prepend OK\")\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        tok.save(tmp_dir)\n        tok_reloaded = RustBPETokenizer.from_directory(tmp_dir)\n        ids_reloaded = tok_reloaded.encode(encode_text)\n        assert ids_reloaded == ids, \"Reloaded tokenizer should produce same results\"\n        print(\"✅ Save/load through temporary directory OK\")\n\n"
    }
  ]
}